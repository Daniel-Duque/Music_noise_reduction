{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.io.wavfile import write\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=r'wavs\\clean\\clnsp0.wav'\n",
    "ms=read(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176320,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampFreq, snd =read(r'wavs\\clean\\clnsp0.wav')\n",
    "write('test-2.wav',sampFreq,snd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, numpy.ndarray)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampFreq),type(snd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def create_data_from_dir(dir_path,top_x=200,vector_min_size=5000):\n",
    "    #dir_path directorio o folder donde estan los wavs\n",
    "    # top_x opcional cuantos archivos maximo desea usar, dejar vacio para usarlos todos\n",
    "    dir = os.listdir(dir_path)\n",
    "    snd_list=[]\n",
    "    s_rates=[]\n",
    "    for i, file in enumerate(dir):\n",
    "        if i<=top_x:\n",
    "            input_file_path = os.path.join(dir_path, file)\n",
    "            sampFreq, snd =read(input_file_path)\n",
    "            num_batches=ceil(snd.shape[0]/vector_min_size) if snd.shape[0]>vector_min_size else 1\n",
    "            ms=np.resize(snd,(vector_min_size*num_batches))\n",
    "            batches=np.hsplit(ms,num_batches)\n",
    "            for batch in batches:\n",
    "                snd_list.append(batch)\n",
    "                s_rates.append(sampFreq)\n",
    "    return snd_list,s_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_min_size=2500\n",
    "n_samples=2000\n",
    "clean_specs,clean_s_rates=create_data_from_dir(r'wavs\\clean',top_x=n_samples,vector_min_size=vector_min_size)\n",
    "noisy_specs,noisy_s_rates=create_data_from_dir(r'wavs\\noisy',top_x=n_samples,vector_min_size=vector_min_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=min(len(clean_specs),len(noisy_specs))\n",
    "clean_specs=clean_specs[:cap]\n",
    "noisy_specs=noisy_specs[:cap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102036, 102036)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_specs),len(noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_specs=np.array(clean_specs)\n",
    "noisy_specs=np.array(noisy_specs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np   \n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(noisy_specs, clean_specs, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Dense,Flatten,Reshape,InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from PIL import Image\n",
    "import re\n",
    "from math import sqrt\n",
    "from tensorflow.keras.applications import VGG16,ResNet50,MobileNet\n",
    "from tensorflow.keras import layers, Model, Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs):\n",
    "    my_callbacks = [\n",
    "        #tf.keras.callbacks.ModelCheckpoint(filepath='callbacks/model.{epoch:02d}.h5')\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "        #filepath='callbacks/model-{epoch:02d}-{loss:2f}.h5'\n",
    "        filepath='callbacks/model.h5'\n",
    "        ,monitor='loss'\n",
    "        ,verbose=1\n",
    "        ,save_weights_only=True\n",
    "        ,save_best_only=True\n",
    "        ,mode='min')\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "                    train_gen\n",
    "                    ,epochs=epochs\n",
    "                    ,callbacks=my_callbacks\n",
    "                    )\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Loss'], loc = 'upper left')\n",
    "\n",
    "    return model,history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function based on Audio sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_audio(audio_path, model,file_name):\n",
    "    vector_min_size=2500\n",
    "    preds=[]\n",
    "    sampFreq, snd=read(audio_path)\n",
    "    num_batches=ceil(snd.shape[0]/vector_min_size) if snd.shape[0]>vector_min_size else 1\n",
    "    ms=np.resize(snd,(vector_min_size*num_batches))\n",
    "    batches=np.hsplit(ms,num_batches)\n",
    "    batches=np.array(batches)\n",
    "    preds=model.predict(batches)\n",
    "    preds=np.concatenate([pred for pred in preds])\n",
    "    write(f'{file_name}.wav',sampFreq,preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102036, 2500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted transformer block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.05):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.num_heads=num_heads\n",
    "        self.ff_dim=ff_dim\n",
    "        self.rate=rate\n",
    "        self.embed_dim=embed_dim\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate,\n",
    "\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pool of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name,inputs,drop_out=0.1):\n",
    "\n",
    "    image_side=int(sqrt(clean_specs.shape[-1]))\n",
    "    image_dims=(image_side,image_side,1)\n",
    "\n",
    "\n",
    "    if model_name==\"naive\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(64))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(64))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn1\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(3000))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn2\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(1024))\n",
    "        model.add(layers.Dense(256))\n",
    "        model.add(layers.Dense(128))\n",
    "        model.add(layers.Dense(64))\n",
    "        model.add(layers.Dense(128))\n",
    "        model.add(layers.Dense(256))\n",
    "        model.add(layers.Dense(320))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn3\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(1024,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(64,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(320,activation=\"relu\"))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn4\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn4_stacked2\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        \n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn3_stacked2\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "\n",
    "        model.add(layers.Dense(1024,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(64,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(320,activation=\"relu\"))\n",
    "        model.add(layers.Dense(1024,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(64,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(320,activation=\"relu\"))\n",
    "        \n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "\n",
    "    elif model_name==\"cnn1\":\n",
    "\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Reshape(image_dims))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (1,1),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 1, kernel_size = (3,3), padding = 'Same',\n",
    "                        activation ='relu'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"cnn2\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Reshape(image_dims))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (1,1),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 1, kernel_size = (3,3), padding = 'Same',\n",
    "                        activation ='relu'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"VGG16\":\n",
    "        # model_VGG16=VGG16(weights='imagenet',include_top= False)\n",
    "        # model_VGG16.trainable=False\n",
    "        # model=models.Sequential()\n",
    "        # model.add(layers.Input(shape=(inputs,)))\n",
    "        # model.add(layers.Reshape(image_dims))\n",
    "        # model.add(layers.UpSampling2D(5))\n",
    "        # model.add(layers.Conv2D(filters = 3, kernel_size = (27,27),padding = 'Same', \n",
    "        #                 activation ='relu'))\n",
    "        # model.add(model_VGG16)\n",
    "        # model.add(layers.Flatten())\n",
    "        # model.add(layers.Dense(1024))\n",
    "        # model.add(layers.Dense(inputs))\n",
    "        # model.compile(optimizer='adamax', loss='mse')\n",
    "        # model.summary()\n",
    "\n",
    "        Input_vector=keras.Input(shape=(inputs,))\n",
    "        Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "        upscale=keras.layers.UpSampling2D(5)(Input_img)\n",
    "        x1 = keras.layers.Conv2D(3, (27, 27), activation='relu', padding='valid')(upscale)\n",
    "\n",
    "\n",
    "        model_transfer_test = keras.applications.VGG16(\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False\n",
    "        )\n",
    "        model_transfer_test.trainable = False\n",
    "        inputs_transfer = keras.Input(shape=(224, 224, 3))\n",
    "        x = model_transfer_test(x1, training=False)\n",
    "        #pooling layer \n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dense(1024, activation = 'softmax')(x)\n",
    "        x = keras.layers.Dropout(drop_out, input_shape=(1024,))(x)\n",
    "        #final dense layer\n",
    "        outputs = keras.layers.Dense(inputs, activation = 'relu')(x)\n",
    "        model = keras.Model(Input_vector,outputs)\n",
    "        model.compile(optimizer='adam', loss = 'mse')\n",
    "        model.summary()\n",
    "\n",
    "    elif model_name==\"MobileNet\":\n",
    "        Input_vector=keras.Input(shape=(inputs,))\n",
    "        Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "        upscale=keras.layers.UpSampling2D(5)(Input_img)\n",
    "        x1 = keras.layers.Conv2D(3, (27, 27), activation='relu', padding='valid')(upscale)\n",
    "\n",
    "\n",
    "        model_transfer_test = keras.applications.MobileNet(\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False\n",
    "        )\n",
    "        model_transfer_test.trainable = False\n",
    "        inputs_transfer = keras.Input(shape=(224, 224, 3))\n",
    "        x = model_transfer_test(x1, training=False)\n",
    "        #pooling layer \n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dense(1024, activation = 'softmax')(x)\n",
    "        x = keras.layers.Dropout(drop_out, input_shape=(1024,))(x)\n",
    "        #final dense layer\n",
    "        outputs = keras.layers.Dense(inputs, activation = 'relu')(x)\n",
    "        model = keras.Model(Input_vector,outputs)\n",
    "        model.compile(optimizer='adam', loss = 'mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"MobileNet_2\":\n",
    "        Input_vector=keras.Input(shape=(inputs,))\n",
    "        Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "        upscale=keras.layers.UpSampling2D(3)(Input_img)\n",
    "        x1 = keras.layers.Conv2D(3, (3, 3), activation='relu', padding='same')(upscale)\n",
    "\n",
    "\n",
    "        model_transfer_test = keras.applications.MobileNet(\n",
    "        weights='imagenet',\n",
    "        input_shape=(150, 150, 3),\n",
    "        include_top=False\n",
    "        )\n",
    "        model_transfer_test.trainable = False\n",
    "        inputs_transfer = keras.Input(shape=(150, 150, 3))\n",
    "        x = model_transfer_test(x1, training=False)\n",
    "        #pooling layer \n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dense(2500, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop_out, input_shape=(2500,))(x)\n",
    "\n",
    "        #decoder\n",
    "        x = keras.layers.Dense(1024, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(64, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(320, activation = 'relu')(x)\n",
    "\n",
    "\n",
    "        # Input_img2=keras.layers.Reshape(image_dims)(x)\n",
    "        # x1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(Input_img2)\n",
    "        # x1 = keras.layers.UpSampling2D(2)(x1)\n",
    "        # x2 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x1)\n",
    "        # x2 = keras.layers.UpSampling2D(2)(x2)\n",
    "        # x3 = keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x2)\n",
    "        # x3 = keras.layers.UpSampling2D(2)(x3)\n",
    "        # x4 = keras.layers.Conv2D(1, (3, 3), activation='relu', padding='same')(x3)\n",
    "        #flat = keras.layers.Flatten()(x4)\n",
    "        #final dense layer\n",
    "        outputs = keras.layers.Dense(inputs, activation = 'softmax')(x)\n",
    "        model = keras.Model(Input_vector,outputs)\n",
    "        model.compile(optimizer='adam', loss = 'mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"MobileNet_3\":\n",
    "        Input_vector=keras.Input(shape=(inputs,))\n",
    "        Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "        x = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(Input_img)\n",
    "        x = keras.layers.MaxPooling2D(2,strides=2)(x)\n",
    "        x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.MaxPooling2D(2,strides=2)(x)\n",
    "        x = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.MaxPooling2D(2,strides=2)(x)\n",
    "        x = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.UpSampling2D(2)(x)\n",
    "        x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.UpSampling2D(2)(x)\n",
    "        x = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.UpSampling2D(2)(x)\n",
    "        x = keras.layers.Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "        upscale=keras.layers.UpSampling2D(4)(x)\n",
    "        # x1 = keras.layers.Conv2D(3, (3, 3), activation='relu', padding='same')(upscale)\n",
    "\n",
    "\n",
    "        model_transfer_test = keras.applications.MobileNet(\n",
    "        weights='imagenet',\n",
    "        input_shape=(192, 192, 3),\n",
    "        include_top=False\n",
    "        )\n",
    "        model_transfer_test.trainable = False\n",
    "        inputs_transfer = keras.Input(shape=(192, 192, 3))\n",
    "        x = model_transfer_test(upscale, training=False)\n",
    "        #pooling layer \n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dense(300, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop_out, input_shape=(300,))(x)\n",
    "\n",
    "        #decoder\n",
    "        # x = keras.layers.Dense(1024, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(64, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(320, activation = 'relu')(x)\n",
    "\n",
    "\n",
    "        # Input_img2=keras.layers.Reshape(image_dims)(x)\n",
    "        # x1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(Input_img2)\n",
    "        # x1 = keras.layers.UpSampling2D(2)(x1)\n",
    "        # x2 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x1)\n",
    "        # x2 = keras.layers.UpSampling2D(2)(x2)\n",
    "        # x3 = keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x2)\n",
    "        # x3 = keras.layers.UpSampling2D(2)(x3)\n",
    "        # x4 = keras.layers.Conv2D(1, (3, 3), activation='relu', padding='same')(x3)\n",
    "        #flat = keras.layers.Flatten()(x4)\n",
    "        #final dense layer\n",
    "        outputs = keras.layers.Dense(inputs, activation = 'softmax')(x)\n",
    "        model = keras.Model(Input_vector,outputs)\n",
    "        model.compile(optimizer='adam', loss = 'mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"Transformer1\":\n",
    "        num_heads=3\n",
    "        ff_dim=10\n",
    "\n",
    "        model=keras.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Reshape(image_dims))\n",
    "        model.add(layers.Flatten()) \n",
    "        model.add(layers.Embedding(input_dim=(inputs),\n",
    "                                         output_dim=20))\n",
    "\n",
    "        model.add(TransformerBlock(20, num_heads, ff_dim,rate=drop_out))              \n",
    "        model.add(layers.Dense(1))\n",
    "        #model.add(layers.Dense(inputs))\n",
    "\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_7 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 2500)              0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 2500, 20)          50000     \n",
      "                                                                 \n",
      " transformer_block_3 (Transf  (None, 2500, 20)         5510      \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 2500, 1)           21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,531\n",
      "Trainable params: 55,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2725700.7500\n",
      "Epoch 1: loss improved from inf to 2725700.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 278s 87ms/step - loss: 2725700.7500\n",
      "Epoch 2/10\n",
      "  48/3189 [..............................] - ETA: 4:33 - loss: 2819495.2500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m INPUTS\u001b[39m=\u001b[39mclean_specs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m custom_mobile_net\u001b[39m=\u001b[39mselect_model(\u001b[39m\"\u001b[39m\u001b[39mTransformer1\u001b[39m\u001b[39m\"\u001b[39m,INPUTS,drop_out\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m custom_mobile_net,history_custom_mobile_net\u001b[39m=\u001b[39mtrain_model(custom_mobile_net,\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model,epochs):\n\u001b[0;32m      2\u001b[0m     my_callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         \u001b[39m#tf.keras.callbacks.ModelCheckpoint(filepath='callbacks/model.{epoch:02d}.h5')\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         ,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     ]\n\u001b[1;32m---> 14\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     15\u001b[0m                     train_gen\n\u001b[0;32m     16\u001b[0m                     ,epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m     17\u001b[0m                     ,callbacks\u001b[39m=\u001b[39;49mmy_callbacks\n\u001b[0;32m     18\u001b[0m                     )\n\u001b[0;32m     20\u001b[0m     plt\u001b[39m.\u001b[39mclf()\n\u001b[0;32m     21\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUTS=clean_specs.shape[-1]\n",
    "custom_mobile_net=select_model(\"Transformer1\",INPUTS,drop_out=0.1)\n",
    "custom_mobile_net,history_custom_mobile_net=train_model(custom_mobile_net,10)\n",
    "#2736502.5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFLow Experimentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom crafted networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 19:47:25 INFO mlflow.tracking.fluent: Experiment with name 'naivevector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 1:29:44 - loss: 6463151.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2605078.0000\n",
      "Epoch 1: loss improved from inf to 2602324.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 2ms/step - loss: 2602324.2500\n",
      "Epoch 2/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2440344.0000\n",
      "Epoch 2: loss improved from 2602324.25000 to 2442504.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2442504.0000\n",
      "Epoch 3/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2412632.0000\n",
      "Epoch 3: loss improved from 2442504.00000 to 2412264.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2412264.2500\n",
      "Epoch 4/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2393152.0000\n",
      "Epoch 4: loss improved from 2412264.25000 to 2393724.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2393724.2500\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2380439.7500\n",
      "Epoch 5: loss improved from 2393724.25000 to 2380707.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2380707.7500\n",
      "Epoch 6/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2370361.0000\n",
      "Epoch 6: loss improved from 2380707.75000 to 2370161.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2370161.2500\n",
      "Epoch 7/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2365976.0000\n",
      "Epoch 7: loss improved from 2370161.25000 to 2364179.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2364179.2500\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2359368.0000\n",
      "Epoch 8: loss improved from 2364179.25000 to 2359692.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2359692.2500\n",
      "Epoch 9/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2354779.0000\n",
      "Epoch 9: loss improved from 2359692.25000 to 2354810.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2354810.2500\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2349304.7500\n",
      "Epoch 10: loss improved from 2354810.25000 to 2348834.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2348834.7500\n",
      "Epoch 11/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2344967.7500\n",
      "Epoch 11: loss improved from 2348834.75000 to 2345544.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2345544.0000\n",
      "Epoch 12/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2343694.0000\n",
      "Epoch 12: loss improved from 2345544.00000 to 2343307.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2343307.2500\n",
      "Epoch 13/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2341847.5000\n",
      "Epoch 13: loss improved from 2343307.25000 to 2341554.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2341554.0000\n",
      "Epoch 14/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2336245.0000\n",
      "Epoch 14: loss improved from 2341554.00000 to 2338480.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2338480.0000\n",
      "Epoch 15/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2335219.2500\n",
      "Epoch 15: loss improved from 2338480.00000 to 2335778.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2335778.0000\n",
      "Epoch 16/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2335471.5000\n",
      "Epoch 16: loss improved from 2335778.00000 to 2333960.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2333960.2500\n",
      "Epoch 17/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2332032.2500\n",
      "Epoch 17: loss improved from 2333960.25000 to 2332204.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2332204.5000\n",
      "Epoch 18/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2331701.7500\n",
      "Epoch 18: loss improved from 2332204.50000 to 2331564.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2331564.5000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2331105.0000\n",
      "Epoch 19: loss improved from 2331564.50000 to 2330072.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2330072.5000\n",
      "Epoch 20/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2329864.0000\n",
      "Epoch 20: loss improved from 2330072.50000 to 2329194.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2329194.2500\n",
      "Epoch 21/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2328807.7500\n",
      "Epoch 21: loss improved from 2329194.25000 to 2327223.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2327223.2500\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2326712.5000\n",
      "Epoch 22: loss improved from 2327223.25000 to 2326397.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2326397.7500\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2324650.7500\n",
      "Epoch 23: loss improved from 2326397.75000 to 2324469.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2324469.7500\n",
      "Epoch 24/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2321721.0000\n",
      "Epoch 24: loss improved from 2324469.75000 to 2322196.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2322196.0000\n",
      "Epoch 25/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2321256.0000\n",
      "Epoch 25: loss improved from 2322196.00000 to 2320912.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2320912.5000\n",
      "Epoch 26/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2320351.5000\n",
      "Epoch 26: loss improved from 2320912.50000 to 2320818.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2320818.7500\n",
      "Epoch 27/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2322815.0000\n",
      "Epoch 27: loss improved from 2320818.75000 to 2320503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2320503.7500\n",
      "Epoch 28/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2318249.7500\n",
      "Epoch 28: loss improved from 2320503.75000 to 2319082.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2319082.0000\n",
      "Epoch 29/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2317314.0000\n",
      "Epoch 29: loss improved from 2319082.00000 to 2318463.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2318463.0000\n",
      "Epoch 30/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2318792.0000\n",
      "Epoch 30: loss improved from 2318463.00000 to 2318450.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2318450.0000\n",
      "Epoch 31/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2320999.7500\n",
      "Epoch 31: loss improved from 2318450.00000 to 2318286.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2318286.7500\n",
      "Epoch 32/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2317060.2500\n",
      "Epoch 32: loss improved from 2318286.75000 to 2317621.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2317621.2500\n",
      "Epoch 33/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2315735.7500\n",
      "Epoch 33: loss improved from 2317621.25000 to 2315246.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2315246.5000\n",
      "Epoch 34/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2316080.0000\n",
      "Epoch 34: loss did not improve from 2315246.50000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2316097.0000\n",
      "Epoch 35/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2315603.0000\n",
      "Epoch 35: loss did not improve from 2315246.50000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2315805.0000\n",
      "Epoch 36/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2314906.0000\n",
      "Epoch 36: loss improved from 2315246.50000 to 2314353.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2314353.7500\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2314942.0000\n",
      "Epoch 37: loss did not improve from 2314353.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2314634.2500\n",
      "Epoch 38/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2314253.7500\n",
      "Epoch 38: loss improved from 2314353.75000 to 2313710.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2313710.5000\n",
      "Epoch 39/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2315163.7500\n",
      "Epoch 39: loss did not improve from 2313710.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2313771.2500\n",
      "Epoch 40/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2313366.2500\n",
      "Epoch 40: loss improved from 2313710.50000 to 2312444.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2312444.5000\n",
      "Epoch 41/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2312784.0000\n",
      "Epoch 41: loss did not improve from 2312444.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2312730.2500\n",
      "Epoch 42/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2312866.7500\n",
      "Epoch 42: loss improved from 2312444.50000 to 2311900.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2311900.7500\n",
      "Epoch 43/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2311005.2500\n",
      "Epoch 43: loss improved from 2311900.75000 to 2311894.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2311894.7500\n",
      "Epoch 44/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2312308.0000\n",
      "Epoch 44: loss improved from 2311894.75000 to 2311704.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2311704.0000\n",
      "Epoch 45/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2312609.7500\n",
      "Epoch 45: loss did not improve from 2311704.00000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2312509.0000\n",
      "Epoch 46/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2312558.0000\n",
      "Epoch 46: loss did not improve from 2311704.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2312460.5000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2311104.0000\n",
      "Epoch 47: loss improved from 2311704.00000 to 2311104.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311104.0000\n",
      "Epoch 48/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2311238.0000\n",
      "Epoch 48: loss did not improve from 2311104.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311175.5000\n",
      "Epoch 49/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2308779.2500\n",
      "Epoch 49: loss improved from 2311104.00000 to 2310666.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310666.7500\n",
      "Epoch 50/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2310456.5000\n",
      "Epoch 50: loss improved from 2310666.75000 to 2310409.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310409.5000\n",
      "Epoch 51/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2310299.2500\n",
      "Epoch 51: loss improved from 2310409.50000 to 2310124.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310124.2500\n",
      "Epoch 52/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2311929.7500\n",
      "Epoch 52: loss did not improve from 2310124.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310601.5000\n",
      "Epoch 53/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2309463.2500\n",
      "Epoch 53: loss improved from 2310124.25000 to 2309483.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2309483.7500\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2309158.7500\n",
      "Epoch 54: loss did not improve from 2309483.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2310031.5000\n",
      "Epoch 55/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2309401.0000\n",
      "Epoch 55: loss improved from 2309483.75000 to 2308485.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308485.7500\n",
      "Epoch 56/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2309545.7500\n",
      "Epoch 56: loss improved from 2308485.75000 to 2308159.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308159.7500\n",
      "Epoch 57/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2309113.5000\n",
      "Epoch 57: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2309095.7500\n",
      "Epoch 58/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2310232.2500\n",
      "Epoch 58: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2308563.0000\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2308232.5000\n",
      "Epoch 59: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2308693.7500\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2308718.7500\n",
      "Epoch 60: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308534.0000\n",
      "Epoch 61/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2308142.5000\n",
      "Epoch 61: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308611.7500\n",
      "Epoch 62/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2308689.7500\n",
      "Epoch 62: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308321.0000\n",
      "Epoch 63/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2306702.5000\n",
      "Epoch 63: loss improved from 2308159.75000 to 2307394.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2307394.5000\n",
      "Epoch 64/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2308689.5000\n",
      "Epoch 64: loss did not improve from 2307394.50000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308586.7500\n",
      "Epoch 65/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2307864.5000\n",
      "Epoch 65: loss did not improve from 2307394.50000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308105.5000\n",
      "Epoch 66/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2306869.7500\n",
      "Epoch 66: loss did not improve from 2307394.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2308066.0000\n",
      "Epoch 67/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2303739.2500\n",
      "Epoch 67: loss improved from 2307394.50000 to 2306300.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2306300.2500\n",
      "Epoch 68/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2307276.7500\n",
      "Epoch 68: loss did not improve from 2306300.25000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2306576.0000\n",
      "Epoch 69/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2306764.0000\n",
      "Epoch 69: loss did not improve from 2306300.25000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2307060.0000\n",
      "Epoch 70/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2308126.2500\n",
      "Epoch 70: loss improved from 2306300.25000 to 2306075.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2306075.7500\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2306392.5000\n",
      "Epoch 71: loss improved from 2306075.75000 to 2305890.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305890.0000\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2306411.7500\n",
      "Epoch 72: loss did not improve from 2305890.00000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2306411.7500\n",
      "Epoch 73/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2305524.0000\n",
      "Epoch 73: loss improved from 2305890.00000 to 2305856.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2305856.7500\n",
      "Epoch 74/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2306055.5000\n",
      "Epoch 74: loss did not improve from 2305856.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2306550.5000\n",
      "Epoch 75/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2307077.0000\n",
      "Epoch 75: loss did not improve from 2305856.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2306068.7500\n",
      "Epoch 76/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2306438.0000\n",
      "Epoch 76: loss did not improve from 2305856.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306266.7500\n",
      "Epoch 77/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2306273.0000\n",
      "Epoch 77: loss improved from 2305856.75000 to 2305533.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305533.0000\n",
      "Epoch 78/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2305613.5000\n",
      "Epoch 78: loss improved from 2305533.00000 to 2305458.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305458.5000\n",
      "Epoch 79/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2306171.0000\n",
      "Epoch 79: loss did not improve from 2305458.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306550.5000\n",
      "Epoch 80/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2306320.7500\n",
      "Epoch 80: loss improved from 2305458.50000 to 2305438.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305438.7500\n",
      "Epoch 81/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2304703.2500\n",
      "Epoch 81: loss improved from 2305438.75000 to 2305139.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305139.7500\n",
      "Epoch 82/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2303373.2500\n",
      "Epoch 82: loss did not improve from 2305139.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305926.2500\n",
      "Epoch 83/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2303931.7500\n",
      "Epoch 83: loss improved from 2305139.75000 to 2304588.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304588.0000\n",
      "Epoch 84/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2306890.7500\n",
      "Epoch 84: loss did not improve from 2304588.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305110.5000\n",
      "Epoch 85/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2306132.5000\n",
      "Epoch 85: loss did not improve from 2304588.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304901.2500\n",
      "Epoch 86/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2305158.7500\n",
      "Epoch 86: loss improved from 2304588.00000 to 2303979.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303979.0000\n",
      "Epoch 87/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2307104.5000\n",
      "Epoch 87: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304906.7500\n",
      "Epoch 88/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2302791.0000\n",
      "Epoch 88: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304529.2500\n",
      "Epoch 89/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2304245.7500\n",
      "Epoch 89: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304454.7500\n",
      "Epoch 90/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2304997.5000\n",
      "Epoch 90: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304728.5000\n",
      "Epoch 91/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2305020.7500\n",
      "Epoch 91: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305460.7500\n",
      "Epoch 92/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2304250.2500\n",
      "Epoch 92: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304295.5000\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2303844.0000\n",
      "Epoch 93: loss improved from 2303979.00000 to 2303844.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303844.0000\n",
      "Epoch 94/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2304540.0000\n",
      "Epoch 94: loss did not improve from 2303844.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304837.0000\n",
      "Epoch 95/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2304651.5000\n",
      "Epoch 95: loss improved from 2303844.00000 to 2303542.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303542.7500\n",
      "Epoch 96/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2304884.5000\n",
      "Epoch 96: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304748.0000\n",
      "Epoch 97/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2303307.7500\n",
      "Epoch 97: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304290.2500\n",
      "Epoch 98/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2304543.0000\n",
      "Epoch 98: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304467.7500\n",
      "Epoch 99/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2303664.5000\n",
      "Epoch 99: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303848.7500\n",
      "Epoch 100/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2304794.7500\n",
      "Epoch 100: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304272.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:00:15 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 20:00:15 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmps2sh4u7w\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:00:54 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "2023/05/26 20:01:01 INFO mlflow.tracking.fluent: Experiment with name 'naivevector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2722772.0000\n",
      "Epoch 1: loss improved from inf to 2722473.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 62s 18ms/step - loss: 2722473.5000\n",
      "Epoch 2/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2585965.7500\n",
      "Epoch 2: loss improved from 2722473.50000 to 2586382.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2586382.2500\n",
      "Epoch 3/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2567833.2500\n",
      "Epoch 3: loss improved from 2586382.25000 to 2569288.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2569288.2500\n",
      "Epoch 4/100\n",
      "3165/3189 [============================>.] - ETA: 0s - loss: 2556589.2500\n",
      "Epoch 4: loss improved from 2569288.25000 to 2560930.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2560930.7500\n",
      "Epoch 5/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2555681.5000\n",
      "Epoch 5: loss improved from 2560930.75000 to 2554782.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2554782.0000\n",
      "Epoch 6/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2550899.2500\n",
      "Epoch 6: loss improved from 2554782.00000 to 2550218.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2550218.0000\n",
      "Epoch 7/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2545456.0000\n",
      "Epoch 7: loss improved from 2550218.00000 to 2546862.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2546862.0000\n",
      "Epoch 8/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2544502.2500\n",
      "Epoch 8: loss improved from 2546862.00000 to 2544213.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2544213.7500\n",
      "Epoch 9/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2540942.5000\n",
      "Epoch 9: loss improved from 2544213.75000 to 2541356.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2541356.2500\n",
      "Epoch 10/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2538108.0000\n",
      "Epoch 10: loss improved from 2541356.25000 to 2537610.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2537610.2500\n",
      "Epoch 11/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2535556.5000\n",
      "Epoch 11: loss improved from 2537610.25000 to 2536277.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2536277.7500\n",
      "Epoch 12/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2534399.7500\n",
      "Epoch 12: loss improved from 2536277.75000 to 2534993.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2534993.7500\n",
      "Epoch 13/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2534784.2500\n",
      "Epoch 13: loss improved from 2534993.75000 to 2534055.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2534055.2500\n",
      "Epoch 14/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2534833.2500\n",
      "Epoch 14: loss improved from 2534055.25000 to 2532504.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2532504.0000\n",
      "Epoch 15/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2532274.7500\n",
      "Epoch 15: loss improved from 2532504.00000 to 2532161.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2532161.7500\n",
      "Epoch 16/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2530099.5000\n",
      "Epoch 16: loss improved from 2532161.75000 to 2530412.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2530412.5000\n",
      "Epoch 17/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2529417.2500\n",
      "Epoch 17: loss improved from 2530412.50000 to 2529262.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2529262.5000\n",
      "Epoch 18/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2531274.0000\n",
      "Epoch 18: loss did not improve from 2529262.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2529534.5000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2530008.0000\n",
      "Epoch 19: loss did not improve from 2529262.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2529591.7500\n",
      "Epoch 20/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2526666.2500\n",
      "Epoch 20: loss improved from 2529262.50000 to 2528327.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2528327.7500\n",
      "Epoch 21/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2528293.5000\n",
      "Epoch 21: loss improved from 2528327.75000 to 2527234.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2527234.2500\n",
      "Epoch 22/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2527475.7500\n",
      "Epoch 22: loss improved from 2527234.25000 to 2527155.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2527155.0000\n",
      "Epoch 23/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2526973.5000\n",
      "Epoch 23: loss improved from 2527155.00000 to 2526400.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2526400.5000\n",
      "Epoch 24/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2529772.2500\n",
      "Epoch 24: loss did not improve from 2526400.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2527277.2500\n",
      "Epoch 25/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2523510.0000\n",
      "Epoch 25: loss improved from 2526400.50000 to 2525435.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525435.7500\n",
      "Epoch 26/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2524787.2500\n",
      "Epoch 26: loss improved from 2525435.75000 to 2525428.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525428.2500\n",
      "Epoch 27/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2524784.7500\n",
      "Epoch 27: loss did not improve from 2525428.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525793.5000\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2525657.5000\n",
      "Epoch 28: loss improved from 2525428.25000 to 2525119.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525119.0000\n",
      "Epoch 29/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2524597.2500\n",
      "Epoch 29: loss improved from 2525119.00000 to 2524987.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2524987.5000\n",
      "Epoch 30/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2524711.5000\n",
      "Epoch 30: loss improved from 2524987.50000 to 2524469.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2524469.0000\n",
      "Epoch 31/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2522321.0000\n",
      "Epoch 31: loss improved from 2524469.00000 to 2523055.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2523055.0000\n",
      "Epoch 32/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2527765.0000\n",
      "Epoch 32: loss did not improve from 2523055.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525414.7500\n",
      "Epoch 33/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2523394.7500\n",
      "Epoch 33: loss did not improve from 2523055.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2523284.0000\n",
      "Epoch 34/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2523745.7500\n",
      "Epoch 34: loss did not improve from 2523055.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2523303.2500\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2522539.0000\n",
      "Epoch 35: loss improved from 2523055.00000 to 2522931.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2522931.5000\n",
      "Epoch 36/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2522564.2500\n",
      "Epoch 36: loss improved from 2522931.50000 to 2522696.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2522696.2500\n",
      "Epoch 37/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2525087.7500\n",
      "Epoch 37: loss did not improve from 2522696.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2523318.5000\n",
      "Epoch 38/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2523180.0000\n",
      "Epoch 38: loss improved from 2522696.25000 to 2522554.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2522554.5000\n",
      "Epoch 39/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2521603.7500\n",
      "Epoch 39: loss improved from 2522554.50000 to 2521954.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2521954.5000\n",
      "Epoch 40/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2521967.2500\n",
      "Epoch 40: loss improved from 2521954.50000 to 2521842.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2521842.5000\n",
      "Epoch 41/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2520817.5000\n",
      "Epoch 41: loss improved from 2521842.50000 to 2521280.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2521280.5000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2520886.7500\n",
      "Epoch 42: loss improved from 2521280.50000 to 2521134.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2521134.7500\n",
      "Epoch 43/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2522616.5000\n",
      "Epoch 43: loss improved from 2521134.75000 to 2520469.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2520469.5000\n",
      "Epoch 44/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2520889.5000\n",
      "Epoch 44: loss did not improve from 2520469.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2520889.5000\n",
      "Epoch 45/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2518918.2500\n",
      "Epoch 45: loss improved from 2520469.50000 to 2519864.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519864.2500\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2518226.0000\n",
      "Epoch 46: loss did not improve from 2519864.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519880.7500\n",
      "Epoch 47/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2519744.5000\n",
      "Epoch 47: loss improved from 2519864.25000 to 2519816.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519816.7500\n",
      "Epoch 48/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2517017.2500\n",
      "Epoch 48: loss improved from 2519816.75000 to 2518696.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518696.0000\n",
      "Epoch 49/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2519213.0000\n",
      "Epoch 49: loss did not improve from 2518696.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518956.7500\n",
      "Epoch 50/100\n",
      "3166/3189 [============================>.] - ETA: 0s - loss: 2519201.7500\n",
      "Epoch 50: loss did not improve from 2518696.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519213.5000\n",
      "Epoch 51/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2518777.0000\n",
      "Epoch 51: loss improved from 2518696.00000 to 2518480.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518480.2500\n",
      "Epoch 52/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2516858.5000\n",
      "Epoch 52: loss improved from 2518480.25000 to 2518330.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518330.2500\n",
      "Epoch 53/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2517421.0000\n",
      "Epoch 53: loss improved from 2518330.25000 to 2517381.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517381.2500\n",
      "Epoch 54/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2518174.0000\n",
      "Epoch 54: loss did not improve from 2517381.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517633.5000\n",
      "Epoch 55/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2518395.5000\n",
      "Epoch 55: loss did not improve from 2517381.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518374.0000\n",
      "Epoch 56/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2515167.0000\n",
      "Epoch 56: loss improved from 2517381.25000 to 2516851.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516851.5000\n",
      "Epoch 57/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2516794.2500\n",
      "Epoch 57: loss did not improve from 2516851.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517461.5000\n",
      "Epoch 58/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2516170.2500\n",
      "Epoch 58: loss did not improve from 2516851.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517239.2500\n",
      "Epoch 59/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2516882.2500\n",
      "Epoch 59: loss improved from 2516851.50000 to 2516640.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516640.5000\n",
      "Epoch 60/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2516490.2500\n",
      "Epoch 60: loss improved from 2516640.50000 to 2515898.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515898.0000\n",
      "Epoch 61/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2517255.5000\n",
      "Epoch 61: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516445.7500\n",
      "Epoch 62/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2518946.2500\n",
      "Epoch 62: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516879.2500\n",
      "Epoch 63/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2517102.7500\n",
      "Epoch 63: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517511.0000\n",
      "Epoch 64/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2514824.7500\n",
      "Epoch 64: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516190.2500\n",
      "Epoch 65/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2514750.5000\n",
      "Epoch 65: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516200.7500\n",
      "Epoch 66/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2512032.7500\n",
      "Epoch 66: loss improved from 2515898.00000 to 2515044.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515044.7500\n",
      "Epoch 67/100\n",
      "3165/3189 [============================>.] - ETA: 0s - loss: 2514891.0000\n",
      "Epoch 67: loss did not improve from 2515044.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515052.7500\n",
      "Epoch 68/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2515565.7500\n",
      "Epoch 68: loss did not improve from 2515044.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516021.0000\n",
      "Epoch 69/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2515184.7500\n",
      "Epoch 69: loss did not improve from 2515044.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515184.7500\n",
      "Epoch 70/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2515543.2500\n",
      "Epoch 70: loss did not improve from 2515044.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515277.2500\n",
      "Epoch 71/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2514791.5000\n",
      "Epoch 71: loss improved from 2515044.75000 to 2514802.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514802.5000\n",
      "Epoch 72/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2512833.5000\n",
      "Epoch 72: loss improved from 2514802.50000 to 2514074.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514074.2500\n",
      "Epoch 73/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2516472.0000\n",
      "Epoch 73: loss improved from 2514074.25000 to 2513680.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513680.0000\n",
      "Epoch 74/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2512954.0000\n",
      "Epoch 74: loss did not improve from 2513680.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514316.0000\n",
      "Epoch 75/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2513216.2500\n",
      "Epoch 75: loss did not improve from 2513680.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513848.0000\n",
      "Epoch 76/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2515196.0000\n",
      "Epoch 76: loss improved from 2513680.00000 to 2513151.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513151.5000\n",
      "Epoch 77/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2513890.5000\n",
      "Epoch 77: loss did not improve from 2513151.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513809.5000\n",
      "Epoch 78/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2513574.7500\n",
      "Epoch 78: loss did not improve from 2513151.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513957.2500\n",
      "Epoch 79/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2511891.7500\n",
      "Epoch 79: loss improved from 2513151.50000 to 2513022.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513022.2500\n",
      "Epoch 80/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2514103.5000\n",
      "Epoch 80: loss improved from 2513022.25000 to 2511813.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511813.7500\n",
      "Epoch 81/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2513286.2500\n",
      "Epoch 81: loss did not improve from 2511813.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512658.0000\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2512891.0000\n",
      "Epoch 82: loss did not improve from 2511813.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512529.7500\n",
      "Epoch 83/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2511723.7500\n",
      "Epoch 83: loss did not improve from 2511813.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512018.5000\n",
      "Epoch 84/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2513289.0000\n",
      "Epoch 84: loss improved from 2511813.75000 to 2511581.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511581.2500\n",
      "Epoch 85/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2511965.2500\n",
      "Epoch 85: loss improved from 2511581.25000 to 2511339.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511339.0000\n",
      "Epoch 86/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2510545.2500\n",
      "Epoch 86: loss improved from 2511339.00000 to 2510966.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510966.2500\n",
      "Epoch 87/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2508315.0000\n",
      "Epoch 87: loss improved from 2510966.25000 to 2510750.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510750.2500\n",
      "Epoch 88/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2509805.5000\n",
      "Epoch 88: loss improved from 2510750.25000 to 2510658.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510658.0000\n",
      "Epoch 89/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2510441.5000\n",
      "Epoch 89: loss improved from 2510658.00000 to 2510411.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510411.2500\n",
      "Epoch 90/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2506473.7500\n",
      "Epoch 90: loss improved from 2510411.25000 to 2509874.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2509874.7500\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2510291.5000\n",
      "Epoch 91: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510629.7500\n",
      "Epoch 92/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2508956.7500\n",
      "Epoch 92: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510590.0000\n",
      "Epoch 93/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2506109.7500\n",
      "Epoch 93: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510083.0000\n",
      "Epoch 94/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2510412.5000\n",
      "Epoch 94: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2509984.5000\n",
      "Epoch 95/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2510416.5000\n",
      "Epoch 95: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510542.2500\n",
      "Epoch 96/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2510398.0000\n",
      "Epoch 96: loss improved from 2509874.75000 to 2509348.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2509348.0000\n",
      "Epoch 97/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2512928.7500\n",
      "Epoch 97: loss did not improve from 2509348.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510263.2500\n",
      "Epoch 98/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2509760.0000\n",
      "Epoch 98: loss improved from 2509348.00000 to 2508850.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2508850.7500\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2508247.7500\n",
      "Epoch 99: loss improved from 2508850.75000 to 2507456.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2507456.2500\n",
      "Epoch 100/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2506773.5000\n",
      "Epoch 100: loss did not improve from 2507456.25000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2508106.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:13:52 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 20:13:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpse3yw75e\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:14:39 INFO mlflow.tracking.fluent: Experiment with name 'dnn1vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 3:12 - loss: 25237928.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0188s vs `on_train_batch_end` time: 0.0308s). Check your callbacks.\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 3116367.5000\n",
      "Epoch 1: loss improved from inf to 3114861.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 3114861.7500\n",
      "Epoch 2/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2389506.0000\n",
      "Epoch 2: loss improved from 3114861.75000 to 2389677.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2389677.5000\n",
      "Epoch 3/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2343839.7500\n",
      "Epoch 3: loss improved from 2389677.50000 to 2343122.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2343122.7500\n",
      "Epoch 4/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2305812.2500\n",
      "Epoch 4: loss improved from 2343122.75000 to 2306677.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2306677.7500\n",
      "Epoch 5/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2328884.7500\n",
      "Epoch 5: loss did not improve from 2306677.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2328805.7500\n",
      "Epoch 6/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2305745.5000\n",
      "Epoch 6: loss improved from 2306677.75000 to 2305949.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2305949.0000\n",
      "Epoch 7/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2292381.7500\n",
      "Epoch 7: loss improved from 2305949.00000 to 2292362.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2292362.2500\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2277615.2500\n",
      "Epoch 8: loss improved from 2292362.25000 to 2276743.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2276743.7500\n",
      "Epoch 9/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2270412.5000\n",
      "Epoch 9: loss improved from 2276743.75000 to 2270754.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2270754.7500\n",
      "Epoch 10/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2276892.5000\n",
      "Epoch 10: loss did not improve from 2270754.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2275593.0000\n",
      "Epoch 11/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2240151.0000\n",
      "Epoch 11: loss improved from 2270754.75000 to 2240152.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2240152.0000\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2240827.2500\n",
      "Epoch 12: loss did not improve from 2240152.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2240827.2500\n",
      "Epoch 13/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2234795.0000\n",
      "Epoch 13: loss improved from 2240152.00000 to 2236581.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2236581.7500\n",
      "Epoch 14/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2241845.7500\n",
      "Epoch 14: loss did not improve from 2236581.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2242469.2500\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2233020.7500\n",
      "Epoch 15: loss improved from 2236581.75000 to 2233285.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2233285.7500\n",
      "Epoch 16/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2240107.2500\n",
      "Epoch 16: loss did not improve from 2233285.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2238934.5000\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2234025.7500\n",
      "Epoch 17: loss did not improve from 2233285.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2234475.7500\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2220509.7500\n",
      "Epoch 18: loss improved from 2233285.75000 to 2220509.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2220509.5000\n",
      "Epoch 19/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2217812.2500\n",
      "Epoch 19: loss improved from 2220509.50000 to 2217677.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2217677.7500\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2224279.5000\n",
      "Epoch 20: loss did not improve from 2217677.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2223277.7500\n",
      "Epoch 21/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2222628.5000\n",
      "Epoch 21: loss did not improve from 2217677.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2222904.2500\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2208561.0000\n",
      "Epoch 22: loss improved from 2217677.75000 to 2208519.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2208519.0000\n",
      "Epoch 23/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2217894.0000\n",
      "Epoch 23: loss did not improve from 2208519.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2217373.2500\n",
      "Epoch 24/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2208823.7500\n",
      "Epoch 24: loss improved from 2208519.00000 to 2208454.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2208454.5000\n",
      "Epoch 25/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2202746.0000\n",
      "Epoch 25: loss improved from 2208454.50000 to 2204225.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2204225.2500\n",
      "Epoch 26/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2203097.7500\n",
      "Epoch 26: loss improved from 2204225.25000 to 2202841.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2202841.7500\n",
      "Epoch 27/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2214963.5000\n",
      "Epoch 27: loss did not improve from 2202841.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2213591.2500\n",
      "Epoch 28/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2212599.5000\n",
      "Epoch 28: loss did not improve from 2202841.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2211329.5000\n",
      "Epoch 29/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2206546.5000\n",
      "Epoch 29: loss did not improve from 2202841.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2207447.0000\n",
      "Epoch 30/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2190346.2500\n",
      "Epoch 30: loss improved from 2202841.75000 to 2190748.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2190748.0000\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2202135.2500\n",
      "Epoch 31: loss did not improve from 2190748.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2202135.2500\n",
      "Epoch 32/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2202553.7500\n",
      "Epoch 32: loss did not improve from 2190748.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2203054.7500\n",
      "Epoch 33/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2202203.5000\n",
      "Epoch 33: loss did not improve from 2190748.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2202204.7500\n",
      "Epoch 34/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2179440.0000\n",
      "Epoch 34: loss improved from 2190748.00000 to 2179412.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2179412.5000\n",
      "Epoch 35/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2199143.5000\n",
      "Epoch 35: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2198981.5000\n",
      "Epoch 36/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2199900.7500\n",
      "Epoch 36: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2199740.2500\n",
      "Epoch 37/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2183944.5000\n",
      "Epoch 37: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2185332.7500\n",
      "Epoch 38/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2199498.2500\n",
      "Epoch 38: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2200541.0000\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2194765.5000\n",
      "Epoch 39: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2193998.7500\n",
      "Epoch 40/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2195593.0000\n",
      "Epoch 40: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2195193.7500\n",
      "Epoch 41/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2187018.5000\n",
      "Epoch 41: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2186806.0000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2186419.0000\n",
      "Epoch 42: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2184160.2500\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2189085.7500\n",
      "Epoch 43: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2189085.7500\n",
      "Epoch 44/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2187785.0000\n",
      "Epoch 44: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186543.2500\n",
      "Epoch 45/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2193007.5000\n",
      "Epoch 45: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192686.5000\n",
      "Epoch 46/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2184395.5000\n",
      "Epoch 46: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186207.5000\n",
      "Epoch 47/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2187988.5000\n",
      "Epoch 47: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2187415.2500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2192978.7500\n",
      "Epoch 48: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192961.0000\n",
      "Epoch 49/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2188859.0000\n",
      "Epoch 49: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2189294.0000\n",
      "Epoch 50/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2182340.5000\n",
      "Epoch 50: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182631.5000\n",
      "Epoch 51/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2187973.5000\n",
      "Epoch 51: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186580.7500\n",
      "Epoch 52/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2178298.2500\n",
      "Epoch 52: loss improved from 2179412.50000 to 2177704.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2177704.5000\n",
      "Epoch 53/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2185256.2500\n",
      "Epoch 53: loss did not improve from 2177704.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185647.2500\n",
      "Epoch 54/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2176563.5000\n",
      "Epoch 54: loss improved from 2177704.50000 to 2176803.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2176803.5000\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2172674.5000\n",
      "Epoch 55: loss improved from 2176803.50000 to 2172239.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2172239.2500\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2185344.0000\n",
      "Epoch 56: loss did not improve from 2172239.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185158.5000\n",
      "Epoch 57/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2180815.5000\n",
      "Epoch 57: loss did not improve from 2172239.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180913.0000\n",
      "Epoch 58/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2167078.2500\n",
      "Epoch 58: loss improved from 2172239.25000 to 2168698.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2168698.7500\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2181656.2500\n",
      "Epoch 59: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2181427.2500\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2182080.5000\n",
      "Epoch 60: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2182080.5000\n",
      "Epoch 61/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2192352.0000\n",
      "Epoch 61: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2191335.2500\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2179200.0000\n",
      "Epoch 62: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2179200.0000\n",
      "Epoch 63/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2187344.0000\n",
      "Epoch 63: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2188471.0000\n",
      "Epoch 64/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2174149.5000\n",
      "Epoch 64: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2174430.0000\n",
      "Epoch 65/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2188444.5000\n",
      "Epoch 65: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2187467.2500\n",
      "Epoch 66/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2186102.2500\n",
      "Epoch 66: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2187174.0000\n",
      "Epoch 67/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2175864.0000\n",
      "Epoch 67: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2176209.2500\n",
      "Epoch 68/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2178020.2500\n",
      "Epoch 68: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2179060.0000\n",
      "Epoch 69/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2178536.0000\n",
      "Epoch 69: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2178527.2500\n",
      "Epoch 70/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2180602.5000\n",
      "Epoch 70: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2179502.7500\n",
      "Epoch 71/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2183240.5000\n",
      "Epoch 71: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2183418.7500\n",
      "Epoch 72/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2183286.7500\n",
      "Epoch 72: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2182995.7500\n",
      "Epoch 73/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2180976.0000\n",
      "Epoch 73: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2179996.5000\n",
      "Epoch 74/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2180503.5000\n",
      "Epoch 74: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2180806.0000\n",
      "Epoch 75/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2170686.5000\n",
      "Epoch 75: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2170143.7500\n",
      "Epoch 76/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2186110.5000\n",
      "Epoch 76: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2185486.0000\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2184913.2500\n",
      "Epoch 77: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2184921.5000\n",
      "Epoch 78/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2178020.7500\n",
      "Epoch 78: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2178164.0000\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2175746.5000\n",
      "Epoch 79: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2175716.0000\n",
      "Epoch 80/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2172809.7500\n",
      "Epoch 80: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2174257.5000\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2184373.0000\n",
      "Epoch 81: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2184373.0000\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2178543.0000\n",
      "Epoch 82: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2178521.2500\n",
      "Epoch 83/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2173868.0000\n",
      "Epoch 83: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2175026.2500\n",
      "Epoch 84/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2180005.5000\n",
      "Epoch 84: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2180519.0000\n",
      "Epoch 85/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2183952.7500\n",
      "Epoch 85: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2183572.7500\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2170678.2500\n",
      "Epoch 86: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2171654.7500\n",
      "Epoch 87/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2178011.5000\n",
      "Epoch 87: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2178173.0000\n",
      "Epoch 88/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2175192.2500\n",
      "Epoch 88: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2175444.7500\n",
      "Epoch 89/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2181377.0000\n",
      "Epoch 89: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2181018.2500\n",
      "Epoch 90/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2168382.5000\n",
      "Epoch 90: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2169466.5000\n",
      "Epoch 91/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2181213.0000\n",
      "Epoch 91: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2181703.2500\n",
      "Epoch 92/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2170542.0000\n",
      "Epoch 92: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2171385.0000\n",
      "Epoch 93/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2158449.0000\n",
      "Epoch 93: loss improved from 2168698.75000 to 2157095.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2157095.0000\n",
      "Epoch 94/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2173452.5000\n",
      "Epoch 94: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2174830.7500\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2177404.7500\n",
      "Epoch 95: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2178000.7500\n",
      "Epoch 96/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2171843.2500\n",
      "Epoch 96: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2173422.0000\n",
      "Epoch 97/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2177091.5000\n",
      "Epoch 97: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2176639.7500\n",
      "Epoch 98/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2174805.0000\n",
      "Epoch 98: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2174215.2500\n",
      "Epoch 99/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2176623.2500\n",
      "Epoch 99: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2175831.7500\n",
      "Epoch 100/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2173997.2500\n",
      "Epoch 100: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2172982.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:34:59 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 20:34:59 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmps7r9jioq\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:36:14 INFO mlflow.tracking.fluent: Experiment with name 'dnn1vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 1:23 - loss: 124459152.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0092s vs `on_train_batch_end` time: 0.0214s). Check your callbacks.\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 12120683.0000\n",
      "Epoch 1: loss improved from inf to 12116690.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 12116690.0000\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2896840.5000\n",
      "Epoch 2: loss improved from 12116690.00000 to 2896840.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2896840.5000\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2641287.2500\n",
      "Epoch 3: loss improved from 2896840.50000 to 2641247.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2641247.2500\n",
      "Epoch 4/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2602502.2500\n",
      "Epoch 4: loss improved from 2641247.25000 to 2602214.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2602214.5000\n",
      "Epoch 5/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2583407.0000\n",
      "Epoch 5: loss improved from 2602214.50000 to 2583296.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2583296.2500\n",
      "Epoch 6/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2563538.0000\n",
      "Epoch 6: loss improved from 2583296.25000 to 2564046.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2564046.7500\n",
      "Epoch 7/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2554781.7500\n",
      "Epoch 7: loss improved from 2564046.75000 to 2555140.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2555140.2500\n",
      "Epoch 8/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2557326.5000\n",
      "Epoch 8: loss did not improve from 2555140.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2557842.0000\n",
      "Epoch 9/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2554835.0000\n",
      "Epoch 9: loss improved from 2555140.25000 to 2554944.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2554944.7500\n",
      "Epoch 10/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2552513.2500\n",
      "Epoch 10: loss improved from 2554944.75000 to 2552721.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2552721.0000\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2550573.7500\n",
      "Epoch 11: loss improved from 2552721.00000 to 2550294.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550294.5000\n",
      "Epoch 12/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2544873.0000\n",
      "Epoch 12: loss improved from 2550294.50000 to 2544411.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2544411.2500\n",
      "Epoch 13/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2550150.5000\n",
      "Epoch 13: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550163.0000\n",
      "Epoch 14/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2550482.5000\n",
      "Epoch 14: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550192.7500\n",
      "Epoch 15/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2547265.7500\n",
      "Epoch 15: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2547265.7500\n",
      "Epoch 16/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2546884.5000\n",
      "Epoch 16: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2546884.5000\n",
      "Epoch 17/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2548866.0000\n",
      "Epoch 17: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2548858.2500\n",
      "Epoch 18/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2550459.7500\n",
      "Epoch 18: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2549970.0000\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2541015.0000\n",
      "Epoch 19: loss improved from 2544411.25000 to 2542175.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542175.2500\n",
      "Epoch 20/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2543735.0000\n",
      "Epoch 20: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2544508.7500\n",
      "Epoch 21/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2543376.7500\n",
      "Epoch 21: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542993.2500\n",
      "Epoch 22/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2541304.0000\n",
      "Epoch 22: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543724.2500\n",
      "Epoch 23/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2543210.0000\n",
      "Epoch 23: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543637.5000\n",
      "Epoch 24/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2543198.2500\n",
      "Epoch 24: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545746.2500\n",
      "Epoch 25/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2537889.2500\n",
      "Epoch 25: loss improved from 2542175.25000 to 2537889.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537889.2500\n",
      "Epoch 26/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2541069.0000\n",
      "Epoch 26: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542186.0000\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2543958.7500\n",
      "Epoch 27: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543099.0000\n",
      "Epoch 28/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2540179.2500\n",
      "Epoch 28: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2542260.7500\n",
      "Epoch 29/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2539776.7500\n",
      "Epoch 29: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539755.0000\n",
      "Epoch 30/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2546314.5000\n",
      "Epoch 30: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545942.7500\n",
      "Epoch 31/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2545421.7500\n",
      "Epoch 31: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545172.0000\n",
      "Epoch 32/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2539523.5000\n",
      "Epoch 32: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539350.0000\n",
      "Epoch 33/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2539235.5000\n",
      "Epoch 33: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539418.7500\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2540374.5000\n",
      "Epoch 34: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540551.5000\n",
      "Epoch 35/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2543186.2500\n",
      "Epoch 35: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542569.5000\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2542450.2500\n",
      "Epoch 36: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542029.5000\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2540752.2500\n",
      "Epoch 37: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540988.7500\n",
      "Epoch 38/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2536605.5000\n",
      "Epoch 38: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2538654.2500\n",
      "Epoch 39/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2537843.2500\n",
      "Epoch 39: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539115.2500\n",
      "Epoch 40/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2538567.5000\n",
      "Epoch 40: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539366.2500\n",
      "Epoch 41/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2534054.5000\n",
      "Epoch 41: loss improved from 2537889.25000 to 2536399.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2536399.5000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2540317.0000\n",
      "Epoch 42: loss did not improve from 2536399.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539422.2500\n",
      "Epoch 43/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2539298.5000\n",
      "Epoch 43: loss did not improve from 2536399.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540481.2500\n",
      "Epoch 44/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2539064.2500\n",
      "Epoch 44: loss did not improve from 2536399.50000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2540025.5000\n",
      "Epoch 45/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2530805.2500\n",
      "Epoch 45: loss improved from 2536399.50000 to 2530491.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530491.7500\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2541222.0000\n",
      "Epoch 46: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539527.2500\n",
      "Epoch 47/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2533466.7500\n",
      "Epoch 47: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533899.0000\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2541820.5000\n",
      "Epoch 48: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2541665.0000\n",
      "Epoch 49/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2535560.2500\n",
      "Epoch 49: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534752.2500\n",
      "Epoch 50/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2539672.5000\n",
      "Epoch 50: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540001.5000\n",
      "Epoch 51/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2538582.7500\n",
      "Epoch 51: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2538850.5000\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2530526.7500\n",
      "Epoch 52: loss improved from 2530491.75000 to 2529932.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2529932.0000\n",
      "Epoch 53/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2534331.7500\n",
      "Epoch 53: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533585.7500\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2540037.2500\n",
      "Epoch 54: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539079.0000\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2534888.2500\n",
      "Epoch 55: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535211.5000\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2537379.7500\n",
      "Epoch 56: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537371.5000\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2537716.0000\n",
      "Epoch 57: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2538415.5000\n",
      "Epoch 58/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2535982.7500\n",
      "Epoch 58: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537231.0000\n",
      "Epoch 59/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2532925.7500\n",
      "Epoch 59: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532877.7500\n",
      "Epoch 60/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2536129.2500\n",
      "Epoch 60: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537519.5000\n",
      "Epoch 61/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2538296.2500\n",
      "Epoch 61: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537590.0000\n",
      "Epoch 62/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2536066.2500\n",
      "Epoch 62: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2536390.5000\n",
      "Epoch 63/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2535056.0000\n",
      "Epoch 63: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533075.2500\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2536087.7500\n",
      "Epoch 64: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2536087.7500\n",
      "Epoch 65/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2535831.2500\n",
      "Epoch 65: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535814.7500\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2537379.2500\n",
      "Epoch 66: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537379.2500\n",
      "Epoch 67/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2531078.7500\n",
      "Epoch 67: loss improved from 2529932.00000 to 2529853.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2529853.7500\n",
      "Epoch 68/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2529779.2500\n",
      "Epoch 68: loss improved from 2529853.75000 to 2529666.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2529666.0000\n",
      "Epoch 69/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2531784.2500\n",
      "Epoch 69: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531378.5000\n",
      "Epoch 70/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533859.2500\n",
      "Epoch 70: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533634.5000\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2536495.5000\n",
      "Epoch 71: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2536495.5000\n",
      "Epoch 72/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2532836.2500\n",
      "Epoch 72: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2533415.2500\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2535951.7500\n",
      "Epoch 73: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535794.5000\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2534653.0000\n",
      "Epoch 74: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2534488.2500\n",
      "Epoch 75/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2532250.0000\n",
      "Epoch 75: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2531769.2500\n",
      "Epoch 76/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2535480.7500\n",
      "Epoch 76: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2533113.0000\n",
      "Epoch 77/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2533062.7500\n",
      "Epoch 77: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532381.7500\n",
      "Epoch 78/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2534161.7500\n",
      "Epoch 78: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533730.7500\n",
      "Epoch 79/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2535099.7500\n",
      "Epoch 79: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534117.5000\n",
      "Epoch 80/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2531684.5000\n",
      "Epoch 80: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531555.0000\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2529047.0000\n",
      "Epoch 81: loss improved from 2529666.00000 to 2528561.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2528561.2500\n",
      "Epoch 82/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2528095.2500\n",
      "Epoch 82: loss improved from 2528561.25000 to 2528100.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2528100.7500\n",
      "Epoch 83/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2533397.0000\n",
      "Epoch 83: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533097.2500\n",
      "Epoch 84/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2531031.5000\n",
      "Epoch 84: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531406.5000\n",
      "Epoch 85/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2533089.0000\n",
      "Epoch 85: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2532338.2500\n",
      "Epoch 86/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2530725.5000\n",
      "Epoch 86: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532256.0000\n",
      "Epoch 87/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2533558.2500\n",
      "Epoch 87: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533733.7500\n",
      "Epoch 88/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2532143.7500\n",
      "Epoch 88: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532069.7500\n",
      "Epoch 89/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2530272.5000\n",
      "Epoch 89: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530774.0000\n",
      "Epoch 90/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2533151.0000\n",
      "Epoch 90: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533311.0000\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2532821.7500\n",
      "Epoch 91: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531932.0000\n",
      "Epoch 92/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2535044.0000\n",
      "Epoch 92: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534246.2500\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2531134.5000\n",
      "Epoch 93: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532333.2500\n",
      "Epoch 94/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2534269.2500\n",
      "Epoch 94: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533561.5000\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2526682.7500\n",
      "Epoch 95: loss improved from 2528100.75000 to 2526656.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526656.0000\n",
      "Epoch 96/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2533993.0000\n",
      "Epoch 96: loss did not improve from 2526656.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534799.0000\n",
      "Epoch 97/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2533275.7500\n",
      "Epoch 97: loss did not improve from 2526656.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535122.0000\n",
      "Epoch 98/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2534241.5000\n",
      "Epoch 98: loss did not improve from 2526656.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533524.5000\n",
      "Epoch 99/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2525821.0000\n",
      "Epoch 99: loss improved from 2526656.00000 to 2526289.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526289.2500\n",
      "Epoch 100/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2527671.5000\n",
      "Epoch 100: loss did not improve from 2526289.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2529752.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:56:22 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 20:56:22 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpocstk9yw\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:57:07 INFO mlflow.tracking.fluent: Experiment with name 'dnn2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2633882.0000\n",
      "Epoch 1: loss improved from inf to 2635471.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 5ms/step - loss: 2635471.0000\n",
      "Epoch 2/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2398270.7500\n",
      "Epoch 2: loss improved from 2635471.00000 to 2396747.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2396747.0000\n",
      "Epoch 3/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2368647.5000\n",
      "Epoch 3: loss improved from 2396747.00000 to 2367878.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2367878.0000\n",
      "Epoch 4/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2354219.2500\n",
      "Epoch 4: loss improved from 2367878.00000 to 2355703.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2355703.5000\n",
      "Epoch 5/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2346573.2500\n",
      "Epoch 5: loss improved from 2355703.50000 to 2346131.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2346131.0000\n",
      "Epoch 6/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2339683.0000\n",
      "Epoch 6: loss improved from 2346131.00000 to 2338758.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2338758.2500\n",
      "Epoch 7/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2334505.7500\n",
      "Epoch 7: loss improved from 2338758.25000 to 2333098.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2333098.5000\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2330694.7500\n",
      "Epoch 8: loss improved from 2333098.50000 to 2329934.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2329934.0000\n",
      "Epoch 9/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2325806.7500\n",
      "Epoch 9: loss improved from 2329934.00000 to 2325310.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2325310.5000\n",
      "Epoch 10/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2321642.7500\n",
      "Epoch 10: loss improved from 2325310.50000 to 2320962.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2320962.5000\n",
      "Epoch 11/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2319119.0000\n",
      "Epoch 11: loss improved from 2320962.50000 to 2318872.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2318872.7500\n",
      "Epoch 12/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2315351.7500\n",
      "Epoch 12: loss improved from 2318872.75000 to 2315235.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2315235.7500\n",
      "Epoch 13/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2314288.2500\n",
      "Epoch 13: loss improved from 2315235.75000 to 2314233.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2314233.0000\n",
      "Epoch 14/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2312455.5000\n",
      "Epoch 14: loss improved from 2314233.00000 to 2311989.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2311989.0000\n",
      "Epoch 15/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2308825.0000\n",
      "Epoch 15: loss improved from 2311989.00000 to 2308520.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2308520.7500\n",
      "Epoch 16/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2307503.2500\n",
      "Epoch 16: loss improved from 2308520.75000 to 2307393.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2307393.0000\n",
      "Epoch 17/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2306250.5000\n",
      "Epoch 17: loss improved from 2307393.00000 to 2305392.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2305392.5000\n",
      "Epoch 18/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2303366.0000\n",
      "Epoch 18: loss improved from 2305392.50000 to 2304378.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2304378.0000\n",
      "Epoch 19/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2299146.2500\n",
      "Epoch 19: loss improved from 2304378.00000 to 2300795.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2300795.0000\n",
      "Epoch 20/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2302018.7500\n",
      "Epoch 20: loss did not improve from 2300795.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2301410.2500\n",
      "Epoch 21/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2300627.5000\n",
      "Epoch 21: loss improved from 2300795.00000 to 2298646.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2298646.7500\n",
      "Epoch 22/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2296299.2500\n",
      "Epoch 22: loss improved from 2298646.75000 to 2298645.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2298645.0000\n",
      "Epoch 23/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2297722.2500\n",
      "Epoch 23: loss improved from 2298645.00000 to 2296935.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2296935.5000\n",
      "Epoch 24/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2295456.7500\n",
      "Epoch 24: loss improved from 2296935.50000 to 2295484.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2295484.2500\n",
      "Epoch 25/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2292931.0000\n",
      "Epoch 25: loss improved from 2295484.25000 to 2294161.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2294161.5000\n",
      "Epoch 26/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2291080.0000\n",
      "Epoch 26: loss improved from 2294161.50000 to 2292463.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2292463.5000\n",
      "Epoch 27/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2289703.7500\n",
      "Epoch 27: loss improved from 2292463.50000 to 2291402.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2291402.7500\n",
      "Epoch 28/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2292468.2500\n",
      "Epoch 28: loss did not improve from 2291402.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2292068.7500\n",
      "Epoch 29/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2290989.7500\n",
      "Epoch 29: loss improved from 2291402.75000 to 2290971.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2290971.7500\n",
      "Epoch 30/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2288405.0000\n",
      "Epoch 30: loss improved from 2290971.75000 to 2288543.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2288543.2500\n",
      "Epoch 31/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2291767.2500\n",
      "Epoch 31: loss did not improve from 2288543.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2290283.0000\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2288279.5000\n",
      "Epoch 32: loss improved from 2288543.25000 to 2288279.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2288279.5000\n",
      "Epoch 33/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2288796.7500\n",
      "Epoch 33: loss did not improve from 2288279.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2288406.7500\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2285637.2500\n",
      "Epoch 34: loss improved from 2288279.50000 to 2285637.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285637.2500\n",
      "Epoch 35/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2284671.0000\n",
      "Epoch 35: loss improved from 2285637.25000 to 2285373.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285373.0000\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2285428.2500\n",
      "Epoch 36: loss did not improve from 2285373.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285721.5000\n",
      "Epoch 37/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2284894.0000\n",
      "Epoch 37: loss improved from 2285373.00000 to 2283441.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2283441.0000\n",
      "Epoch 38/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2285550.5000\n",
      "Epoch 38: loss did not improve from 2283441.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2284036.2500\n",
      "Epoch 39/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2284774.2500\n",
      "Epoch 39: loss did not improve from 2283441.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2284103.7500\n",
      "Epoch 40/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2285122.7500\n",
      "Epoch 40: loss did not improve from 2283441.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285600.0000\n",
      "Epoch 41/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2283392.7500\n",
      "Epoch 41: loss improved from 2283441.00000 to 2283145.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2283145.2500\n",
      "Epoch 42/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2281241.5000\n",
      "Epoch 42: loss did not improve from 2283145.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2283522.7500\n",
      "Epoch 43/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2283041.2500\n",
      "Epoch 43: loss improved from 2283145.25000 to 2282808.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2282808.0000\n",
      "Epoch 44/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2283772.2500\n",
      "Epoch 44: loss improved from 2282808.00000 to 2282717.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2282717.0000\n",
      "Epoch 45/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2279813.2500\n",
      "Epoch 45: loss improved from 2282717.00000 to 2281592.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2281592.2500\n",
      "Epoch 46/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2279946.5000\n",
      "Epoch 46: loss improved from 2281592.25000 to 2279809.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2279809.2500\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2280282.5000\n",
      "Epoch 47: loss did not improve from 2279809.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2280282.5000\n",
      "Epoch 48/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2280597.7500\n",
      "Epoch 48: loss did not improve from 2279809.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2279999.0000\n",
      "Epoch 49/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2278970.7500\n",
      "Epoch 49: loss improved from 2279809.25000 to 2278450.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2278450.7500\n",
      "Epoch 50/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2277661.2500\n",
      "Epoch 50: loss improved from 2278450.75000 to 2278334.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2278334.7500\n",
      "Epoch 51/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2277792.2500\n",
      "Epoch 51: loss did not improve from 2278334.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2279182.0000\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2278900.0000\n",
      "Epoch 52: loss did not improve from 2278334.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278738.5000\n",
      "Epoch 53/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2277761.5000\n",
      "Epoch 53: loss improved from 2278334.75000 to 2278180.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2278180.0000\n",
      "Epoch 54/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2280972.2500\n",
      "Epoch 54: loss did not improve from 2278180.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2278350.0000\n",
      "Epoch 55/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2279078.7500\n",
      "Epoch 55: loss did not improve from 2278180.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2278211.2500\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2276097.7500\n",
      "Epoch 56: loss improved from 2278180.00000 to 2277133.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2277133.0000\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2274511.2500\n",
      "Epoch 57: loss improved from 2277133.00000 to 2274511.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2274511.2500\n",
      "Epoch 58/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2278216.0000\n",
      "Epoch 58: loss did not improve from 2274511.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2277306.2500\n",
      "Epoch 59/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2275222.2500\n",
      "Epoch 59: loss did not improve from 2274511.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2275531.0000\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2275599.5000\n",
      "Epoch 60: loss did not improve from 2274511.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2275786.2500\n",
      "Epoch 61/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2273569.7500\n",
      "Epoch 61: loss improved from 2274511.25000 to 2273741.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273741.0000\n",
      "Epoch 62/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2276052.0000\n",
      "Epoch 62: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2275554.7500\n",
      "Epoch 63/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2276251.2500\n",
      "Epoch 63: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2274756.0000\n",
      "Epoch 64/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2273025.5000\n",
      "Epoch 64: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273875.0000\n",
      "Epoch 65/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2275852.7500\n",
      "Epoch 65: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2275430.5000\n",
      "Epoch 66/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2273435.0000\n",
      "Epoch 66: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2273843.2500\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2272648.7500\n",
      "Epoch 67: loss improved from 2273741.00000 to 2271974.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271974.2500\n",
      "Epoch 68/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2274732.2500\n",
      "Epoch 68: loss did not improve from 2271974.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2272814.0000\n",
      "Epoch 69/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2271918.7500\n",
      "Epoch 69: loss did not improve from 2271974.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273238.0000\n",
      "Epoch 70/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2272706.2500\n",
      "Epoch 70: loss did not improve from 2271974.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2272616.0000\n",
      "Epoch 71/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2274124.7500\n",
      "Epoch 71: loss did not improve from 2271974.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2273407.7500\n",
      "Epoch 72/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2272333.5000\n",
      "Epoch 72: loss improved from 2271974.25000 to 2271594.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271594.7500\n",
      "Epoch 73/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2273740.2500\n",
      "Epoch 73: loss did not improve from 2271594.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2272184.0000\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2271909.7500\n",
      "Epoch 74: loss did not improve from 2271594.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271909.7500\n",
      "Epoch 75/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2271794.7500\n",
      "Epoch 75: loss did not improve from 2271594.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2271852.5000\n",
      "Epoch 76/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2269669.2500\n",
      "Epoch 76: loss did not improve from 2271594.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271896.7500\n",
      "Epoch 77/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2272002.2500\n",
      "Epoch 77: loss improved from 2271594.75000 to 2270983.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2270983.7500\n",
      "Epoch 78/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2269450.2500\n",
      "Epoch 78: loss improved from 2270983.75000 to 2269539.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2269539.7500\n",
      "Epoch 79/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2271268.5000\n",
      "Epoch 79: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2270258.7500\n",
      "Epoch 80/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2270772.0000\n",
      "Epoch 80: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2270063.2500\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2269832.0000\n",
      "Epoch 81: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2269832.0000\n",
      "Epoch 82/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2270705.5000\n",
      "Epoch 82: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2269790.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2270930.2500\n",
      "Epoch 83: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2270257.0000\n",
      "Epoch 84/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2266878.2500\n",
      "Epoch 84: loss improved from 2269539.75000 to 2268292.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2268292.2500\n",
      "Epoch 85/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2269389.2500\n",
      "Epoch 85: loss did not improve from 2268292.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2269375.0000\n",
      "Epoch 86/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2269458.2500\n",
      "Epoch 86: loss did not improve from 2268292.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2269348.5000\n",
      "Epoch 87/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2269142.0000\n",
      "Epoch 87: loss improved from 2268292.25000 to 2268007.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2268007.2500\n",
      "Epoch 88/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2271281.7500\n",
      "Epoch 88: loss did not improve from 2268007.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2269995.5000\n",
      "Epoch 89/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2268491.7500\n",
      "Epoch 89: loss improved from 2268007.25000 to 2267922.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2267922.0000\n",
      "Epoch 90/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2267615.5000\n",
      "Epoch 90: loss did not improve from 2267922.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2269194.0000\n",
      "Epoch 91/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2269110.7500\n",
      "Epoch 91: loss did not improve from 2267922.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2268487.0000\n",
      "Epoch 92/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2268369.7500\n",
      "Epoch 92: loss did not improve from 2267922.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2268914.0000\n",
      "Epoch 93/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2269632.7500\n",
      "Epoch 93: loss did not improve from 2267922.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268345.0000\n",
      "Epoch 94/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2266565.0000\n",
      "Epoch 94: loss improved from 2267922.00000 to 2267308.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2267308.7500\n",
      "Epoch 95/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2268589.5000\n",
      "Epoch 95: loss did not improve from 2267308.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2268498.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2266532.7500\n",
      "Epoch 96: loss improved from 2267308.75000 to 2266532.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2266532.7500\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2267488.7500\n",
      "Epoch 97: loss did not improve from 2266532.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2267488.7500\n",
      "Epoch 98/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2267113.5000\n",
      "Epoch 98: loss did not improve from 2266532.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2266820.5000\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2267313.0000\n",
      "Epoch 99: loss did not improve from 2266532.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2267337.0000\n",
      "Epoch 100/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2267190.0000\n",
      "Epoch 100: loss did not improve from 2266532.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2266787.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 21:16:20 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 21:16:20 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_okbdfa6\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 21:16:53 INFO mlflow.tracking.fluent: Experiment with name 'dnn2vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2645581.7500\n",
      "Epoch 1: loss improved from inf to 2645760.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2645760.0000\n",
      "Epoch 2/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2400213.0000\n",
      "Epoch 2: loss improved from 2645760.00000 to 2400359.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2400359.5000\n",
      "Epoch 3/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2368283.0000\n",
      "Epoch 3: loss improved from 2400359.50000 to 2368649.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2368649.5000\n",
      "Epoch 4/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2357101.7500\n",
      "Epoch 4: loss improved from 2368649.50000 to 2356814.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2356814.7500\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2346609.2500\n",
      "Epoch 5: loss improved from 2356814.75000 to 2346301.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2346301.5000\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2339584.0000\n",
      "Epoch 6: loss improved from 2346301.50000 to 2339584.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2339584.0000\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2332570.0000\n",
      "Epoch 7: loss improved from 2339584.00000 to 2332570.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2332570.0000\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2329626.5000\n",
      "Epoch 8: loss improved from 2332570.00000 to 2329201.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2329201.7500\n",
      "Epoch 9/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2322918.0000\n",
      "Epoch 9: loss improved from 2329201.75000 to 2324004.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2324004.7500\n",
      "Epoch 10/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2319944.0000\n",
      "Epoch 10: loss improved from 2324004.75000 to 2319830.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2319830.5000\n",
      "Epoch 11/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2319237.0000\n",
      "Epoch 11: loss improved from 2319830.50000 to 2318743.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2318743.7500\n",
      "Epoch 12/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2317261.2500\n",
      "Epoch 12: loss improved from 2318743.75000 to 2315727.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2315727.2500\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2310332.7500\n",
      "Epoch 13: loss improved from 2315727.25000 to 2310332.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2310332.7500\n",
      "Epoch 14/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2313438.7500\n",
      "Epoch 14: loss did not improve from 2310332.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2312906.2500\n",
      "Epoch 15/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2309765.5000\n",
      "Epoch 15: loss improved from 2310332.75000 to 2309442.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2309442.0000\n",
      "Epoch 16/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2306582.2500\n",
      "Epoch 16: loss improved from 2309442.00000 to 2305531.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2305531.5000\n",
      "Epoch 17/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2306386.2500\n",
      "Epoch 17: loss improved from 2305531.50000 to 2305517.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2305517.7500\n",
      "Epoch 18/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2304106.0000\n",
      "Epoch 18: loss improved from 2305517.75000 to 2302980.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2302980.2500\n",
      "Epoch 19/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2303695.0000\n",
      "Epoch 19: loss improved from 2302980.25000 to 2302705.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2302705.2500\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2300536.7500\n",
      "Epoch 20: loss improved from 2302705.25000 to 2299935.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2299935.7500\n",
      "Epoch 21/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2299573.0000\n",
      "Epoch 21: loss improved from 2299935.75000 to 2299547.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2299547.2500\n",
      "Epoch 22/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2298933.7500\n",
      "Epoch 22: loss did not improve from 2299547.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2299797.5000\n",
      "Epoch 23/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2298049.0000\n",
      "Epoch 23: loss improved from 2299547.25000 to 2297364.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2297364.5000\n",
      "Epoch 24/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2295872.0000\n",
      "Epoch 24: loss improved from 2297364.50000 to 2295755.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2295755.2500\n",
      "Epoch 25/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2295252.5000\n",
      "Epoch 25: loss improved from 2295755.25000 to 2294586.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2294586.5000\n",
      "Epoch 26/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2295299.5000\n",
      "Epoch 26: loss improved from 2294586.50000 to 2294082.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2294082.5000\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2293244.0000\n",
      "Epoch 27: loss improved from 2294082.50000 to 2293944.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2293944.7500\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2292270.0000\n",
      "Epoch 28: loss improved from 2293944.75000 to 2291789.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2291789.7500\n",
      "Epoch 29/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2291282.2500\n",
      "Epoch 29: loss improved from 2291789.75000 to 2291558.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2291558.5000\n",
      "Epoch 30/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2292765.2500\n",
      "Epoch 30: loss did not improve from 2291558.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2292115.5000\n",
      "Epoch 31/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2288753.0000\n",
      "Epoch 31: loss improved from 2291558.50000 to 2288095.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2288095.7500\n",
      "Epoch 32/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2287613.5000\n",
      "Epoch 32: loss did not improve from 2288095.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2288992.7500\n",
      "Epoch 33/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2287444.5000\n",
      "Epoch 33: loss improved from 2288095.75000 to 2287347.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2287347.0000\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2286793.0000\n",
      "Epoch 34: loss improved from 2287347.00000 to 2286743.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2286743.7500\n",
      "Epoch 35/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2284347.7500\n",
      "Epoch 35: loss improved from 2286743.75000 to 2285348.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285348.7500\n",
      "Epoch 36/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2286483.0000\n",
      "Epoch 36: loss did not improve from 2285348.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2286471.0000\n",
      "Epoch 37/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2283362.7500\n",
      "Epoch 37: loss improved from 2285348.75000 to 2283759.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2283759.7500\n",
      "Epoch 38/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2285024.7500\n",
      "Epoch 38: loss did not improve from 2283759.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2284687.5000\n",
      "Epoch 39/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2282314.7500\n",
      "Epoch 39: loss improved from 2283759.75000 to 2282611.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2282611.2500\n",
      "Epoch 40/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2284768.2500\n",
      "Epoch 40: loss did not improve from 2282611.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2284599.7500\n",
      "Epoch 41/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2283117.5000\n",
      "Epoch 41: loss did not improve from 2282611.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2283188.0000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2283790.0000\n",
      "Epoch 42: loss did not improve from 2282611.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2282726.5000\n",
      "Epoch 43/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2280445.2500\n",
      "Epoch 43: loss improved from 2282611.25000 to 2280891.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2280891.2500\n",
      "Epoch 44/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2280227.2500\n",
      "Epoch 44: loss improved from 2280891.25000 to 2280270.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2280270.2500\n",
      "Epoch 45/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2279306.0000\n",
      "Epoch 45: loss improved from 2280270.25000 to 2279893.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2279893.0000\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2279894.2500\n",
      "Epoch 46: loss did not improve from 2279893.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2280298.0000\n",
      "Epoch 47/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2278948.5000\n",
      "Epoch 47: loss improved from 2279893.00000 to 2278727.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2278727.0000\n",
      "Epoch 48/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2279877.5000\n",
      "Epoch 48: loss did not improve from 2278727.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2279851.0000\n",
      "Epoch 49/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2279470.5000\n",
      "Epoch 49: loss improved from 2278727.00000 to 2278365.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2278365.0000\n",
      "Epoch 50/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2276905.2500\n",
      "Epoch 50: loss improved from 2278365.00000 to 2278344.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2278344.0000\n",
      "Epoch 51/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2279330.2500\n",
      "Epoch 51: loss did not improve from 2278344.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2278576.7500\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2279425.0000\n",
      "Epoch 52: loss did not improve from 2278344.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2278945.5000\n",
      "Epoch 53/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2278566.0000\n",
      "Epoch 53: loss improved from 2278344.00000 to 2277824.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2277824.0000\n",
      "Epoch 54/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2276741.2500\n",
      "Epoch 54: loss improved from 2277824.00000 to 2276823.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2276823.0000\n",
      "Epoch 55/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2278273.0000\n",
      "Epoch 55: loss did not improve from 2276823.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2277484.5000\n",
      "Epoch 56/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2276941.7500\n",
      "Epoch 56: loss improved from 2276823.00000 to 2276572.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2276572.5000\n",
      "Epoch 57/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2276221.2500\n",
      "Epoch 57: loss improved from 2276572.50000 to 2275814.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2275814.2500\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2276327.5000\n",
      "Epoch 58: loss did not improve from 2275814.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2276327.5000\n",
      "Epoch 59/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2275082.2500\n",
      "Epoch 59: loss improved from 2275814.25000 to 2273982.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2273982.7500\n",
      "Epoch 60/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2276369.2500\n",
      "Epoch 60: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2275938.5000\n",
      "Epoch 61/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2274036.5000\n",
      "Epoch 61: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2274488.0000\n",
      "Epoch 62/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2275049.0000\n",
      "Epoch 62: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2274910.5000\n",
      "Epoch 63/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2274315.7500\n",
      "Epoch 63: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2275171.0000\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2274760.2500\n",
      "Epoch 64: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2274760.2500\n",
      "Epoch 65/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2273830.7500\n",
      "Epoch 65: loss improved from 2273982.75000 to 2273830.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273830.7500\n",
      "Epoch 66/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2273736.5000\n",
      "Epoch 66: loss did not improve from 2273830.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273956.5000\n",
      "Epoch 67/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2271836.7500\n",
      "Epoch 67: loss improved from 2273830.75000 to 2271836.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2271836.7500\n",
      "Epoch 68/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2273616.7500\n",
      "Epoch 68: loss did not improve from 2271836.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2273862.0000\n",
      "Epoch 69/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2271580.7500\n",
      "Epoch 69: loss did not improve from 2271836.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2272092.0000\n",
      "Epoch 70/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2270828.5000\n",
      "Epoch 70: loss improved from 2271836.75000 to 2270497.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2270497.7500\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2271861.7500\n",
      "Epoch 71: loss did not improve from 2270497.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271810.7500\n",
      "Epoch 72/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2273444.5000\n",
      "Epoch 72: loss did not improve from 2270497.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2272644.7500\n",
      "Epoch 73/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2271182.2500\n",
      "Epoch 73: loss did not improve from 2270497.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271935.5000\n",
      "Epoch 74/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2269609.2500\n",
      "Epoch 74: loss did not improve from 2270497.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271608.5000\n",
      "Epoch 75/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2265405.7500\n",
      "Epoch 75: loss improved from 2270497.75000 to 2270370.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2270370.5000\n",
      "Epoch 76/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2270036.7500\n",
      "Epoch 76: loss did not improve from 2270370.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2271592.5000\n",
      "Epoch 77/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2270001.7500\n",
      "Epoch 77: loss did not improve from 2270370.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2270530.5000\n",
      "Epoch 78/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2269865.0000\n",
      "Epoch 78: loss improved from 2270370.50000 to 2269874.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2269874.7500\n",
      "Epoch 79/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2271083.0000\n",
      "Epoch 79: loss did not improve from 2269874.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2270403.7500\n",
      "Epoch 80/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2270276.5000\n",
      "Epoch 80: loss improved from 2269874.75000 to 2269729.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2269729.5000\n",
      "Epoch 81/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2270292.2500\n",
      "Epoch 81: loss did not improve from 2269729.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2269781.7500\n",
      "Epoch 82/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2267466.2500\n",
      "Epoch 82: loss improved from 2269729.50000 to 2268590.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2268590.2500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2269210.5000\n",
      "Epoch 83: loss did not improve from 2268590.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269589.7500\n",
      "Epoch 84/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2271159.7500\n",
      "Epoch 84: loss did not improve from 2268590.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270608.0000\n",
      "Epoch 85/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2270096.7500\n",
      "Epoch 85: loss did not improve from 2268590.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269009.2500\n",
      "Epoch 86/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2268498.2500\n",
      "Epoch 86: loss did not improve from 2268590.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269357.2500\n",
      "Epoch 87/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2268273.7500\n",
      "Epoch 87: loss improved from 2268590.25000 to 2268481.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268481.5000\n",
      "Epoch 88/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2267695.5000\n",
      "Epoch 88: loss did not improve from 2268481.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269072.5000\n",
      "Epoch 89/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2267895.0000\n",
      "Epoch 89: loss improved from 2268481.50000 to 2267288.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267288.5000\n",
      "Epoch 90/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2268530.7500\n",
      "Epoch 90: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268226.5000\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2268464.0000\n",
      "Epoch 91: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268464.0000\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2267702.5000\n",
      "Epoch 92: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267666.0000\n",
      "Epoch 93/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2268370.0000\n",
      "Epoch 93: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267728.7500\n",
      "Epoch 94/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2268347.5000\n",
      "Epoch 94: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268810.2500\n",
      "Epoch 95/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2269824.0000\n",
      "Epoch 95: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267892.5000\n",
      "Epoch 96/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2267496.2500\n",
      "Epoch 96: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267792.2500\n",
      "Epoch 97/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2267814.5000\n",
      "Epoch 97: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268282.0000\n",
      "Epoch 98/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2268472.2500\n",
      "Epoch 98: loss improved from 2267288.50000 to 2267237.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267237.0000\n",
      "Epoch 99/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2266674.0000\n",
      "Epoch 99: loss improved from 2267237.00000 to 2266091.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2266091.0000\n",
      "Epoch 100/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2265527.0000\n",
      "Epoch 100: loss did not improve from 2266091.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266925.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 21:35:19 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 21:35:19 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp8_xk00mc\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 21:39:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp8_xk00mc\\model, flavor: tensorflow), fall back to return ['tensorflow==2.9.0']. Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/26 21:39:42 INFO mlflow.tracking.fluent: Experiment with name 'cnn1vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 25, 25, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 12, 12, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736766.5000\n",
      "Epoch 1: loss improved from inf to 2736991.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 206s 14ms/step - loss: 2736991.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2737436.7500\n",
      "Epoch 2: loss improved from 2736991.50000 to 2736504.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736504.5000\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2734603.0000\n",
      "Epoch 3: loss did not improve from 2736504.50000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736505.2500\n",
      "Epoch 4/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736733.7500\n",
      "Epoch 4: loss improved from 2736504.50000 to 2736502.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.5000\n",
      "Epoch 5/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736612.7500\n",
      "Epoch 5: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736505.0000\n",
      "Epoch 6/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2735858.5000\n",
      "Epoch 6: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736506.2500\n",
      "Epoch 7/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2738445.0000\n",
      "Epoch 7: loss improved from 2736502.50000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.2500\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737184.2500\n",
      "Epoch 8: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.7500\n",
      "Epoch 9/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 9: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736504.0000\n",
      "Epoch 10/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 10: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.5000\n",
      "Epoch 11/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735303.2500\n",
      "Epoch 11: loss improved from 2736502.25000 to 2736500.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736500.7500\n",
      "Epoch 12/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736135.5000\n",
      "Epoch 12: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736501.2500\n",
      "Epoch 13/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737211.2500\n",
      "Epoch 13: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.2500\n",
      "Epoch 14/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736218.7500\n",
      "Epoch 14: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.7500\n",
      "Epoch 15/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736961.5000\n",
      "Epoch 15: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.5000\n",
      "Epoch 16/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2734565.0000\n",
      "Epoch 16: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736501.2500\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736825.5000\n",
      "Epoch 17: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.7500\n",
      "Epoch 18/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735272.0000\n",
      "Epoch 18: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736500.7500\n",
      "Epoch 19/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736732.7500\n",
      "Epoch 19: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.7500\n",
      "Epoch 20/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737526.0000\n",
      "Epoch 20: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736502.2500\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.5000\n",
      "Epoch 21: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736501.5000\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735358.5000\n",
      "Epoch 22: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.7500\n",
      "Epoch 23/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737107.7500\n",
      "Epoch 23: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.5000\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737957.0000\n",
      "Epoch 24: loss improved from 2736500.75000 to 2736499.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736499.2500\n",
      "Epoch 25/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2733744.5000\n",
      "Epoch 25: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.5000\n",
      "Epoch 26/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736697.2500\n",
      "Epoch 26: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736501.5000\n",
      "Epoch 27/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736757.2500\n",
      "Epoch 27: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.5000\n",
      "Epoch 28/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735465.2500\n",
      "Epoch 28: loss improved from 2736499.25000 to 2736499.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736499.0000\n",
      "Epoch 29/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736033.5000\n",
      "Epoch 29: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736500.2500\n",
      "Epoch 30/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735614.0000\n",
      "Epoch 30: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736503.5000\n",
      "Epoch 31/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736855.2500\n",
      "Epoch 31: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.7500\n",
      "Epoch 32/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736439.0000\n",
      "Epoch 32: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736502.5000\n",
      "Epoch 33/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737418.7500\n",
      "Epoch 33: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.5000\n",
      "Epoch 34/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737853.2500\n",
      "Epoch 34: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2736500.0000\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736110.2500\n",
      "Epoch 35: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2736502.2500\n",
      "Epoch 36/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736844.7500\n",
      "Epoch 36: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736499.2500\n",
      "Epoch 37/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735962.7500\n",
      "Epoch 37: loss improved from 2736499.00000 to 2736498.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736498.2500\n",
      "Epoch 38/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736989.0000\n",
      "Epoch 38: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.0000\n",
      "Epoch 39/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735999.5000\n",
      "Epoch 39: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736502.2500\n",
      "Epoch 40/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737375.2500\n",
      "Epoch 40: loss improved from 2736498.25000 to 2736498.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736498.0000\n",
      "Epoch 41/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736828.0000\n",
      "Epoch 41: loss did not improve from 2736498.00000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.7500\n",
      "Epoch 42/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736773.5000\n",
      "Epoch 42: loss did not improve from 2736498.00000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736499.0000\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736954.0000\n",
      "Epoch 43: loss did not improve from 2736498.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736498.0000\n",
      "Epoch 44/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735662.7500\n",
      "Epoch 44: loss improved from 2736498.00000 to 2736497.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.7500\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736329.5000\n",
      "Epoch 45: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736498.2500\n",
      "Epoch 46/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736675.0000\n",
      "Epoch 46: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736500.7500\n",
      "Epoch 47/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2732057.5000\n",
      "Epoch 47: loss improved from 2736497.75000 to 2736496.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736496.7500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736757.7500\n",
      "Epoch 48: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736497.2500\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736497.2500\n",
      "Epoch 49: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.2500\n",
      "Epoch 50/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2738377.0000\n",
      "Epoch 50: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736500.2500\n",
      "Epoch 51/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735647.0000\n",
      "Epoch 51: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736499.2500\n",
      "Epoch 52/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736661.5000\n",
      "Epoch 52: loss improved from 2736496.75000 to 2736496.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736496.0000\n",
      "Epoch 53/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2734570.5000\n",
      "Epoch 53: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736497.7500\n",
      "Epoch 54/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736037.7500\n",
      "Epoch 54: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.7500\n",
      "Epoch 55/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736683.5000\n",
      "Epoch 55: loss improved from 2736496.00000 to 2736495.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736495.0000\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736405.5000\n",
      "Epoch 56: loss improved from 2736495.00000 to 2736492.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736492.7500\n",
      "Epoch 57/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735056.2500\n",
      "Epoch 57: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736496.0000\n",
      "Epoch 58/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736914.0000\n",
      "Epoch 58: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.7500\n",
      "Epoch 59/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2734132.2500\n",
      "Epoch 59: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.2500\n",
      "Epoch 60/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736407.2500\n",
      "Epoch 60: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736497.2500\n",
      "Epoch 61/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737259.5000\n",
      "Epoch 61: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.2500\n",
      "Epoch 62/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737449.7500\n",
      "Epoch 62: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736497.0000\n",
      "Epoch 63/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736497.2500\n",
      "Epoch 63: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.2500\n",
      "Epoch 64/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2737977.5000\n",
      "Epoch 64: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736494.7500\n",
      "Epoch 65/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735915.2500\n",
      "Epoch 65: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.0000\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737066.5000\n",
      "Epoch 66: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736886.2500\n",
      "Epoch 67: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.0000\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736145.2500\n",
      "Epoch 68: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 69/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737335.7500\n",
      "Epoch 69: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.0000\n",
      "Epoch 70/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737083.2500\n",
      "Epoch 70: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.0000\n",
      "Epoch 71/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737011.0000\n",
      "Epoch 71: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 72/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2735739.5000\n",
      "Epoch 72: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 73/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2738035.5000\n",
      "Epoch 73: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.5000\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737544.0000\n",
      "Epoch 74: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.7500\n",
      "Epoch 75/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2734051.7500\n",
      "Epoch 75: loss improved from 2736492.75000 to 2736492.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735564.0000\n",
      "Epoch 76: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 77/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2735283.7500\n",
      "Epoch 77: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 78/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736612.5000\n",
      "Epoch 78: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.7500\n",
      "Epoch 79/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2735346.2500\n",
      "Epoch 79: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.0000\n",
      "Epoch 80/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736455.2500\n",
      "Epoch 80: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736495.5000\n",
      "Epoch 81: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.5000\n",
      "Epoch 82/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736430.0000\n",
      "Epoch 82: loss improved from 2736492.25000 to 2736490.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.5000\n",
      "Epoch 83/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736491.2500\n",
      "Epoch 83: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.7500\n",
      "Epoch 84/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2735652.5000\n",
      "Epoch 84: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 85/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736491.2500\n",
      "Epoch 85: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.2500\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 86: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 87/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735152.5000\n",
      "Epoch 87: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.0000\n",
      "Epoch 88/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736010.7500\n",
      "Epoch 88: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.5000\n",
      "Epoch 89/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737226.2500\n",
      "Epoch 89: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.0000\n",
      "Epoch 90/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2735185.5000\n",
      "Epoch 90: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736387.5000\n",
      "Epoch 91: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 92/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736652.0000\n",
      "Epoch 92: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737311.2500\n",
      "Epoch 93: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 94/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736653.7500\n",
      "Epoch 94: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 95/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735650.2500\n",
      "Epoch 95: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 96/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737971.5000\n",
      "Epoch 96: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.5000\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736490.0000\n",
      "Epoch 97: loss improved from 2736490.50000 to 2736490.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.0000\n",
      "Epoch 98/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736342.0000\n",
      "Epoch 98: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.5000\n",
      "Epoch 99/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737115.2500\n",
      "Epoch 99: loss improved from 2736490.00000 to 2736489.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736489.5000\n",
      "Epoch 100/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737174.2500\n",
      "Epoch 100: loss improved from 2736489.50000 to 2736489.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736489.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 22:13:11 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 22:13:11 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7f4iasyi\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7f4iasyi\\model\\data\\model\\assets\n",
      "2023/05/26 22:14:01 INFO mlflow.tracking.fluent: Experiment with name 'cnn1vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 25, 25, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSampling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_4 (UpSampling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   3/3189 [..............................] - ETA: 6:00 - loss: 1797291.0000  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736875.5000\n",
      "Epoch 1: loss improved from inf to 2736508.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736508.5000\n",
      "Epoch 2/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736613.0000\n",
      "Epoch 2: loss improved from 2736508.50000 to 2736507.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736507.0000\n",
      "Epoch 3/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2735947.2500\n",
      "Epoch 3: loss did not improve from 2736507.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736509.2500\n",
      "Epoch 4/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737578.5000\n",
      "Epoch 4: loss improved from 2736507.00000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.0000\n",
      "Epoch 5/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737905.5000\n",
      "Epoch 5: loss improved from 2736503.00000 to 2736502.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.5000\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.5000\n",
      "Epoch 6: loss improved from 2736502.50000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 7/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737234.0000\n",
      "Epoch 7: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736507.2500\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2702161.0000\n",
      "Epoch 8: loss improved from 2736501.50000 to 2703252.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2703252.7500\n",
      "Epoch 9/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2544300.2500\n",
      "Epoch 9: loss improved from 2703252.75000 to 2544025.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2544025.0000\n",
      "Epoch 10/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2391632.7500\n",
      "Epoch 10: loss improved from 2544025.00000 to 2391069.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2391069.7500\n",
      "Epoch 11/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2280872.2500\n",
      "Epoch 11: loss improved from 2391069.75000 to 2280697.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2280697.7500\n",
      "Epoch 12/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2195186.0000\n",
      "Epoch 12: loss improved from 2280697.75000 to 2194244.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2194244.2500\n",
      "Epoch 13/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2123835.7500\n",
      "Epoch 13: loss improved from 2194244.25000 to 2124353.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2124353.2500\n",
      "Epoch 14/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2076272.0000\n",
      "Epoch 14: loss improved from 2124353.25000 to 2075844.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2075844.6250\n",
      "Epoch 15/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2024277.2500\n",
      "Epoch 15: loss improved from 2075844.62500 to 2024038.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2024038.0000\n",
      "Epoch 16/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1984109.6250\n",
      "Epoch 16: loss improved from 2024038.00000 to 1983486.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1983486.3750\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1951336.5000\n",
      "Epoch 17: loss improved from 1983486.37500 to 1951176.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1951176.5000\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1924460.5000\n",
      "Epoch 18: loss improved from 1951176.50000 to 1924433.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1924433.7500\n",
      "Epoch 19/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1896129.0000\n",
      "Epoch 19: loss improved from 1924433.75000 to 1895131.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1895131.7500\n",
      "Epoch 20/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1868498.6250\n",
      "Epoch 20: loss improved from 1895131.75000 to 1868498.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 1868498.6250\n",
      "Epoch 21/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1848674.8750\n",
      "Epoch 21: loss improved from 1868498.62500 to 1848553.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1848553.3750\n",
      "Epoch 22/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1826879.3750\n",
      "Epoch 22: loss improved from 1848553.37500 to 1827353.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1827353.7500\n",
      "Epoch 23/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1806971.7500\n",
      "Epoch 23: loss improved from 1827353.75000 to 1806971.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1806971.7500\n",
      "Epoch 24/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1794231.6250\n",
      "Epoch 24: loss improved from 1806971.75000 to 1794087.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1794087.0000\n",
      "Epoch 25/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1773664.6250\n",
      "Epoch 25: loss improved from 1794087.00000 to 1773109.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1773109.7500\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1758569.3750\n",
      "Epoch 26: loss improved from 1773109.75000 to 1758569.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1758569.3750\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1743945.2500\n",
      "Epoch 27: loss improved from 1758569.37500 to 1745145.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1745145.5000\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1733679.6250\n",
      "Epoch 28: loss improved from 1745145.50000 to 1733679.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1733679.6250\n",
      "Epoch 29/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1720667.8750\n",
      "Epoch 29: loss improved from 1733679.62500 to 1720661.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1720661.3750\n",
      "Epoch 30/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1711864.7500\n",
      "Epoch 30: loss improved from 1720661.37500 to 1712243.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1712243.5000\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1697659.1250\n",
      "Epoch 31: loss improved from 1712243.50000 to 1697659.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1697659.1250\n",
      "Epoch 32/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1688557.0000\n",
      "Epoch 32: loss improved from 1697659.12500 to 1688670.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1688670.8750\n",
      "Epoch 33/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1686408.7500\n",
      "Epoch 33: loss improved from 1688670.87500 to 1685643.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1685643.6250\n",
      "Epoch 34/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1662932.0000\n",
      "Epoch 34: loss improved from 1685643.62500 to 1662951.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1662951.2500\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1662989.6250\n",
      "Epoch 35: loss did not improve from 1662951.25000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1662989.6250\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1653388.6250\n",
      "Epoch 36: loss improved from 1662951.25000 to 1653995.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1653995.7500\n",
      "Epoch 37/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1647153.0000\n",
      "Epoch 37: loss improved from 1653995.75000 to 1646347.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1646347.7500\n",
      "Epoch 38/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1637545.6250\n",
      "Epoch 38: loss improved from 1646347.75000 to 1637332.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1637332.3750\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1629795.1250\n",
      "Epoch 39: loss improved from 1637332.37500 to 1629795.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1629795.1250\n",
      "Epoch 40/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1624545.3750\n",
      "Epoch 40: loss improved from 1629795.12500 to 1624750.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1624750.0000\n",
      "Epoch 41/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1616648.8750\n",
      "Epoch 41: loss improved from 1624750.00000 to 1616941.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1616941.6250\n",
      "Epoch 42/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1610869.6250\n",
      "Epoch 42: loss improved from 1616941.62500 to 1610644.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1610644.7500\n",
      "Epoch 43/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1600616.0000\n",
      "Epoch 43: loss improved from 1610644.75000 to 1600779.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1600779.8750\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1599234.5000\n",
      "Epoch 44: loss improved from 1600779.87500 to 1598528.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1598528.0000\n",
      "Epoch 45/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1590304.6250\n",
      "Epoch 45: loss improved from 1598528.00000 to 1590419.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1590419.2500\n",
      "Epoch 46/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1587923.7500\n",
      "Epoch 46: loss improved from 1590419.25000 to 1587950.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1587950.8750\n",
      "Epoch 47/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1581815.0000\n",
      "Epoch 47: loss improved from 1587950.87500 to 1581532.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1581532.1250\n",
      "Epoch 48/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1577135.2500\n",
      "Epoch 48: loss improved from 1581532.12500 to 1577199.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1577199.1250\n",
      "Epoch 49/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1571683.2500\n",
      "Epoch 49: loss improved from 1577199.12500 to 1571051.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1571051.1250\n",
      "Epoch 50/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1566474.3750\n",
      "Epoch 50: loss improved from 1571051.12500 to 1566578.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1566578.5000\n",
      "Epoch 51/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1564313.3750\n",
      "Epoch 51: loss improved from 1566578.50000 to 1563697.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1563697.5000\n",
      "Epoch 52/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1558657.3750\n",
      "Epoch 52: loss improved from 1563697.50000 to 1558801.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1558801.7500\n",
      "Epoch 53/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1551732.1250\n",
      "Epoch 53: loss improved from 1558801.75000 to 1551622.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1551622.2500\n",
      "Epoch 54/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1550417.1250\n",
      "Epoch 54: loss improved from 1551622.25000 to 1550417.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1550417.1250\n",
      "Epoch 55/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1544959.0000\n",
      "Epoch 55: loss improved from 1550417.12500 to 1544914.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1544914.6250\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1545230.1250\n",
      "Epoch 56: loss did not improve from 1544914.62500\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1545516.2500\n",
      "Epoch 57/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1540253.7500\n",
      "Epoch 57: loss improved from 1544914.62500 to 1540438.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1540438.8750\n",
      "Epoch 58/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1533923.3750\n",
      "Epoch 58: loss improved from 1540438.87500 to 1533454.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 1533454.0000\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1530465.3750\n",
      "Epoch 59: loss improved from 1533454.00000 to 1530262.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1530262.3750\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1524705.7500\n",
      "Epoch 60: loss improved from 1530262.37500 to 1524705.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1524705.7500\n",
      "Epoch 61/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1525753.1250\n",
      "Epoch 61: loss did not improve from 1524705.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1525780.7500\n",
      "Epoch 62/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1520279.5000\n",
      "Epoch 62: loss improved from 1524705.75000 to 1520303.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1520303.7500\n",
      "Epoch 63/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1519904.0000\n",
      "Epoch 63: loss improved from 1520303.75000 to 1519359.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1519359.8750\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1511633.5000\n",
      "Epoch 64: loss improved from 1519359.87500 to 1511763.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1511763.7500\n",
      "Epoch 65/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1512280.1250\n",
      "Epoch 65: loss did not improve from 1511763.75000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1512280.1250\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1508215.5000\n",
      "Epoch 66: loss improved from 1511763.75000 to 1508333.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1508333.3750\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1503338.8750\n",
      "Epoch 67: loss improved from 1508333.37500 to 1504521.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1504521.7500\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1500431.1250\n",
      "Epoch 68: loss improved from 1504521.75000 to 1500431.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1500431.1250\n",
      "Epoch 69/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1499534.3750\n",
      "Epoch 69: loss improved from 1500431.12500 to 1498838.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1498838.0000\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1494802.6250\n",
      "Epoch 70: loss improved from 1498838.00000 to 1494802.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1494802.6250\n",
      "Epoch 71/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1494295.3750\n",
      "Epoch 71: loss improved from 1494802.62500 to 1494431.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1494431.8750\n",
      "Epoch 72/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1492360.0000\n",
      "Epoch 72: loss improved from 1494431.87500 to 1492463.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1492463.6250\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1486360.1250\n",
      "Epoch 73: loss improved from 1492463.62500 to 1486360.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1486360.1250\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1485988.2500\n",
      "Epoch 74: loss improved from 1486360.12500 to 1485903.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1485903.5000\n",
      "Epoch 75/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1483580.2500\n",
      "Epoch 75: loss improved from 1485903.50000 to 1483438.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1483438.0000\n",
      "Epoch 76/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1480795.5000\n",
      "Epoch 76: loss improved from 1483438.00000 to 1480865.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1480865.1250\n",
      "Epoch 77/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1478660.3750\n",
      "Epoch 77: loss improved from 1480865.12500 to 1478618.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1478618.2500\n",
      "Epoch 78/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1475365.5000\n",
      "Epoch 78: loss improved from 1478618.25000 to 1475826.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1475826.1250\n",
      "Epoch 79/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1475658.3750\n",
      "Epoch 79: loss did not improve from 1475826.12500\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1475909.7500\n",
      "Epoch 80/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1472964.0000\n",
      "Epoch 80: loss improved from 1475826.12500 to 1472711.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1472711.2500\n",
      "Epoch 81/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1471626.7500\n",
      "Epoch 81: loss improved from 1472711.25000 to 1471454.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1471454.1250\n",
      "Epoch 82/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1468536.0000\n",
      "Epoch 82: loss improved from 1471454.12500 to 1469370.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1469370.3750\n",
      "Epoch 83/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1463263.3750\n",
      "Epoch 83: loss improved from 1469370.37500 to 1463292.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1463292.7500\n",
      "Epoch 84/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1465985.8750\n",
      "Epoch 84: loss did not improve from 1463292.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1465796.7500\n",
      "Epoch 85/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1461092.3750\n",
      "Epoch 85: loss improved from 1463292.75000 to 1461518.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 1461518.8750\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1461178.5000\n",
      "Epoch 86: loss improved from 1461518.87500 to 1461309.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 23s 7ms/step - loss: 1461309.0000\n",
      "Epoch 87/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1458641.7500\n",
      "Epoch 87: loss improved from 1461309.00000 to 1458743.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 1458743.7500\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1453095.6250\n",
      "Epoch 88: loss improved from 1458743.75000 to 1453188.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1453188.8750\n",
      "Epoch 89/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1454567.3750\n",
      "Epoch 89: loss did not improve from 1453188.87500\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1455397.5000\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1452518.2500\n",
      "Epoch 90: loss improved from 1453188.87500 to 1452518.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1452518.2500\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1452148.1250\n",
      "Epoch 91: loss improved from 1452518.25000 to 1452107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1452107.7500\n",
      "Epoch 92/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1448278.8750\n",
      "Epoch 92: loss improved from 1452107.75000 to 1448053.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1448053.6250\n",
      "Epoch 93/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1445561.2500\n",
      "Epoch 93: loss improved from 1448053.62500 to 1446286.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1446286.3750\n",
      "Epoch 94/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1447753.8750\n",
      "Epoch 94: loss did not improve from 1446286.37500\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1447582.8750\n",
      "Epoch 95/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1444480.7500\n",
      "Epoch 95: loss improved from 1446286.37500 to 1443919.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1443919.3750\n",
      "Epoch 96/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1440702.7500\n",
      "Epoch 96: loss improved from 1443919.37500 to 1441792.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1441792.7500\n",
      "Epoch 97/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1440638.5000\n",
      "Epoch 97: loss improved from 1441792.75000 to 1439988.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1439988.5000\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1439147.7500\n",
      "Epoch 98: loss improved from 1439988.50000 to 1439147.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1439147.7500\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1436770.2500\n",
      "Epoch 99: loss improved from 1439147.75000 to 1436770.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1436770.2500\n",
      "Epoch 100/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1435171.0000\n",
      "Epoch 100: loss improved from 1436770.25000 to 1435283.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1435283.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 22:46:08 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 22:46:08 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp4ulbad_c\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp4ulbad_c\\model\\data\\model\\assets\n",
      "2023/05/26 22:46:49 INFO mlflow.tracking.fluent: Experiment with name 'cnn2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 25, 25, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 25, 25, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 6, 6, 32)          18464     \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSampling  (None, 12, 12, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSampling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,132,005\n",
      "Trainable params: 6,132,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 54s - loss: 1721651.5000    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.0100s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.0100s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736242.7500\n",
      "Epoch 1: loss improved from inf to 2736506.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 36s 10ms/step - loss: 2736506.2500\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736702.5000\n",
      "Epoch 2: loss improved from 2736506.25000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.0000\n",
      "Epoch 3/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736609.5000\n",
      "Epoch 3: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736505.7500\n",
      "Epoch 4/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737202.2500\n",
      "Epoch 4: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736506.7500\n",
      "Epoch 5/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736396.7500\n",
      "Epoch 5: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736505.7500\n",
      "Epoch 6/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736732.5000\n",
      "Epoch 6: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736505.0000\n",
      "Epoch 7/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736698.0000\n",
      "Epoch 7: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736505.0000\n",
      "Epoch 8/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736198.7500\n",
      "Epoch 8: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736505.0000\n",
      "Epoch 9/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 9: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736503.5000\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737081.7500\n",
      "Epoch 10: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736503.7500\n",
      "Epoch 11/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736962.0000\n",
      "Epoch 11: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736505.7500\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736274.0000\n",
      "Epoch 12: loss improved from 2736503.00000 to 2736500.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736500.7500\n",
      "Epoch 13/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737069.2500\n",
      "Epoch 13: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736502.7500\n",
      "Epoch 14/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736880.7500\n",
      "Epoch 14: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736500.7500\n",
      "Epoch 15/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736602.7500\n",
      "Epoch 15: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736502.7500\n",
      "Epoch 16/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736470.0000\n",
      "Epoch 16: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736501.5000\n",
      "Epoch 17/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737035.0000\n",
      "Epoch 17: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736503.5000\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736865.0000\n",
      "Epoch 18: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736504.5000\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736727.2500\n",
      "Epoch 19: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736504.5000\n",
      "Epoch 20/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736247.7500\n",
      "Epoch 20: loss improved from 2736500.75000 to 2736500.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736500.0000\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 21: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736502.5000\n",
      "Epoch 22/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.2500\n",
      "Epoch 22: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736501.2500\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736328.5000\n",
      "Epoch 23: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736502.7500\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737051.2500\n",
      "Epoch 24: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736500.7500\n",
      "Epoch 25/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736283.2500\n",
      "Epoch 25: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736502.2500\n",
      "Epoch 26/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736894.7500\n",
      "Epoch 26: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736501.7500\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.2500\n",
      "Epoch 27: loss improved from 2736500.00000 to 2736499.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736499.2500\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736342.7500\n",
      "Epoch 28: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736503.5000\n",
      "Epoch 29/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736377.2500\n",
      "Epoch 29: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736501.5000\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735573.5000\n",
      "Epoch 30: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736501.5000\n",
      "Epoch 31/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737548.7500\n",
      "Epoch 31: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736501.2500\n",
      "Epoch 32/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736826.5000\n",
      "Epoch 32: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736503.0000\n",
      "Epoch 33/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736823.5000\n",
      "Epoch 33: loss improved from 2736499.25000 to 2736497.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736497.7500\n",
      "Epoch 34/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735688.7500\n",
      "Epoch 34: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736502.7500\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736810.5000\n",
      "Epoch 35: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736499.5000\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736778.7500\n",
      "Epoch 36: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736500.5000\n",
      "Epoch 37/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736604.5000\n",
      "Epoch 37: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736499.5000\n",
      "Epoch 38/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736312.7500\n",
      "Epoch 38: loss improved from 2736497.75000 to 2736496.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736496.7500\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.0000\n",
      "Epoch 39: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.0000\n",
      "Epoch 40/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736133.5000\n",
      "Epoch 40: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 41/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737555.5000\n",
      "Epoch 41: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736500.7500\n",
      "Epoch 42/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735564.7500\n",
      "Epoch 42: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736551.5000\n",
      "Epoch 43: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.0000\n",
      "Epoch 44/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736792.0000\n",
      "Epoch 44: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.2500\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736949.7500\n",
      "Epoch 45: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 46/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734988.0000\n",
      "Epoch 46: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736501.5000\n",
      "Epoch 47/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737038.0000\n",
      "Epoch 47: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736501.2500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736813.7500\n",
      "Epoch 48: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736502.2500\n",
      "Epoch 49/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737211.2500\n",
      "Epoch 49: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736498.0000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737210.0000\n",
      "Epoch 50: loss improved from 2736496.75000 to 2736495.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736495.5000\n",
      "Epoch 51/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736447.5000\n",
      "Epoch 51: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736500.5000\n",
      "Epoch 52/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736756.5000\n",
      "Epoch 52: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.0000\n",
      "Epoch 53/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736823.5000\n",
      "Epoch 53: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.0000\n",
      "Epoch 54/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.0000\n",
      "Epoch 54: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.0000\n",
      "Epoch 55/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737017.0000\n",
      "Epoch 55: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736495.7500\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737348.7500\n",
      "Epoch 56: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736498.5000\n",
      "Epoch 57/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735988.7500\n",
      "Epoch 57: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.2500\n",
      "Epoch 58/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737899.7500\n",
      "Epoch 58: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736498.5000\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736306.0000\n",
      "Epoch 59: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.2500\n",
      "Epoch 60/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736641.7500\n",
      "Epoch 60: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.2500\n",
      "Epoch 61/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736247.5000\n",
      "Epoch 61: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 62/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736838.0000\n",
      "Epoch 62: loss improved from 2736495.50000 to 2736494.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 63/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736795.0000\n",
      "Epoch 63: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736495.0000\n",
      "Epoch 64/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736244.0000\n",
      "Epoch 64: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736495.0000\n",
      "Epoch 65/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737426.0000\n",
      "Epoch 65: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.7500\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736712.0000\n",
      "Epoch 66: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736495.7500\n",
      "Epoch 67/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736633.0000\n",
      "Epoch 67: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736494.7500\n",
      "Epoch 68/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736766.5000\n",
      "Epoch 68: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.0000\n",
      "Epoch 69/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737594.0000\n",
      "Epoch 69: loss improved from 2736494.75000 to 2736493.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.7500\n",
      "Epoch 70/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736432.7500\n",
      "Epoch 70: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 71/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737244.7500\n",
      "Epoch 71: loss improved from 2736493.75000 to 2736493.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736493.2500\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736494.7500\n",
      "Epoch 72: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736498.0000\n",
      "Epoch 73: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736498.0000\n",
      "Epoch 74/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735998.5000\n",
      "Epoch 74: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.2500\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2738334.2500\n",
      "Epoch 75: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736494.7500\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736495.0000\n",
      "Epoch 76: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 33s 10ms/step - loss: 2736495.0000\n",
      "Epoch 77/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737118.0000\n",
      "Epoch 77: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736495.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736691.0000\n",
      "Epoch 78: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736494.5000\n",
      "Epoch 79/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735918.0000\n",
      "Epoch 79: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736495.7500\n",
      "Epoch 80/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735558.2500\n",
      "Epoch 80: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.5000\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736308.2500\n",
      "Epoch 81: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736495.0000\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735944.7500\n",
      "Epoch 82: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 33s 10ms/step - loss: 2736495.7500\n",
      "Epoch 83/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735189.5000\n",
      "Epoch 83: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736493.5000\n",
      "Epoch 84/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735441.5000\n",
      "Epoch 84: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736495.0000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735816.5000\n",
      "Epoch 85: loss improved from 2736493.25000 to 2736492.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736492.5000\n",
      "Epoch 86/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734492.5000\n",
      "Epoch 86: loss did not improve from 2736492.50000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736493.2500\n",
      "Epoch 87/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736493.7500\n",
      "Epoch 87: loss did not improve from 2736492.50000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736493.7500\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737027.0000\n",
      "Epoch 88: loss did not improve from 2736492.50000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736492.7500\n",
      "Epoch 89/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735406.2500\n",
      "Epoch 89: loss improved from 2736492.50000 to 2736490.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736490.2500\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736491.7500\n",
      "Epoch 90: loss did not improve from 2736490.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.7500\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736490.0000\n",
      "Epoch 91: loss improved from 2736490.25000 to 2736490.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736490.0000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735132.2500\n",
      "Epoch 92: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.7500\n",
      "Epoch 93/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736625.5000\n",
      "Epoch 93: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736490.2500\n",
      "Epoch 94/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737863.0000\n",
      "Epoch 94: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.2500\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735666.5000\n",
      "Epoch 95: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736493.5000\n",
      "Epoch 96: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.5000\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736864.7500\n",
      "Epoch 97: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 98/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736625.0000\n",
      "Epoch 98: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.5000\n",
      "Epoch 99: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736492.5000\n",
      "Epoch 100/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736751.0000\n",
      "Epoch 100: loss improved from 2736490.00000 to 2736489.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736489.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 23:36:40 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 23:36:40 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp24pbejzk\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp24pbejzk\\model\\data\\model\\assets\n",
      "2023/05/26 23:37:51 INFO mlflow.tracking.fluent: Experiment with name 'cnn2vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 25, 25, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 25, 25, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 6, 6, 32)          18464     \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 12, 12, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " up_sampling2d_10 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_11 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,132,005\n",
      "Trainable params: 6,132,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.5000\n",
      "Epoch 1: loss improved from inf to 2736507.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 36s 10ms/step - loss: 2736507.5000\n",
      "Epoch 2/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734500.5000\n",
      "Epoch 2: loss improved from 2736507.50000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.0000\n",
      "Epoch 3/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737917.7500\n",
      "Epoch 3: loss improved from 2736503.00000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.5000\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 4: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736506.2500\n",
      "Epoch 5/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736947.2500\n",
      "Epoch 5: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736505.0000\n",
      "Epoch 6/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736896.7500\n",
      "Epoch 6: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736505.2500\n",
      "Epoch 7/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736698.0000\n",
      "Epoch 7: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736502.5000\n",
      "Epoch 8/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736418.2500\n",
      "Epoch 8: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736504.0000\n",
      "Epoch 9/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736929.2500\n",
      "Epoch 9: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736507.0000\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2738533.5000\n",
      "Epoch 10: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.0000\n",
      "Epoch 11/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734593.0000\n",
      "Epoch 11: loss improved from 2736501.50000 to 2736500.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736500.5000\n",
      "Epoch 12/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736177.5000\n",
      "Epoch 12: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736503.7500\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736074.0000\n",
      "Epoch 13: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736507.2500\n",
      "Epoch 14/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737400.5000\n",
      "Epoch 14: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736502.5000\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736109.2500\n",
      "Epoch 15: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736502.7500\n",
      "Epoch 16/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736399.7500\n",
      "Epoch 16: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736502.7500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735577.5000\n",
      "Epoch 17: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736504.7500\n",
      "Epoch 18/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737266.0000\n",
      "Epoch 18: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 19/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.0000\n",
      "Epoch 19: loss improved from 2736500.50000 to 2736499.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.0000\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736482.5000\n",
      "Epoch 20: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.7500\n",
      "Epoch 21/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737329.5000\n",
      "Epoch 21: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.7500\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736424.0000\n",
      "Epoch 22: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.7500\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736557.2500\n",
      "Epoch 23: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736559.0000\n",
      "Epoch 24: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.7500\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737088.2500\n",
      "Epoch 25: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.7500\n",
      "Epoch 26/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735923.0000\n",
      "Epoch 26: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.5000\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736498.5000\n",
      "Epoch 27: loss improved from 2736499.00000 to 2736498.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736498.5000\n",
      "Epoch 28/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735999.2500\n",
      "Epoch 28: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 29/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736886.7500\n",
      "Epoch 29: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.7500\n",
      "Epoch 30/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735404.0000\n",
      "Epoch 30: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 31/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736197.7500\n",
      "Epoch 31: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.5000\n",
      "Epoch 32/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736880.7500\n",
      "Epoch 32: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 33/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737814.7500\n",
      "Epoch 33: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.0000\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.2500\n",
      "Epoch 34: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.2500\n",
      "Epoch 35/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736151.7500\n",
      "Epoch 35: loss improved from 2736498.50000 to 2736498.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 36/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735990.5000\n",
      "Epoch 36: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.0000\n",
      "Epoch 37/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736582.0000\n",
      "Epoch 37: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.5000\n",
      "Epoch 38/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735959.7500\n",
      "Epoch 38: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.0000\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 39: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.7500\n",
      "Epoch 40/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.7500\n",
      "Epoch 40: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.7500\n",
      "Epoch 41/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737785.5000\n",
      "Epoch 41: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.2500\n",
      "Epoch 42/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735460.2500\n",
      "Epoch 42: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 43/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736989.2500\n",
      "Epoch 43: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.0000\n",
      "Epoch 44/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736205.5000\n",
      "Epoch 44: loss improved from 2736498.25000 to 2736496.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.0000\n",
      "Epoch 45/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737452.2500\n",
      "Epoch 45: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 46/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736498.2500\n",
      "Epoch 46: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 47/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736489.5000\n",
      "Epoch 47: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735955.5000\n",
      "Epoch 48: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.0000\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736496.0000\n",
      "Epoch 49: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.0000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735762.5000\n",
      "Epoch 50: loss improved from 2736496.00000 to 2736493.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.7500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736497.7500\n",
      "Epoch 51: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737029.0000\n",
      "Epoch 52: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.5000\n",
      "Epoch 53/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736440.2500\n",
      "Epoch 53: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.7500\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735695.5000\n",
      "Epoch 54: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736497.2500\n",
      "Epoch 55/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737319.0000\n",
      "Epoch 55: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736499.0000\n",
      "Epoch 56/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734964.0000\n",
      "Epoch 56: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.5000\n",
      "Epoch 57/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736331.7500\n",
      "Epoch 57: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.7500\n",
      "Epoch 58/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737488.2500\n",
      "Epoch 58: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.5000\n",
      "Epoch 59/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737136.0000\n",
      "Epoch 59: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.0000\n",
      "Epoch 60/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2738013.7500\n",
      "Epoch 60: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 61/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736825.7500\n",
      "Epoch 61: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736495.7500\n",
      "Epoch 62: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.7500\n",
      "Epoch 63/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737305.7500\n",
      "Epoch 63: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736564.7500\n",
      "Epoch 64: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.5000\n",
      "Epoch 65/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736489.0000\n",
      "Epoch 65: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 66/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736664.2500\n",
      "Epoch 66: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.5000\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736926.7500\n",
      "Epoch 67: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.7500\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736369.5000\n",
      "Epoch 68: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.7500\n",
      "Epoch 69/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735998.5000\n",
      "Epoch 69: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.5000\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 70: loss improved from 2736493.75000 to 2736492.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.2500\n",
      "Epoch 71/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736801.5000\n",
      "Epoch 71: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.7500\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736108.0000\n",
      "Epoch 72: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.2500\n",
      "Epoch 73/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736606.2500\n",
      "Epoch 73: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 74/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736295.5000\n",
      "Epoch 74: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.5000\n",
      "Epoch 75/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735796.7500\n",
      "Epoch 75: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.2500\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736497.7500\n",
      "Epoch 76: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736494.5000\n",
      "Epoch 77: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737232.5000\n",
      "Epoch 78: loss improved from 2736492.25000 to 2736491.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 79/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2738183.7500\n",
      "Epoch 79: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.0000\n",
      "Epoch 80/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736145.7500\n",
      "Epoch 80: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 81/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736805.5000\n",
      "Epoch 81: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.7500\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736861.5000\n",
      "Epoch 82: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.2500\n",
      "Epoch 83/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735732.5000\n",
      "Epoch 83: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.5000\n",
      "Epoch 84/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737746.0000\n",
      "Epoch 84: loss improved from 2736491.50000 to 2736489.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736489.0000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736014.7500\n",
      "Epoch 85: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 86: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.2500\n",
      "Epoch 87/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736179.7500\n",
      "Epoch 87: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.5000\n",
      "Epoch 88/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736868.2500\n",
      "Epoch 88: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.5000\n",
      "Epoch 89/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735482.2500\n",
      "Epoch 89: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 90/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736086.5000\n",
      "Epoch 90: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736489.0000\n",
      "Epoch 91/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737188.5000\n",
      "Epoch 91: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736491.2500\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736764.5000\n",
      "Epoch 92: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736492.2500\n",
      "Epoch 93/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735491.0000\n",
      "Epoch 93: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736494.5000\n",
      "Epoch 94/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736544.0000\n",
      "Epoch 94: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736493.5000\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736928.5000\n",
      "Epoch 95: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736489.5000\n",
      "Epoch 96/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736426.7500\n",
      "Epoch 96: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.7500\n",
      "Epoch 97/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735040.2500\n",
      "Epoch 97: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.7500\n",
      "Epoch 98/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735479.7500\n",
      "Epoch 98: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736490.0000\n",
      "Epoch 99/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736416.7500\n",
      "Epoch 99: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.7500\n",
      "Epoch 100/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737183.0000\n",
      "Epoch 100: loss improved from 2736489.00000 to 2736488.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736488.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 00:26:56 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 00:26:56 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpy4h3nw1e\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpy4h3nw1e\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_34 (Dense)            (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 46s - loss: 3325607.0000    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.0245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.0245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186/3189 [============================>.] - ETA: 0s - loss: 2603394.0000\n",
      "Epoch 1: loss improved from inf to 2602878.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 4ms/step - loss: 2602878.5000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2449903.2500\n",
      "Epoch 2: loss improved from 2602878.50000 to 2449336.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2449336.0000\n",
      "Epoch 3/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2415777.5000\n",
      "Epoch 3: loss improved from 2449336.00000 to 2414382.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2414382.2500\n",
      "Epoch 4/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2397420.0000\n",
      "Epoch 4: loss improved from 2414382.25000 to 2396892.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2396892.2500\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2384340.0000\n",
      "Epoch 5: loss improved from 2396892.25000 to 2383863.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2383863.7500\n",
      "Epoch 6/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2372725.0000\n",
      "Epoch 6: loss improved from 2383863.75000 to 2372451.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2372451.5000\n",
      "Epoch 7/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2366146.7500\n",
      "Epoch 7: loss improved from 2372451.50000 to 2366517.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2366517.5000\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2359472.7500\n",
      "Epoch 8: loss improved from 2366517.50000 to 2359321.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2359321.0000\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2355448.2500\n",
      "Epoch 9: loss improved from 2359321.00000 to 2354577.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2354577.2500\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2348332.5000\n",
      "Epoch 10: loss improved from 2354577.25000 to 2348100.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2348100.5000\n",
      "Epoch 11/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2344911.5000\n",
      "Epoch 11: loss improved from 2348100.50000 to 2344734.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2344734.7500\n",
      "Epoch 12/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2340517.2500\n",
      "Epoch 12: loss improved from 2344734.75000 to 2341755.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2341755.5000\n",
      "Epoch 13/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2338860.0000\n",
      "Epoch 13: loss improved from 2341755.50000 to 2338762.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2338762.5000\n",
      "Epoch 14/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2336938.0000\n",
      "Epoch 14: loss improved from 2338762.50000 to 2337269.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2337269.2500\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2330934.2500\n",
      "Epoch 15: loss improved from 2337269.25000 to 2334600.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2334600.2500\n",
      "Epoch 16/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2330460.5000\n",
      "Epoch 16: loss improved from 2334600.25000 to 2330903.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2330903.5000\n",
      "Epoch 17/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2328631.7500\n",
      "Epoch 17: loss improved from 2330903.50000 to 2328846.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2328846.0000\n",
      "Epoch 18/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2327124.2500\n",
      "Epoch 18: loss improved from 2328846.00000 to 2327658.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2327658.5000\n",
      "Epoch 19/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2326148.7500\n",
      "Epoch 19: loss improved from 2327658.50000 to 2326051.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2326051.0000\n",
      "Epoch 20/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2325596.0000\n",
      "Epoch 20: loss improved from 2326051.00000 to 2324977.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2324977.7500\n",
      "Epoch 21/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2325600.7500\n",
      "Epoch 21: loss improved from 2324977.75000 to 2323576.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2323576.2500\n",
      "Epoch 22/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2323626.5000\n",
      "Epoch 22: loss improved from 2323576.25000 to 2322742.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2322742.7500\n",
      "Epoch 23/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2321824.5000\n",
      "Epoch 23: loss improved from 2322742.75000 to 2322407.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2322407.5000\n",
      "Epoch 24/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2323352.7500\n",
      "Epoch 24: loss improved from 2322407.50000 to 2321416.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2321416.7500\n",
      "Epoch 25/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2319918.7500\n",
      "Epoch 25: loss improved from 2321416.75000 to 2321005.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2321005.2500\n",
      "Epoch 26/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2319572.5000\n",
      "Epoch 26: loss improved from 2321005.25000 to 2319408.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2319408.2500\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2319682.5000\n",
      "Epoch 27: loss improved from 2319408.25000 to 2318717.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2318717.5000\n",
      "Epoch 28/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2319526.7500\n",
      "Epoch 28: loss did not improve from 2318717.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2319816.2500\n",
      "Epoch 29/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2321163.7500\n",
      "Epoch 29: loss improved from 2318717.50000 to 2317909.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2317909.0000\n",
      "Epoch 30/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2318341.7500\n",
      "Epoch 30: loss did not improve from 2317909.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2318017.2500\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2316938.0000\n",
      "Epoch 31: loss improved from 2317909.00000 to 2316938.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2316938.0000\n",
      "Epoch 32/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2314854.5000\n",
      "Epoch 32: loss improved from 2316938.00000 to 2315076.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2315076.0000\n",
      "Epoch 33/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2315442.2500\n",
      "Epoch 33: loss did not improve from 2315076.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2316176.7500\n",
      "Epoch 34/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2315213.5000\n",
      "Epoch 34: loss improved from 2315076.00000 to 2314931.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2314931.5000\n",
      "Epoch 35/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2314752.5000\n",
      "Epoch 35: loss improved from 2314931.50000 to 2313857.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2313857.0000\n",
      "Epoch 36/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2313802.7500\n",
      "Epoch 36: loss did not improve from 2313857.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2314077.0000\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2314481.2500\n",
      "Epoch 37: loss did not improve from 2313857.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2314481.2500\n",
      "Epoch 38/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2313612.7500\n",
      "Epoch 38: loss improved from 2313857.00000 to 2313809.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2313809.7500\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2313766.7500\n",
      "Epoch 39: loss improved from 2313809.75000 to 2313766.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2313766.7500\n",
      "Epoch 40/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2312805.5000\n",
      "Epoch 40: loss improved from 2313766.75000 to 2312971.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2312971.7500\n",
      "Epoch 41/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2313389.5000\n",
      "Epoch 41: loss improved from 2312971.75000 to 2312818.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2312818.7500\n",
      "Epoch 42/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2312247.2500\n",
      "Epoch 42: loss improved from 2312818.75000 to 2312280.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2312280.2500\n",
      "Epoch 43/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2312641.7500\n",
      "Epoch 43: loss improved from 2312280.25000 to 2311907.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311907.7500\n",
      "Epoch 44/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2314092.0000\n",
      "Epoch 44: loss did not improve from 2311907.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311973.5000\n",
      "Epoch 45/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2314425.5000\n",
      "Epoch 45: loss improved from 2311907.75000 to 2311809.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311809.2500\n",
      "Epoch 46/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2311303.5000\n",
      "Epoch 46: loss improved from 2311809.25000 to 2310980.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310980.7500\n",
      "Epoch 47/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2310282.7500\n",
      "Epoch 47: loss did not improve from 2310980.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311392.5000\n",
      "Epoch 48/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2310350.2500\n",
      "Epoch 48: loss improved from 2310980.75000 to 2310332.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310332.7500\n",
      "Epoch 49/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2311723.5000\n",
      "Epoch 49: loss did not improve from 2310332.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311152.5000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2311366.5000\n",
      "Epoch 50: loss did not improve from 2310332.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310560.2500\n",
      "Epoch 51/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2309386.2500\n",
      "Epoch 51: loss improved from 2310332.75000 to 2309237.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2309237.0000\n",
      "Epoch 52/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2309923.0000\n",
      "Epoch 52: loss did not improve from 2309237.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310105.0000\n",
      "Epoch 53/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2308935.7500\n",
      "Epoch 53: loss did not improve from 2309237.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310159.5000\n",
      "Epoch 54/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2310392.0000\n",
      "Epoch 54: loss improved from 2309237.00000 to 2308610.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308610.5000\n",
      "Epoch 55/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2308658.5000\n",
      "Epoch 55: loss did not improve from 2308610.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308895.0000\n",
      "Epoch 56/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2309633.5000\n",
      "Epoch 56: loss did not improve from 2308610.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2309408.7500\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2307786.0000\n",
      "Epoch 57: loss improved from 2308610.50000 to 2308113.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308113.5000\n",
      "Epoch 58/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2308458.5000\n",
      "Epoch 58: loss did not improve from 2308113.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308337.7500\n",
      "Epoch 59/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2309703.7500\n",
      "Epoch 59: loss did not improve from 2308113.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2309272.0000\n",
      "Epoch 60/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2307783.7500\n",
      "Epoch 60: loss improved from 2308113.50000 to 2307715.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307715.2500\n",
      "Epoch 61/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2307694.7500\n",
      "Epoch 61: loss did not improve from 2307715.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307738.0000\n",
      "Epoch 62/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2307235.5000\n",
      "Epoch 62: loss did not improve from 2307715.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308086.7500\n",
      "Epoch 63/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2307043.2500\n",
      "Epoch 63: loss did not improve from 2307715.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307971.2500\n",
      "Epoch 64/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2308277.7500\n",
      "Epoch 64: loss did not improve from 2307715.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308109.0000\n",
      "Epoch 65/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2309384.2500\n",
      "Epoch 65: loss improved from 2307715.25000 to 2306701.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2306701.0000\n",
      "Epoch 66/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2306494.2500\n",
      "Epoch 66: loss improved from 2306701.00000 to 2306270.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306270.7500\n",
      "Epoch 67/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2306422.5000\n",
      "Epoch 67: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307030.2500\n",
      "Epoch 68/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2308855.7500\n",
      "Epoch 68: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307137.7500\n",
      "Epoch 69/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2306546.0000\n",
      "Epoch 69: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306805.2500\n",
      "Epoch 70/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2306894.0000\n",
      "Epoch 70: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306841.7500\n",
      "Epoch 71/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2306728.0000\n",
      "Epoch 71: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306559.2500\n",
      "Epoch 72/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2308921.7500\n",
      "Epoch 72: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307111.0000\n",
      "Epoch 73/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2307061.5000\n",
      "Epoch 73: loss improved from 2306270.75000 to 2306047.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306047.0000\n",
      "Epoch 74/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2305553.5000\n",
      "Epoch 74: loss improved from 2306047.00000 to 2305165.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305165.0000\n",
      "Epoch 75/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2305886.7500\n",
      "Epoch 75: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306170.5000\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2305950.7500\n",
      "Epoch 76: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305950.7500\n",
      "Epoch 77/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2306042.5000\n",
      "Epoch 77: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305922.0000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2306174.2500\n",
      "Epoch 78: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306123.2500\n",
      "Epoch 79/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2306819.0000\n",
      "Epoch 79: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305684.5000\n",
      "Epoch 80/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2305553.0000\n",
      "Epoch 80: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305364.5000\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2305801.0000\n",
      "Epoch 81: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305801.0000\n",
      "Epoch 82/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2307041.7500\n",
      "Epoch 82: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305286.5000\n",
      "Epoch 83/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2306241.7500\n",
      "Epoch 83: loss improved from 2305165.00000 to 2305101.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305101.0000\n",
      "Epoch 84/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2306182.5000\n",
      "Epoch 84: loss improved from 2305101.00000 to 2304841.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304841.5000\n",
      "Epoch 85/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2306232.2500\n",
      "Epoch 85: loss improved from 2304841.50000 to 2304197.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304197.0000\n",
      "Epoch 86/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2304700.5000\n",
      "Epoch 86: loss did not improve from 2304197.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305171.0000\n",
      "Epoch 87/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2305118.5000\n",
      "Epoch 87: loss did not improve from 2304197.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304414.2500\n",
      "Epoch 88/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2307119.7500\n",
      "Epoch 88: loss did not improve from 2304197.00000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2305012.5000\n",
      "Epoch 89/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2304374.7500\n",
      "Epoch 89: loss improved from 2304197.00000 to 2304137.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304137.7500\n",
      "Epoch 90/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2305236.7500\n",
      "Epoch 90: loss did not improve from 2304137.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305301.0000\n",
      "Epoch 91/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2304434.7500\n",
      "Epoch 91: loss did not improve from 2304137.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304370.7500\n",
      "Epoch 92/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2306352.5000\n",
      "Epoch 92: loss did not improve from 2304137.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304286.7500\n",
      "Epoch 93/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2303907.7500\n",
      "Epoch 93: loss improved from 2304137.75000 to 2304015.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304015.2500\n",
      "Epoch 94/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2304609.0000\n",
      "Epoch 94: loss did not improve from 2304015.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304618.5000\n",
      "Epoch 95/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2303851.0000\n",
      "Epoch 95: loss improved from 2304015.25000 to 2303959.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303959.5000\n",
      "Epoch 96/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2305327.7500\n",
      "Epoch 96: loss did not improve from 2303959.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304547.7500\n",
      "Epoch 97/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2305459.5000\n",
      "Epoch 97: loss did not improve from 2303959.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304476.7500\n",
      "Epoch 98/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2302896.7500\n",
      "Epoch 98: loss improved from 2303959.50000 to 2303230.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303230.7500\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2303682.5000\n",
      "Epoch 99: loss did not improve from 2303230.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303352.5000\n",
      "Epoch 100/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2303687.7500\n",
      "Epoch 100: loss did not improve from 2303230.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303524.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 00:40:25 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 00:40:25 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmppmigv8ce\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmppmigv8ce\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_37 (Dense)            (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 54s - loss: 6680369.5000     WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0143s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0143s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735385.7500\n",
      "Epoch 1: loss improved from inf to 2735723.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 36s 3ms/step - loss: 2735723.2500\n",
      "Epoch 2/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2587657.0000\n",
      "Epoch 2: loss improved from 2735723.25000 to 2587661.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2587661.0000\n",
      "Epoch 3/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2566821.5000\n",
      "Epoch 3: loss improved from 2587661.00000 to 2569381.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2569381.7500\n",
      "Epoch 4/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2563698.2500\n",
      "Epoch 4: loss improved from 2569381.75000 to 2561474.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2561474.7500\n",
      "Epoch 5/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2556242.5000\n",
      "Epoch 5: loss improved from 2561474.75000 to 2555460.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2555460.2500\n",
      "Epoch 6/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2552261.5000\n",
      "Epoch 6: loss improved from 2555460.25000 to 2551152.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2551152.2500\n",
      "Epoch 7/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2547143.0000\n",
      "Epoch 7: loss improved from 2551152.25000 to 2547732.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2547732.7500\n",
      "Epoch 8/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2543054.5000\n",
      "Epoch 8: loss improved from 2547732.75000 to 2543858.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2543858.7500\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2541223.0000\n",
      "Epoch 9: loss improved from 2543858.75000 to 2541617.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2541617.7500\n",
      "Epoch 10/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2540386.5000\n",
      "Epoch 10: loss improved from 2541617.75000 to 2538967.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2538967.7500\n",
      "Epoch 11/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2537429.5000\n",
      "Epoch 11: loss improved from 2538967.75000 to 2538045.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2538045.0000\n",
      "Epoch 12/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2534987.5000\n",
      "Epoch 12: loss improved from 2538045.00000 to 2535433.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2535433.0000\n",
      "Epoch 13/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2536372.5000\n",
      "Epoch 13: loss improved from 2535433.00000 to 2534300.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2534300.2500\n",
      "Epoch 14/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2533505.0000\n",
      "Epoch 14: loss improved from 2534300.25000 to 2534024.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2534024.5000\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2532703.0000\n",
      "Epoch 15: loss improved from 2534024.50000 to 2533059.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2533059.7500\n",
      "Epoch 16/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2533063.7500\n",
      "Epoch 16: loss improved from 2533059.75000 to 2531906.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2531906.0000\n",
      "Epoch 17/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2531353.5000\n",
      "Epoch 17: loss improved from 2531906.00000 to 2531289.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2531289.7500\n",
      "Epoch 18/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2528766.0000\n",
      "Epoch 18: loss improved from 2531289.75000 to 2530286.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2530286.0000\n",
      "Epoch 19/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2528734.5000\n",
      "Epoch 19: loss improved from 2530286.00000 to 2529676.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2529676.7500\n",
      "Epoch 20/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2527638.7500\n",
      "Epoch 20: loss improved from 2529676.75000 to 2528466.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2528466.5000\n",
      "Epoch 21/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2526343.7500\n",
      "Epoch 21: loss improved from 2528466.50000 to 2527711.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2527711.7500\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2526244.2500\n",
      "Epoch 22: loss improved from 2527711.75000 to 2527001.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2527001.2500\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2528197.0000\n",
      "Epoch 23: loss improved from 2527001.25000 to 2526640.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2526640.5000\n",
      "Epoch 24/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2527051.5000\n",
      "Epoch 24: loss did not improve from 2526640.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2526748.7500\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2526528.5000\n",
      "Epoch 25: loss improved from 2526640.50000 to 2525909.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2525909.7500\n",
      "Epoch 26/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2524301.2500\n",
      "Epoch 26: loss improved from 2525909.75000 to 2524961.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2524961.5000\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2524087.0000\n",
      "Epoch 27: loss improved from 2524961.50000 to 2524875.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2524875.2500\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2525861.5000\n",
      "Epoch 28: loss improved from 2524875.25000 to 2524846.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2524846.0000\n",
      "Epoch 29/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2526383.2500\n",
      "Epoch 29: loss improved from 2524846.00000 to 2524475.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2524475.5000\n",
      "Epoch 30/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2524397.7500\n",
      "Epoch 30: loss did not improve from 2524475.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2525367.2500\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2524490.5000\n",
      "Epoch 31: loss improved from 2524475.50000 to 2524066.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2524066.7500\n",
      "Epoch 32/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2523795.7500\n",
      "Epoch 32: loss improved from 2524066.75000 to 2522944.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2522944.0000\n",
      "Epoch 33/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2524240.0000\n",
      "Epoch 33: loss did not improve from 2522944.00000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2523343.2500\n",
      "Epoch 34/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2521337.7500\n",
      "Epoch 34: loss improved from 2522944.00000 to 2521612.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2521612.2500\n",
      "Epoch 35/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2519847.5000\n",
      "Epoch 35: loss did not improve from 2521612.25000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2521979.0000\n",
      "Epoch 36/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2519312.0000\n",
      "Epoch 36: loss did not improve from 2521612.25000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2521845.2500\n",
      "Epoch 37/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2521944.0000\n",
      "Epoch 37: loss improved from 2521612.25000 to 2521107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2521107.7500\n",
      "Epoch 38/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2522182.5000\n",
      "Epoch 38: loss improved from 2521107.75000 to 2520690.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2520690.7500\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2519891.7500\n",
      "Epoch 39: loss improved from 2520690.75000 to 2519891.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2519891.7500\n",
      "Epoch 40/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2519358.2500\n",
      "Epoch 40: loss did not improve from 2519891.75000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2520629.2500\n",
      "Epoch 41/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2518586.7500\n",
      "Epoch 41: loss improved from 2519891.75000 to 2519825.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2519825.5000\n",
      "Epoch 42/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2517253.7500\n",
      "Epoch 42: loss did not improve from 2519825.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2520005.5000\n",
      "Epoch 43/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2520015.2500\n",
      "Epoch 43: loss improved from 2519825.50000 to 2519502.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519502.0000\n",
      "Epoch 44/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2519132.2500\n",
      "Epoch 44: loss improved from 2519502.00000 to 2519288.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519288.0000\n",
      "Epoch 45/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2521453.0000\n",
      "Epoch 45: loss did not improve from 2519288.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519573.2500\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2521434.2500\n",
      "Epoch 46: loss did not improve from 2519288.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519383.0000\n",
      "Epoch 47/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2518722.7500\n",
      "Epoch 47: loss improved from 2519288.00000 to 2518731.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518731.7500\n",
      "Epoch 48/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2519464.2500\n",
      "Epoch 48: loss did not improve from 2518731.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519403.2500\n",
      "Epoch 49/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2519406.5000\n",
      "Epoch 49: loss did not improve from 2518731.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518806.2500\n",
      "Epoch 50/100\n",
      "3166/3189 [============================>.] - ETA: 0s - loss: 2517872.5000\n",
      "Epoch 50: loss improved from 2518731.75000 to 2518115.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518115.0000\n",
      "Epoch 51/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2520017.0000\n",
      "Epoch 51: loss did not improve from 2518115.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518513.7500\n",
      "Epoch 52/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2514780.2500\n",
      "Epoch 52: loss improved from 2518115.00000 to 2517564.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517564.5000\n",
      "Epoch 53/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2518680.7500\n",
      "Epoch 53: loss did not improve from 2517564.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517687.7500\n",
      "Epoch 54/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2516744.5000\n",
      "Epoch 54: loss improved from 2517564.50000 to 2517223.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517223.7500\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2518270.5000\n",
      "Epoch 55: loss did not improve from 2517223.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518084.0000\n",
      "Epoch 56/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2517346.2500\n",
      "Epoch 56: loss did not improve from 2517223.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517293.7500\n",
      "Epoch 57/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2516787.7500\n",
      "Epoch 57: loss did not improve from 2517223.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517515.0000\n",
      "Epoch 58/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2517829.5000\n",
      "Epoch 58: loss improved from 2517223.75000 to 2516087.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516087.0000\n",
      "Epoch 59/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2518053.5000\n",
      "Epoch 59: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518065.0000\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2518004.5000\n",
      "Epoch 60: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518016.0000\n",
      "Epoch 61/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2516238.0000\n",
      "Epoch 61: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516397.2500\n",
      "Epoch 62/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2517395.5000\n",
      "Epoch 62: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516943.2500\n",
      "Epoch 63/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2515999.0000\n",
      "Epoch 63: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516352.2500\n",
      "Epoch 64/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2514467.7500\n",
      "Epoch 64: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516799.0000\n",
      "Epoch 65/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2516967.7500\n",
      "Epoch 65: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516479.7500\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2515625.7500\n",
      "Epoch 66: loss improved from 2516087.00000 to 2515625.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515625.7500\n",
      "Epoch 67/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2515340.5000\n",
      "Epoch 67: loss did not improve from 2515625.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516490.5000\n",
      "Epoch 68/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2513931.5000\n",
      "Epoch 68: loss improved from 2515625.75000 to 2515195.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515195.7500\n",
      "Epoch 69/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2515826.2500\n",
      "Epoch 69: loss did not improve from 2515195.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515734.2500\n",
      "Epoch 70/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2516331.0000\n",
      "Epoch 70: loss improved from 2515195.75000 to 2515181.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515181.0000\n",
      "Epoch 71/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2514769.2500\n",
      "Epoch 71: loss did not improve from 2515181.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515535.7500\n",
      "Epoch 72/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2514982.2500\n",
      "Epoch 72: loss did not improve from 2515181.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515655.2500\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2515221.5000\n",
      "Epoch 73: loss did not improve from 2515181.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515443.7500\n",
      "Epoch 74/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2514254.5000\n",
      "Epoch 74: loss did not improve from 2515181.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515265.7500\n",
      "Epoch 75/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2516589.0000\n",
      "Epoch 75: loss improved from 2515181.00000 to 2514402.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514402.0000\n",
      "Epoch 76/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2513400.0000\n",
      "Epoch 76: loss improved from 2514402.00000 to 2513880.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513880.5000\n",
      "Epoch 77/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2511326.2500\n",
      "Epoch 77: loss did not improve from 2513880.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514030.5000\n",
      "Epoch 78/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2512888.7500\n",
      "Epoch 78: loss did not improve from 2513880.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514033.7500\n",
      "Epoch 79/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2511914.7500\n",
      "Epoch 79: loss improved from 2513880.50000 to 2512349.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512349.5000\n",
      "Epoch 80/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2514276.5000\n",
      "Epoch 80: loss did not improve from 2512349.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513990.2500\n",
      "Epoch 81/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2515239.7500\n",
      "Epoch 81: loss did not improve from 2512349.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514389.5000\n",
      "Epoch 82/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2511715.7500\n",
      "Epoch 82: loss improved from 2512349.50000 to 2511927.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511927.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2512345.2500\n",
      "Epoch 83: loss improved from 2511927.75000 to 2511804.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511804.2500\n",
      "Epoch 84/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2512275.5000\n",
      "Epoch 84: loss did not improve from 2511804.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512891.2500\n",
      "Epoch 85/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2512850.2500\n",
      "Epoch 85: loss did not improve from 2511804.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512423.0000\n",
      "Epoch 86/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2512160.7500\n",
      "Epoch 86: loss did not improve from 2511804.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512328.2500\n",
      "Epoch 87/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2511765.5000\n",
      "Epoch 87: loss did not improve from 2511804.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511947.2500\n",
      "Epoch 88/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2510221.2500\n",
      "Epoch 88: loss improved from 2511804.25000 to 2510706.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510706.5000\n",
      "Epoch 89/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2512463.2500\n",
      "Epoch 89: loss did not improve from 2510706.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511259.2500\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2511512.0000\n",
      "Epoch 90: loss did not improve from 2510706.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511398.5000\n",
      "Epoch 91/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2510710.7500\n",
      "Epoch 91: loss improved from 2510706.50000 to 2510591.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510591.0000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2509684.0000\n",
      "Epoch 92: loss improved from 2510591.00000 to 2509567.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2509567.2500\n",
      "Epoch 93/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2509666.7500\n",
      "Epoch 93: loss improved from 2509567.25000 to 2508868.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508868.7500\n",
      "Epoch 94/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2510466.7500\n",
      "Epoch 94: loss did not improve from 2508868.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511104.7500\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2510169.0000\n",
      "Epoch 95: loss did not improve from 2508868.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510091.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2510075.0000\n",
      "Epoch 96: loss did not improve from 2508868.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510075.0000\n",
      "Epoch 97/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2509060.7500\n",
      "Epoch 97: loss improved from 2508868.75000 to 2508271.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508271.7500\n",
      "Epoch 98/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2508827.7500\n",
      "Epoch 98: loss did not improve from 2508271.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508894.2500\n",
      "Epoch 99/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2509146.7500\n",
      "Epoch 99: loss did not improve from 2508271.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508903.2500\n",
      "Epoch 100/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2508137.0000\n",
      "Epoch 100: loss improved from 2508271.75000 to 2508159.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508159.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 00:53:52 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 00:53:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpk384aee3\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpk384aee3\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 3104133.0000\n",
      "Epoch 1: loss improved from inf to 3104133.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 3104133.0000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2430314.7500\n",
      "Epoch 2: loss improved from 3104133.00000 to 2430734.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2430734.0000\n",
      "Epoch 3/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2365670.2500\n",
      "Epoch 3: loss improved from 2430734.00000 to 2365595.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2365595.2500\n",
      "Epoch 4/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2331921.0000\n",
      "Epoch 4: loss improved from 2365595.25000 to 2333567.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2333567.0000\n",
      "Epoch 5/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2337124.5000\n",
      "Epoch 5: loss did not improve from 2333567.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2337220.7500\n",
      "Epoch 6/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2303511.5000\n",
      "Epoch 6: loss improved from 2333567.00000 to 2302385.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2302385.7500\n",
      "Epoch 7/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2282293.2500\n",
      "Epoch 7: loss improved from 2302385.75000 to 2281136.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2281136.5000\n",
      "Epoch 8/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2271398.5000\n",
      "Epoch 8: loss improved from 2281136.50000 to 2269490.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2269490.0000\n",
      "Epoch 9/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2273038.5000\n",
      "Epoch 9: loss did not improve from 2269490.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2271121.2500\n",
      "Epoch 10/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2268917.7500\n",
      "Epoch 10: loss improved from 2269490.00000 to 2267572.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2267572.7500\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2264538.0000\n",
      "Epoch 11: loss improved from 2267572.75000 to 2264538.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2264538.0000\n",
      "Epoch 12/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2257869.2500\n",
      "Epoch 12: loss improved from 2264538.00000 to 2255202.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2255202.2500\n",
      "Epoch 13/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2244927.2500\n",
      "Epoch 13: loss improved from 2255202.25000 to 2244900.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2244900.2500\n",
      "Epoch 14/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2246913.7500\n",
      "Epoch 14: loss did not improve from 2244900.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2247235.5000\n",
      "Epoch 15/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2219743.5000\n",
      "Epoch 15: loss improved from 2244900.25000 to 2219607.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2219607.7500\n",
      "Epoch 16/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2240469.5000\n",
      "Epoch 16: loss did not improve from 2219607.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2239204.7500\n",
      "Epoch 17/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2232914.2500\n",
      "Epoch 17: loss did not improve from 2219607.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2232755.7500\n",
      "Epoch 18/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2225842.0000\n",
      "Epoch 18: loss did not improve from 2219607.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2225542.2500\n",
      "Epoch 19/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2218313.2500\n",
      "Epoch 19: loss did not improve from 2219607.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2219798.7500\n",
      "Epoch 20/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2212195.7500\n",
      "Epoch 20: loss improved from 2219607.75000 to 2211116.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2211116.5000\n",
      "Epoch 21/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2216749.5000\n",
      "Epoch 21: loss did not improve from 2211116.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2216356.0000\n",
      "Epoch 22/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2214088.7500\n",
      "Epoch 22: loss did not improve from 2211116.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2213971.2500\n",
      "Epoch 23/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2204323.7500\n",
      "Epoch 23: loss improved from 2211116.50000 to 2205018.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2205018.2500\n",
      "Epoch 24/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2215881.2500\n",
      "Epoch 24: loss did not improve from 2205018.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2216004.5000\n",
      "Epoch 25/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2208355.0000\n",
      "Epoch 25: loss did not improve from 2205018.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2208355.0000\n",
      "Epoch 26/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2209458.2500\n",
      "Epoch 26: loss did not improve from 2205018.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2207687.7500\n",
      "Epoch 27/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2211051.5000\n",
      "Epoch 27: loss did not improve from 2205018.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2212167.5000\n",
      "Epoch 28/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2199003.2500\n",
      "Epoch 28: loss improved from 2205018.25000 to 2199016.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2199016.2500\n",
      "Epoch 29/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2205444.0000\n",
      "Epoch 29: loss did not improve from 2199016.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2205128.5000\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2200649.7500\n",
      "Epoch 30: loss did not improve from 2199016.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2200657.0000\n",
      "Epoch 31/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2190145.5000\n",
      "Epoch 31: loss improved from 2199016.25000 to 2189601.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2189601.0000\n",
      "Epoch 32/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2189377.2500\n",
      "Epoch 32: loss did not improve from 2189601.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2190567.2500\n",
      "Epoch 33/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2194688.7500\n",
      "Epoch 33: loss did not improve from 2189601.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2193768.7500\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2203538.2500\n",
      "Epoch 34: loss did not improve from 2189601.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2204182.5000\n",
      "Epoch 35/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2188830.0000\n",
      "Epoch 35: loss improved from 2189601.00000 to 2188679.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2188679.7500\n",
      "Epoch 36/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2204031.7500\n",
      "Epoch 36: loss did not improve from 2188679.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2203059.2500\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2183249.0000\n",
      "Epoch 37: loss improved from 2188679.75000 to 2183249.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2183249.0000\n",
      "Epoch 38/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2197478.5000\n",
      "Epoch 38: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2197025.5000\n",
      "Epoch 39/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2193149.7500\n",
      "Epoch 39: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192320.7500\n",
      "Epoch 40/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2190489.0000\n",
      "Epoch 40: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2191147.5000\n",
      "Epoch 41/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2197291.7500\n",
      "Epoch 41: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2195737.0000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2195268.0000\n",
      "Epoch 42: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2193993.7500\n",
      "Epoch 43/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2191021.5000\n",
      "Epoch 43: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2190754.0000\n",
      "Epoch 44/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2189187.0000\n",
      "Epoch 44: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2189864.7500\n",
      "Epoch 45/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2187011.2500\n",
      "Epoch 45: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185281.5000\n",
      "Epoch 46/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2177947.7500\n",
      "Epoch 46: loss improved from 2183249.00000 to 2179675.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2179675.5000\n",
      "Epoch 47/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2196681.7500\n",
      "Epoch 47: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2195891.0000\n",
      "Epoch 48/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2191602.5000\n",
      "Epoch 48: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2190302.2500\n",
      "Epoch 49/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2185199.7500\n",
      "Epoch 49: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186085.2500\n",
      "Epoch 50/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2186965.7500\n",
      "Epoch 50: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186961.7500\n",
      "Epoch 51/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2184673.7500\n",
      "Epoch 51: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186850.7500\n",
      "Epoch 52/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2175852.0000\n",
      "Epoch 52: loss improved from 2179675.50000 to 2175545.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175545.0000\n",
      "Epoch 53/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2167082.2500\n",
      "Epoch 53: loss improved from 2175545.00000 to 2167510.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2167510.0000\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2185689.0000\n",
      "Epoch 54: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185578.7500\n",
      "Epoch 55/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2192459.5000\n",
      "Epoch 55: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2191676.7500\n",
      "Epoch 56/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2178356.7500\n",
      "Epoch 56: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2177989.0000\n",
      "Epoch 57/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2177938.5000\n",
      "Epoch 57: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177048.0000\n",
      "Epoch 58/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2189103.7500\n",
      "Epoch 58: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2188987.7500\n",
      "Epoch 59/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2187311.5000\n",
      "Epoch 59: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2187407.0000\n",
      "Epoch 60/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2187465.0000\n",
      "Epoch 60: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186999.0000\n",
      "Epoch 61/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2184477.2500\n",
      "Epoch 61: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184183.5000\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2179458.5000\n",
      "Epoch 62: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2179458.5000\n",
      "Epoch 63/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2186803.5000\n",
      "Epoch 63: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2187150.7500\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2172982.2500\n",
      "Epoch 64: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2172982.2500\n",
      "Epoch 65/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2186169.7500\n",
      "Epoch 65: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186664.5000\n",
      "Epoch 66/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2175786.0000\n",
      "Epoch 66: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175794.7500\n",
      "Epoch 67/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2177424.2500\n",
      "Epoch 67: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2178372.0000\n",
      "Epoch 68/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2184033.5000\n",
      "Epoch 68: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2183132.5000\n",
      "Epoch 69/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2184778.5000\n",
      "Epoch 69: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184549.0000\n",
      "Epoch 70/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2184429.0000\n",
      "Epoch 70: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2183119.2500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2178368.2500\n",
      "Epoch 71: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2178368.2500\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2181093.7500\n",
      "Epoch 72: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181820.5000\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2177263.7500\n",
      "Epoch 73: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2176343.5000\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2181218.5000\n",
      "Epoch 74: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181296.2500\n",
      "Epoch 75/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2173352.2500\n",
      "Epoch 75: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2174866.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2165694.0000\n",
      "Epoch 76: loss improved from 2167510.00000 to 2164695.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2164695.7500\n",
      "Epoch 77/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2180433.0000\n",
      "Epoch 77: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180988.0000\n",
      "Epoch 78/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2184563.2500\n",
      "Epoch 78: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184283.2500\n",
      "Epoch 79/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2181064.0000\n",
      "Epoch 79: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182586.0000\n",
      "Epoch 80/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2184064.5000\n",
      "Epoch 80: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184850.2500\n",
      "Epoch 81/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2169678.7500\n",
      "Epoch 81: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2170498.7500\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2177562.7500\n",
      "Epoch 82: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177562.7500\n",
      "Epoch 83/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2159750.5000\n",
      "Epoch 83: loss improved from 2164695.75000 to 2162709.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2162709.5000\n",
      "Epoch 84/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2182130.2500\n",
      "Epoch 84: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182139.0000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2183249.2500\n",
      "Epoch 85: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182901.5000\n",
      "Epoch 86/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2182861.7500\n",
      "Epoch 86: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182156.2500\n",
      "Epoch 87/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2180380.2500\n",
      "Epoch 87: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2178745.0000\n",
      "Epoch 88/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2180310.0000\n",
      "Epoch 88: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180397.0000\n",
      "Epoch 89/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2168347.5000\n",
      "Epoch 89: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2168347.5000\n",
      "Epoch 90/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2182041.0000\n",
      "Epoch 90: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181118.2500\n",
      "Epoch 91/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2176942.7500\n",
      "Epoch 91: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2176230.7500\n",
      "Epoch 92/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2172560.7500\n",
      "Epoch 92: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2173342.5000\n",
      "Epoch 93/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2166977.0000\n",
      "Epoch 93: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2167273.0000\n",
      "Epoch 94/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2170673.0000\n",
      "Epoch 94: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2173425.5000\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2175333.5000\n",
      "Epoch 95: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175333.5000\n",
      "Epoch 96/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2181191.2500\n",
      "Epoch 96: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180004.0000\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2175566.0000\n",
      "Epoch 97: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175725.0000\n",
      "Epoch 98/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2163145.0000\n",
      "Epoch 98: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2163310.7500\n",
      "Epoch 99/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2183938.2500\n",
      "Epoch 99: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182802.5000\n",
      "Epoch 100/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2180333.7500\n",
      "Epoch 100: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180775.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 01:13:22 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 01:13:22 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpbm8o6lwx\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpbm8o6lwx\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 1:01:39 - loss: 19191698.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0202s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0202s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3185/3189 [============================>.] - ETA: 0s - loss: 11107802.0000\n",
      "Epoch 1: loss improved from inf to 11097644.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 11097644.0000\n",
      "Epoch 2/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2844826.7500\n",
      "Epoch 2: loss improved from 11097644.00000 to 2844393.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2844393.7500\n",
      "Epoch 3/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2639946.7500\n",
      "Epoch 3: loss improved from 2844393.75000 to 2640636.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2640636.5000\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2600215.0000\n",
      "Epoch 4: loss improved from 2640636.50000 to 2600215.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2600215.0000\n",
      "Epoch 5/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2576911.7500\n",
      "Epoch 5: loss improved from 2600215.00000 to 2577287.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2577287.2500\n",
      "Epoch 6/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2562509.0000\n",
      "Epoch 6: loss improved from 2577287.25000 to 2562476.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2562476.2500\n",
      "Epoch 7/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2564345.7500\n",
      "Epoch 7: loss did not improve from 2562476.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2563597.5000\n",
      "Epoch 8/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2555764.5000\n",
      "Epoch 8: loss improved from 2562476.25000 to 2557104.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2557104.2500\n",
      "Epoch 9/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2548729.2500\n",
      "Epoch 9: loss improved from 2557104.25000 to 2548526.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2548526.2500\n",
      "Epoch 10/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2552097.7500\n",
      "Epoch 10: loss did not improve from 2548526.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2552584.0000\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2553547.2500\n",
      "Epoch 11: loss did not improve from 2548526.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2551398.0000\n",
      "Epoch 12/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2548623.0000\n",
      "Epoch 12: loss improved from 2548526.25000 to 2546973.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2546973.2500\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2553499.7500\n",
      "Epoch 13: loss did not improve from 2546973.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2553499.7500\n",
      "Epoch 14/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2550616.0000\n",
      "Epoch 14: loss did not improve from 2546973.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2550887.5000\n",
      "Epoch 15/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2550711.0000\n",
      "Epoch 15: loss did not improve from 2546973.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550783.7500\n",
      "Epoch 16/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2545325.7500\n",
      "Epoch 16: loss improved from 2546973.25000 to 2545270.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545270.7500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2546368.5000\n",
      "Epoch 17: loss did not improve from 2545270.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545787.0000\n",
      "Epoch 18/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2545737.7500\n",
      "Epoch 18: loss did not improve from 2545270.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2545977.0000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2541031.0000\n",
      "Epoch 19: loss improved from 2545270.75000 to 2540398.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2540398.2500\n",
      "Epoch 20/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2546630.5000\n",
      "Epoch 20: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2546437.5000\n",
      "Epoch 21/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2547382.7500\n",
      "Epoch 21: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2547993.7500\n",
      "Epoch 22/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2542297.5000\n",
      "Epoch 22: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2541932.5000\n",
      "Epoch 23/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2544895.0000\n",
      "Epoch 23: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2543771.5000\n",
      "Epoch 24/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2544404.2500\n",
      "Epoch 24: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2542964.5000\n",
      "Epoch 25/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2540459.0000\n",
      "Epoch 25: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2541112.7500\n",
      "Epoch 26/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2544204.7500\n",
      "Epoch 26: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2543417.5000\n",
      "Epoch 27/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2541292.5000\n",
      "Epoch 27: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2541275.7500\n",
      "Epoch 28/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2545964.2500\n",
      "Epoch 28: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2546048.7500\n",
      "Epoch 29/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2537435.0000\n",
      "Epoch 29: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2541015.0000\n",
      "Epoch 30/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2542940.5000\n",
      "Epoch 30: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2542426.5000\n",
      "Epoch 31/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2538317.5000\n",
      "Epoch 31: loss improved from 2540398.25000 to 2538113.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2538113.2500\n",
      "Epoch 32/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2544001.0000\n",
      "Epoch 32: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2544040.7500\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2542704.0000\n",
      "Epoch 33: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2542704.0000\n",
      "Epoch 34/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2539693.5000\n",
      "Epoch 34: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539136.0000\n",
      "Epoch 35/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2539645.7500\n",
      "Epoch 35: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539018.0000\n",
      "Epoch 36/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2539398.2500\n",
      "Epoch 36: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2540628.5000\n",
      "Epoch 37/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2542484.7500\n",
      "Epoch 37: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2542416.5000\n",
      "Epoch 38/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2538006.7500\n",
      "Epoch 38: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2538763.7500\n",
      "Epoch 39/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537174.5000\n",
      "Epoch 39: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2538563.5000\n",
      "Epoch 40/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2537633.0000\n",
      "Epoch 40: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2540157.2500\n",
      "Epoch 41/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2541933.5000\n",
      "Epoch 41: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2540665.7500\n",
      "Epoch 42/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2539650.2500\n",
      "Epoch 42: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539531.5000\n",
      "Epoch 43/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2537825.2500\n",
      "Epoch 43: loss improved from 2538113.25000 to 2538015.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2538015.5000\n",
      "Epoch 44/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2540363.2500\n",
      "Epoch 44: loss did not improve from 2538015.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2538606.7500\n",
      "Epoch 45/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2534737.5000\n",
      "Epoch 45: loss improved from 2538015.50000 to 2534249.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2534249.5000\n",
      "Epoch 46/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2539513.5000\n",
      "Epoch 46: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539513.5000\n",
      "Epoch 47/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2538571.2500\n",
      "Epoch 47: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2538196.5000\n",
      "Epoch 48/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2537199.2500\n",
      "Epoch 48: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2538098.2500\n",
      "Epoch 49/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2540297.7500\n",
      "Epoch 49: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539173.5000\n",
      "Epoch 50/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2533227.5000\n",
      "Epoch 50: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535477.7500\n",
      "Epoch 51/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2534814.5000\n",
      "Epoch 51: loss improved from 2534249.50000 to 2533886.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533886.7500\n",
      "Epoch 52/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2532891.2500\n",
      "Epoch 52: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535284.7500\n",
      "Epoch 53/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2541116.5000\n",
      "Epoch 53: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2540405.5000\n",
      "Epoch 54/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533559.0000\n",
      "Epoch 54: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2536287.5000\n",
      "Epoch 55/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537666.5000\n",
      "Epoch 55: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2536971.5000\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2539131.5000\n",
      "Epoch 56: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539175.0000\n",
      "Epoch 57/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2533690.0000\n",
      "Epoch 57: loss improved from 2533886.75000 to 2533788.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533788.5000\n",
      "Epoch 58/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2535573.7500\n",
      "Epoch 58: loss did not improve from 2533788.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2534479.7500\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2534270.7500\n",
      "Epoch 59: loss did not improve from 2533788.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2534129.2500\n",
      "Epoch 60/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2532775.2500\n",
      "Epoch 60: loss did not improve from 2533788.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2534876.7500\n",
      "Epoch 61/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2533157.5000\n",
      "Epoch 61: loss improved from 2533788.50000 to 2533345.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533345.5000\n",
      "Epoch 62/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2535486.5000\n",
      "Epoch 62: loss did not improve from 2533345.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535149.5000\n",
      "Epoch 63/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2534347.0000\n",
      "Epoch 63: loss did not improve from 2533345.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535315.0000\n",
      "Epoch 64/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2535513.7500\n",
      "Epoch 64: loss did not improve from 2533345.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537157.7500\n",
      "Epoch 65/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2536504.2500\n",
      "Epoch 65: loss did not improve from 2533345.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535837.5000\n",
      "Epoch 66/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2530690.2500\n",
      "Epoch 66: loss improved from 2533345.50000 to 2529133.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2529133.2500\n",
      "Epoch 67/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2536785.7500\n",
      "Epoch 67: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535415.0000\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2529861.2500\n",
      "Epoch 68: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2529769.2500\n",
      "Epoch 69/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2531574.0000\n",
      "Epoch 69: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531582.2500\n",
      "Epoch 70/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2533148.7500\n",
      "Epoch 70: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533039.5000\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537630.0000\n",
      "Epoch 71: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537313.5000\n",
      "Epoch 72/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2530070.7500\n",
      "Epoch 72: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532218.0000\n",
      "Epoch 73/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2533588.2500\n",
      "Epoch 73: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533565.7500\n",
      "Epoch 74/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2532389.0000\n",
      "Epoch 74: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532412.7500\n",
      "Epoch 75/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2534423.0000\n",
      "Epoch 75: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533385.7500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533193.2500\n",
      "Epoch 76: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533891.0000\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533551.0000\n",
      "Epoch 77: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532689.7500\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2533885.7500\n",
      "Epoch 78: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532937.5000\n",
      "Epoch 79/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2533221.5000\n",
      "Epoch 79: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2536790.7500\n",
      "Epoch 80/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537110.2500\n",
      "Epoch 80: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535827.7500\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2526967.2500\n",
      "Epoch 81: loss improved from 2529133.25000 to 2526753.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2526753.2500\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2533138.7500\n",
      "Epoch 82: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533545.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2537995.5000\n",
      "Epoch 83: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535835.7500\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2531094.5000\n",
      "Epoch 84: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531822.5000\n",
      "Epoch 85/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2533071.2500\n",
      "Epoch 85: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532976.0000\n",
      "Epoch 86/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2532837.2500\n",
      "Epoch 86: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532623.2500\n",
      "Epoch 87/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2528791.5000\n",
      "Epoch 87: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2529432.7500\n",
      "Epoch 88/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533043.7500\n",
      "Epoch 88: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532796.7500\n",
      "Epoch 89/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2533772.2500\n",
      "Epoch 89: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532908.2500\n",
      "Epoch 90/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2530201.7500\n",
      "Epoch 90: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2530296.0000\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2527925.7500\n",
      "Epoch 91: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2527925.7500\n",
      "Epoch 92/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2531044.5000\n",
      "Epoch 92: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531203.2500\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2531121.0000\n",
      "Epoch 93: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531091.7500\n",
      "Epoch 94/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2536345.2500\n",
      "Epoch 94: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535917.0000\n",
      "Epoch 95/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2537787.5000\n",
      "Epoch 95: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537265.7500\n",
      "Epoch 96/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2530093.7500\n",
      "Epoch 96: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531078.5000\n",
      "Epoch 97/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2535513.7500\n",
      "Epoch 97: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533738.2500\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2531969.7500\n",
      "Epoch 98: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531969.7500\n",
      "Epoch 99/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2527247.5000\n",
      "Epoch 99: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2527489.2500\n",
      "Epoch 100/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2533704.5000\n",
      "Epoch 100: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532724.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 01:32:46 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 01:32:46 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn7lsf9ze\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn7lsf9ze\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 5:28:18 - loss: 2811787.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3180/3189 [============================>.] - ETA: 0s - loss: 2560252.0000\n",
      "Epoch 1: loss improved from inf to 2560849.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 3ms/step - loss: 2560849.0000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2389209.7500\n",
      "Epoch 2: loss improved from 2560849.00000 to 2390110.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2390110.7500\n",
      "Epoch 3/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2368434.0000\n",
      "Epoch 3: loss improved from 2390110.75000 to 2366883.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2366883.2500\n",
      "Epoch 4/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2355722.2500\n",
      "Epoch 4: loss improved from 2366883.25000 to 2354738.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2354738.2500\n",
      "Epoch 5/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2345760.0000\n",
      "Epoch 5: loss improved from 2354738.25000 to 2345166.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2345166.2500\n",
      "Epoch 6/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2337849.5000\n",
      "Epoch 6: loss improved from 2345166.25000 to 2337576.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2337576.2500\n",
      "Epoch 7/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2332957.0000\n",
      "Epoch 7: loss improved from 2337576.25000 to 2332009.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2332009.5000\n",
      "Epoch 8/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2325761.5000\n",
      "Epoch 8: loss improved from 2332009.50000 to 2325662.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2325662.7500\n",
      "Epoch 9/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2324648.2500\n",
      "Epoch 9: loss improved from 2325662.75000 to 2324146.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2324146.5000\n",
      "Epoch 10/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2322286.7500\n",
      "Epoch 10: loss improved from 2324146.50000 to 2320460.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2320460.2500\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2320577.2500\n",
      "Epoch 11: loss improved from 2320460.25000 to 2320188.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2320188.5000\n",
      "Epoch 12/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2314697.7500\n",
      "Epoch 12: loss improved from 2320188.50000 to 2313240.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2313240.0000\n",
      "Epoch 13/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2316544.0000\n",
      "Epoch 13: loss did not improve from 2313240.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2314451.2500\n",
      "Epoch 14/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2310728.5000\n",
      "Epoch 14: loss improved from 2313240.00000 to 2310467.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2310467.0000\n",
      "Epoch 15/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2309554.2500\n",
      "Epoch 15: loss improved from 2310467.00000 to 2309193.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2309193.0000\n",
      "Epoch 16/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2305291.2500\n",
      "Epoch 16: loss improved from 2309193.00000 to 2306636.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2306636.2500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2306075.0000\n",
      "Epoch 17: loss improved from 2306636.25000 to 2305422.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2305422.5000\n",
      "Epoch 18/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2300909.5000\n",
      "Epoch 18: loss improved from 2305422.50000 to 2302379.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2302379.2500\n",
      "Epoch 19/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2301915.5000\n",
      "Epoch 19: loss improved from 2302379.25000 to 2301512.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2301512.7500\n",
      "Epoch 20/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2300240.0000\n",
      "Epoch 20: loss improved from 2301512.75000 to 2301370.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2301370.2500\n",
      "Epoch 21/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2299396.5000\n",
      "Epoch 21: loss improved from 2301370.25000 to 2300274.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2300274.2500\n",
      "Epoch 22/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2300571.2500\n",
      "Epoch 22: loss improved from 2300274.25000 to 2299389.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2299389.5000\n",
      "Epoch 23/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2296819.2500\n",
      "Epoch 23: loss improved from 2299389.50000 to 2297670.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2297670.0000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2295696.2500\n",
      "Epoch 24: loss improved from 2297670.00000 to 2296084.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2296084.2500\n",
      "Epoch 25/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2294882.0000\n",
      "Epoch 25: loss improved from 2296084.25000 to 2295552.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2295552.5000\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2295506.0000\n",
      "Epoch 26: loss improved from 2295552.50000 to 2295506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2295506.0000\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2291655.2500\n",
      "Epoch 27: loss improved from 2295506.00000 to 2291655.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2291655.2500\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2290957.5000\n",
      "Epoch 28: loss improved from 2291655.25000 to 2290102.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2290102.7500\n",
      "Epoch 29/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2290204.7500\n",
      "Epoch 29: loss did not improve from 2290102.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2290362.2500\n",
      "Epoch 30/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2290282.5000\n",
      "Epoch 30: loss did not improve from 2290102.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2290385.2500\n",
      "Epoch 31/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2286929.7500\n",
      "Epoch 31: loss improved from 2290102.75000 to 2287368.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2287368.7500\n",
      "Epoch 32/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2290484.0000\n",
      "Epoch 32: loss did not improve from 2287368.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2290398.0000\n",
      "Epoch 33/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2290487.5000\n",
      "Epoch 33: loss did not improve from 2287368.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2287906.5000\n",
      "Epoch 34/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2286361.5000\n",
      "Epoch 34: loss improved from 2287368.75000 to 2286092.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2286092.0000\n",
      "Epoch 35/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2286885.2500\n",
      "Epoch 35: loss did not improve from 2286092.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2286364.5000\n",
      "Epoch 36/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2286957.0000\n",
      "Epoch 36: loss did not improve from 2286092.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2286957.0000\n",
      "Epoch 37/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2285167.7500\n",
      "Epoch 37: loss improved from 2286092.00000 to 2284869.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2284869.7500\n",
      "Epoch 38/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2283383.0000\n",
      "Epoch 38: loss improved from 2284869.75000 to 2283710.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2283710.7500\n",
      "Epoch 39/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2281891.5000\n",
      "Epoch 39: loss improved from 2283710.75000 to 2283650.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2283650.5000\n",
      "Epoch 40/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2279010.5000\n",
      "Epoch 40: loss improved from 2283650.50000 to 2282116.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282116.2500\n",
      "Epoch 41/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2282332.5000\n",
      "Epoch 41: loss did not improve from 2282116.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282840.7500\n",
      "Epoch 42/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2282573.0000\n",
      "Epoch 42: loss did not improve from 2282116.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282426.5000\n",
      "Epoch 43/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2279686.2500\n",
      "Epoch 43: loss improved from 2282116.25000 to 2282048.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282048.5000\n",
      "Epoch 44/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2283457.7500\n",
      "Epoch 44: loss improved from 2282048.50000 to 2281460.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2281460.0000\n",
      "Epoch 45/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2281939.5000\n",
      "Epoch 45: loss did not improve from 2281460.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2281583.2500\n",
      "Epoch 46/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2282548.7500\n",
      "Epoch 46: loss did not improve from 2281460.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2282099.2500\n",
      "Epoch 47/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2280712.0000\n",
      "Epoch 47: loss improved from 2281460.00000 to 2280459.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280459.5000\n",
      "Epoch 48/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2280253.0000\n",
      "Epoch 48: loss improved from 2280459.50000 to 2279261.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279261.0000\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2279596.0000\n",
      "Epoch 49: loss did not improve from 2279261.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279596.0000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2278614.0000\n",
      "Epoch 50: loss improved from 2279261.00000 to 2278805.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278805.5000\n",
      "Epoch 51/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2279499.2500\n",
      "Epoch 51: loss improved from 2278805.50000 to 2278660.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2278660.5000\n",
      "Epoch 52/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2278584.5000\n",
      "Epoch 52: loss improved from 2278660.50000 to 2277370.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2277370.5000\n",
      "Epoch 53/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2279734.5000\n",
      "Epoch 53: loss did not improve from 2277370.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278704.0000\n",
      "Epoch 54/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2276906.0000\n",
      "Epoch 54: loss improved from 2277370.50000 to 2276749.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276749.5000\n",
      "Epoch 55/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2278281.5000\n",
      "Epoch 55: loss did not improve from 2276749.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2277201.7500\n",
      "Epoch 56/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2279991.7500\n",
      "Epoch 56: loss improved from 2276749.50000 to 2276327.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2276327.0000\n",
      "Epoch 57/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2277407.0000\n",
      "Epoch 57: loss did not improve from 2276327.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276651.0000\n",
      "Epoch 58/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2276829.0000\n",
      "Epoch 58: loss improved from 2276327.00000 to 2276180.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276180.7500\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2275542.7500\n",
      "Epoch 59: loss improved from 2276180.75000 to 2275689.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275689.5000\n",
      "Epoch 60/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2274834.0000\n",
      "Epoch 60: loss did not improve from 2275689.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275782.7500\n",
      "Epoch 61/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2274675.7500\n",
      "Epoch 61: loss improved from 2275689.50000 to 2275326.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2275326.2500\n",
      "Epoch 62/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2276207.5000\n",
      "Epoch 62: loss improved from 2275326.25000 to 2274646.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274646.0000\n",
      "Epoch 63/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2274401.5000\n",
      "Epoch 63: loss did not improve from 2274646.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275051.0000\n",
      "Epoch 64/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2273392.2500\n",
      "Epoch 64: loss improved from 2274646.00000 to 2274387.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274387.5000\n",
      "Epoch 65/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2273008.5000\n",
      "Epoch 65: loss improved from 2274387.50000 to 2273618.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273618.7500\n",
      "Epoch 66/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2275070.7500\n",
      "Epoch 66: loss did not improve from 2273618.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2274448.5000\n",
      "Epoch 67/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2273746.0000\n",
      "Epoch 67: loss improved from 2273618.75000 to 2273398.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2273398.2500\n",
      "Epoch 68/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2273205.2500\n",
      "Epoch 68: loss improved from 2273398.25000 to 2273206.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273206.5000\n",
      "Epoch 69/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2272849.7500\n",
      "Epoch 69: loss did not improve from 2273206.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273665.2500\n",
      "Epoch 70/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2272241.5000\n",
      "Epoch 70: loss improved from 2273206.50000 to 2272110.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2272110.5000\n",
      "Epoch 71/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2271344.5000\n",
      "Epoch 71: loss did not improve from 2272110.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272385.2500\n",
      "Epoch 72/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2270462.2500\n",
      "Epoch 72: loss improved from 2272110.50000 to 2270982.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270982.2500\n",
      "Epoch 73/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2270735.0000\n",
      "Epoch 73: loss improved from 2270982.25000 to 2270666.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270666.5000\n",
      "Epoch 74/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2272719.0000\n",
      "Epoch 74: loss did not improve from 2270666.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272003.5000\n",
      "Epoch 75/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2270623.0000\n",
      "Epoch 75: loss did not improve from 2270666.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271227.0000\n",
      "Epoch 76/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2269256.2500\n",
      "Epoch 76: loss improved from 2270666.50000 to 2270626.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270626.5000\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2271179.0000\n",
      "Epoch 77: loss did not improve from 2270626.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271179.0000\n",
      "Epoch 78/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2271529.5000\n",
      "Epoch 78: loss did not improve from 2270626.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271506.2500\n",
      "Epoch 79/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2272528.2500\n",
      "Epoch 79: loss did not improve from 2270626.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271421.5000\n",
      "Epoch 80/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2269264.5000\n",
      "Epoch 80: loss improved from 2270626.50000 to 2269954.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269954.5000\n",
      "Epoch 81/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2269067.7500\n",
      "Epoch 81: loss improved from 2269954.50000 to 2269442.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269442.2500\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2270836.2500\n",
      "Epoch 82: loss did not improve from 2269442.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270964.0000\n",
      "Epoch 83/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2269263.5000\n",
      "Epoch 83: loss did not improve from 2269442.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270064.7500\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2267840.7500\n",
      "Epoch 84: loss improved from 2269442.25000 to 2268909.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268909.2500\n",
      "Epoch 85/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2268453.5000\n",
      "Epoch 85: loss did not improve from 2268909.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269930.2500\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2270060.0000\n",
      "Epoch 86: loss improved from 2268909.25000 to 2268447.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268447.0000\n",
      "Epoch 87/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2268942.7500\n",
      "Epoch 87: loss did not improve from 2268447.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268570.5000\n",
      "Epoch 88/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2267589.2500\n",
      "Epoch 88: loss did not improve from 2268447.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268629.0000\n",
      "Epoch 89/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2269065.7500\n",
      "Epoch 89: loss did not improve from 2268447.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268871.2500\n",
      "Epoch 90/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2270560.0000\n",
      "Epoch 90: loss did not improve from 2268447.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269459.0000\n",
      "Epoch 91/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2267473.0000\n",
      "Epoch 91: loss improved from 2268447.00000 to 2267350.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267350.5000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2267467.7500\n",
      "Epoch 92: loss improved from 2267350.50000 to 2267344.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267344.7500\n",
      "Epoch 93/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2262728.5000\n",
      "Epoch 93: loss did not improve from 2267344.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268457.0000\n",
      "Epoch 94/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2268690.2500\n",
      "Epoch 94: loss did not improve from 2267344.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268153.0000\n",
      "Epoch 95/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2267831.0000\n",
      "Epoch 95: loss did not improve from 2267344.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267899.5000\n",
      "Epoch 96/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2264009.2500\n",
      "Epoch 96: loss improved from 2267344.75000 to 2266331.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266331.5000\n",
      "Epoch 97/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2268362.0000\n",
      "Epoch 97: loss did not improve from 2266331.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268509.0000\n",
      "Epoch 98/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2267877.7500\n",
      "Epoch 98: loss did not improve from 2266331.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267078.5000\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2267267.7500\n",
      "Epoch 99: loss did not improve from 2266331.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267173.7500\n",
      "Epoch 100/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2267546.0000\n",
      "Epoch 100: loss did not improve from 2266331.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267070.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 01:49:33 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 01:49:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpp7e1ckq1\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpp7e1ckq1\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_56 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   6/3189 [..............................] - ETA: 1:25 - loss: 20811474.0000 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3181/3189 [============================>.] - ETA: 0s - loss: 2759942.0000\n",
      "Epoch 1: loss improved from inf to 2758039.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 24s 3ms/step - loss: 2758039.7500\n",
      "Epoch 2/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2418264.0000\n",
      "Epoch 2: loss improved from 2758039.75000 to 2419434.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2419434.7500\n",
      "Epoch 3/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2375783.2500\n",
      "Epoch 3: loss improved from 2419434.75000 to 2375297.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2375297.2500\n",
      "Epoch 4/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2361154.2500\n",
      "Epoch 4: loss improved from 2375297.25000 to 2359499.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2359499.7500\n",
      "Epoch 5/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2349461.5000\n",
      "Epoch 5: loss improved from 2359499.75000 to 2348873.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2348873.0000\n",
      "Epoch 6/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2341913.2500\n",
      "Epoch 6: loss improved from 2348873.00000 to 2341012.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2341012.0000\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2334512.5000\n",
      "Epoch 7: loss improved from 2341012.00000 to 2334512.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2334512.5000\n",
      "Epoch 8/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2331009.7500\n",
      "Epoch 8: loss improved from 2334512.50000 to 2331009.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2331009.7500\n",
      "Epoch 9/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2321855.2500\n",
      "Epoch 9: loss improved from 2331009.75000 to 2323777.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2323777.5000\n",
      "Epoch 10/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2321313.2500\n",
      "Epoch 10: loss improved from 2323777.50000 to 2322290.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2322290.2500\n",
      "Epoch 11/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2317449.2500\n",
      "Epoch 11: loss improved from 2322290.25000 to 2317652.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2317652.2500\n",
      "Epoch 12/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2314036.2500\n",
      "Epoch 12: loss improved from 2317652.25000 to 2312942.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2312942.5000\n",
      "Epoch 13/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2313133.5000\n",
      "Epoch 13: loss improved from 2312942.50000 to 2312608.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2312608.2500\n",
      "Epoch 14/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2310288.5000\n",
      "Epoch 14: loss improved from 2312608.25000 to 2311047.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2311047.0000\n",
      "Epoch 15/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2308403.2500\n",
      "Epoch 15: loss improved from 2311047.00000 to 2308406.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2308406.2500\n",
      "Epoch 16/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2305285.2500\n",
      "Epoch 16: loss improved from 2308406.25000 to 2306312.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2306312.2500\n",
      "Epoch 17/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2307433.2500\n",
      "Epoch 17: loss improved from 2306312.25000 to 2305961.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2305961.7500\n",
      "Epoch 18/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2300661.7500\n",
      "Epoch 18: loss improved from 2305961.75000 to 2304299.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2304299.5000\n",
      "Epoch 19/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2302123.7500\n",
      "Epoch 19: loss improved from 2304299.50000 to 2303244.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2303244.7500\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2298063.0000\n",
      "Epoch 20: loss improved from 2303244.75000 to 2300004.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2300004.5000\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2299062.7500\n",
      "Epoch 21: loss improved from 2300004.50000 to 2299062.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2299062.7500\n",
      "Epoch 22/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2297088.7500\n",
      "Epoch 22: loss improved from 2299062.75000 to 2296256.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2296256.7500\n",
      "Epoch 23/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2296147.0000\n",
      "Epoch 23: loss did not improve from 2296256.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2296555.5000\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2295291.7500\n",
      "Epoch 24: loss improved from 2296256.75000 to 2294994.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2294994.5000\n",
      "Epoch 25/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2295098.0000\n",
      "Epoch 25: loss did not improve from 2294994.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2296237.5000\n",
      "Epoch 26/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2294448.7500\n",
      "Epoch 26: loss improved from 2294994.50000 to 2294561.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2294561.0000\n",
      "Epoch 27/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2292872.0000\n",
      "Epoch 27: loss improved from 2294561.00000 to 2292580.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2292580.5000\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2289747.7500\n",
      "Epoch 28: loss improved from 2292580.50000 to 2289962.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2289962.7500\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2290249.5000\n",
      "Epoch 29: loss did not improve from 2289962.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2290249.5000\n",
      "Epoch 30/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2290103.5000\n",
      "Epoch 30: loss improved from 2289962.75000 to 2289617.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2289617.2500\n",
      "Epoch 31/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2286504.0000\n",
      "Epoch 31: loss improved from 2289617.25000 to 2287700.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2287700.5000\n",
      "Epoch 32/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2287007.2500\n",
      "Epoch 32: loss improved from 2287700.50000 to 2286866.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2286866.7500\n",
      "Epoch 33/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2288221.5000\n",
      "Epoch 33: loss did not improve from 2286866.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2288381.2500\n",
      "Epoch 34/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2289436.7500\n",
      "Epoch 34: loss did not improve from 2286866.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2288531.5000\n",
      "Epoch 35/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2284622.0000\n",
      "Epoch 35: loss improved from 2286866.75000 to 2286263.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2286263.5000\n",
      "Epoch 36/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2285875.0000\n",
      "Epoch 36: loss improved from 2286263.50000 to 2285628.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2285628.7500\n",
      "Epoch 37/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2282989.5000\n",
      "Epoch 37: loss improved from 2285628.75000 to 2283862.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2283862.0000\n",
      "Epoch 38/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2283189.7500\n",
      "Epoch 38: loss did not improve from 2283862.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2284667.7500\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2284019.7500\n",
      "Epoch 39: loss improved from 2283862.00000 to 2283513.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2283513.5000\n",
      "Epoch 40/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2283844.2500\n",
      "Epoch 40: loss did not improve from 2283513.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2284236.5000\n",
      "Epoch 41/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2284127.0000\n",
      "Epoch 41: loss improved from 2283513.50000 to 2282702.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282702.7500\n",
      "Epoch 42/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2281655.7500\n",
      "Epoch 42: loss improved from 2282702.75000 to 2281764.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2281764.0000\n",
      "Epoch 43/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2280888.7500\n",
      "Epoch 43: loss improved from 2281764.00000 to 2280860.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280860.7500\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2281563.7500\n",
      "Epoch 44: loss did not improve from 2280860.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2281489.2500\n",
      "Epoch 45/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2278245.5000\n",
      "Epoch 45: loss improved from 2280860.75000 to 2279814.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279814.7500\n",
      "Epoch 46/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2281732.7500\n",
      "Epoch 46: loss did not improve from 2279814.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280442.0000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2280151.2500\n",
      "Epoch 47: loss did not improve from 2279814.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280151.2500\n",
      "Epoch 48/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2279210.2500\n",
      "Epoch 48: loss did not improve from 2279814.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279853.2500\n",
      "Epoch 49/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2278132.2500\n",
      "Epoch 49: loss improved from 2279814.75000 to 2277987.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2277987.7500\n",
      "Epoch 50/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2276575.7500\n",
      "Epoch 50: loss did not improve from 2277987.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278090.5000\n",
      "Epoch 51/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2277433.2500\n",
      "Epoch 51: loss improved from 2277987.75000 to 2277752.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2277752.5000\n",
      "Epoch 52/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2276831.5000\n",
      "Epoch 52: loss improved from 2277752.50000 to 2277294.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2277294.2500\n",
      "Epoch 53/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2277888.2500\n",
      "Epoch 53: loss did not improve from 2277294.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278275.7500\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2276161.7500\n",
      "Epoch 54: loss improved from 2277294.25000 to 2276022.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276022.7500\n",
      "Epoch 55/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2279017.0000\n",
      "Epoch 55: loss did not improve from 2276022.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2278244.7500\n",
      "Epoch 56/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2277417.7500\n",
      "Epoch 56: loss did not improve from 2276022.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2276832.2500\n",
      "Epoch 57/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2277462.2500\n",
      "Epoch 57: loss improved from 2276022.75000 to 2274992.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2274992.2500\n",
      "Epoch 58/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2275562.2500\n",
      "Epoch 58: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275589.5000\n",
      "Epoch 59/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2275299.5000\n",
      "Epoch 59: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275181.5000\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2275591.0000\n",
      "Epoch 60: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275127.0000\n",
      "Epoch 61/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2275971.7500\n",
      "Epoch 61: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275112.0000\n",
      "Epoch 62/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2275186.5000\n",
      "Epoch 62: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2275378.7500\n",
      "Epoch 63/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2274865.7500\n",
      "Epoch 63: loss improved from 2274992.25000 to 2274603.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2274603.2500\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2274611.7500\n",
      "Epoch 64: loss improved from 2274603.25000 to 2274319.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2274319.5000\n",
      "Epoch 65/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2273264.7500\n",
      "Epoch 65: loss improved from 2274319.50000 to 2273081.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2273081.5000\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2274766.0000\n",
      "Epoch 66: loss did not improve from 2273081.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274415.5000\n",
      "Epoch 67/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2273320.5000\n",
      "Epoch 67: loss improved from 2273081.50000 to 2272530.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272530.5000\n",
      "Epoch 68/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2273210.2500\n",
      "Epoch 68: loss did not improve from 2272530.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272658.5000\n",
      "Epoch 69/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2271259.2500\n",
      "Epoch 69: loss improved from 2272530.50000 to 2271575.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271575.2500\n",
      "Epoch 70/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2271950.0000\n",
      "Epoch 70: loss did not improve from 2271575.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272617.7500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2271419.5000\n",
      "Epoch 71: loss improved from 2271575.25000 to 2271419.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271419.5000\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2272898.7500\n",
      "Epoch 72: loss did not improve from 2271419.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2272383.5000\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2270842.5000\n",
      "Epoch 73: loss improved from 2271419.50000 to 2271241.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271241.0000\n",
      "Epoch 74/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2270976.2500\n",
      "Epoch 74: loss did not improve from 2271241.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271451.5000\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2271179.2500\n",
      "Epoch 75: loss improved from 2271241.00000 to 2269861.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2269861.5000\n",
      "Epoch 76/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2271062.0000\n",
      "Epoch 76: loss did not improve from 2269861.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271385.0000\n",
      "Epoch 77/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2271949.0000\n",
      "Epoch 77: loss did not improve from 2269861.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271856.5000\n",
      "Epoch 78/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2269559.7500\n",
      "Epoch 78: loss did not improve from 2269861.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271255.0000\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2270864.0000\n",
      "Epoch 79: loss did not improve from 2269861.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271264.5000\n",
      "Epoch 80/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2269402.7500\n",
      "Epoch 80: loss improved from 2269861.50000 to 2269396.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269396.0000\n",
      "Epoch 81/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2269891.2500\n",
      "Epoch 81: loss did not improve from 2269396.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270655.0000\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2269133.2500\n",
      "Epoch 82: loss improved from 2269396.00000 to 2269133.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2269133.2500\n",
      "Epoch 83/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2271036.5000\n",
      "Epoch 83: loss did not improve from 2269133.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270418.5000\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2270326.0000\n",
      "Epoch 84: loss did not improve from 2269133.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269890.2500\n",
      "Epoch 85/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2270123.7500\n",
      "Epoch 85: loss did not improve from 2269133.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269377.5000\n",
      "Epoch 86/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2269897.0000\n",
      "Epoch 86: loss did not improve from 2269133.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269956.7500\n",
      "Epoch 87/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2266414.7500\n",
      "Epoch 87: loss improved from 2269133.25000 to 2269005.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269005.2500\n",
      "Epoch 88/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2268050.2500\n",
      "Epoch 88: loss improved from 2269005.25000 to 2268050.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268050.2500\n",
      "Epoch 89/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2265889.2500\n",
      "Epoch 89: loss improved from 2268050.25000 to 2266858.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266858.5000\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2270061.5000\n",
      "Epoch 90: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268645.5000\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2266799.5000\n",
      "Epoch 91: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267920.0000\n",
      "Epoch 92/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2268547.2500\n",
      "Epoch 92: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267624.0000\n",
      "Epoch 93/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2266680.0000\n",
      "Epoch 93: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268299.0000\n",
      "Epoch 94/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2266261.5000\n",
      "Epoch 94: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266977.0000\n",
      "Epoch 95/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2265476.7500\n",
      "Epoch 95: loss improved from 2266858.50000 to 2265919.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2265919.0000\n",
      "Epoch 96/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2269800.2500\n",
      "Epoch 96: loss did not improve from 2265919.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267434.5000\n",
      "Epoch 97/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2267787.0000\n",
      "Epoch 97: loss did not improve from 2265919.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266428.2500\n",
      "Epoch 98/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2267458.2500\n",
      "Epoch 98: loss did not improve from 2265919.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267604.7500\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2267436.5000\n",
      "Epoch 99: loss did not improve from 2265919.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266886.5000\n",
      "Epoch 100/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2264371.7500\n",
      "Epoch 100: loss improved from 2265919.00000 to 2264374.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2264374.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 02:05:54 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 02:05:54 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp6_rvcflb\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp6_rvcflb\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 25, 25, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 12, 12, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 6, 6, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSamplin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_14 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737094.2500\n",
      "Epoch 1: loss improved from inf to 2736504.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 5ms/step - loss: 2736504.0000\n",
      "Epoch 2/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2737521.2500\n",
      "Epoch 2: loss did not improve from 2736504.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736506.0000\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737306.7500\n",
      "Epoch 3: loss improved from 2736504.00000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.7500\n",
      "Epoch 4/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2736835.7500\n",
      "Epoch 4: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.0000\n",
      "Epoch 5/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736853.5000\n",
      "Epoch 5: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.2500\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 6: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736504.5000\n",
      "Epoch 7/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735920.0000\n",
      "Epoch 7: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736505.7500\n",
      "Epoch 8/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2734503.7500\n",
      "Epoch 8: loss improved from 2736503.75000 to 2736498.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736498.5000\n",
      "Epoch 9/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736230.7500\n",
      "Epoch 9: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736504.0000\n",
      "Epoch 10/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735528.0000\n",
      "Epoch 10: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.5000\n",
      "Epoch 11/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737574.5000\n",
      "Epoch 11: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.7500\n",
      "Epoch 12/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736100.7500\n",
      "Epoch 12: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736022.2500\n",
      "Epoch 13: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 14/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735289.2500\n",
      "Epoch 14: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.7500\n",
      "Epoch 15/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737301.7500\n",
      "Epoch 15: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 16/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737081.0000\n",
      "Epoch 16: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.2500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737010.0000\n",
      "Epoch 17: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.7500\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736552.0000\n",
      "Epoch 18: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.7500\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737216.0000\n",
      "Epoch 19: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.5000\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735598.5000\n",
      "Epoch 20: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 21/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735678.5000\n",
      "Epoch 21: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736502.5000\n",
      "Epoch 22/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2735567.0000\n",
      "Epoch 22: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736504.5000\n",
      "Epoch 23/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736732.0000\n",
      "Epoch 23: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736503.0000\n",
      "Epoch 24/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736589.2500\n",
      "Epoch 24: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.0000\n",
      "Epoch 25/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2737480.7500\n",
      "Epoch 25: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.5000\n",
      "Epoch 26/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736677.2500\n",
      "Epoch 26: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736500.2500\n",
      "Epoch 27/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736914.5000\n",
      "Epoch 27: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736499.0000\n",
      "Epoch 28/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2735519.2500\n",
      "Epoch 28: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.5000\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.2500\n",
      "Epoch 29: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.2500\n",
      "Epoch 30/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736604.7500\n",
      "Epoch 30: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.7500\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2734831.2500\n",
      "Epoch 31: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 32/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736816.0000\n",
      "Epoch 32: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.5000\n",
      "Epoch 33/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736426.5000\n",
      "Epoch 33: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.5000\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737063.7500\n",
      "Epoch 34: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.0000\n",
      "Epoch 35/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736907.0000\n",
      "Epoch 35: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736499.0000\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735358.5000\n",
      "Epoch 36: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736499.2500\n",
      "Epoch 37/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2734436.5000\n",
      "Epoch 37: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.7500\n",
      "Epoch 38/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.5000\n",
      "Epoch 38: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736499.5000\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736084.0000\n",
      "Epoch 39: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736499.5000\n",
      "Epoch 40/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2734399.2500\n",
      "Epoch 40: loss improved from 2736498.50000 to 2736498.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.0000\n",
      "Epoch 41/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736296.7500\n",
      "Epoch 41: loss improved from 2736498.00000 to 2736497.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.7500\n",
      "Epoch 42/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2734896.7500\n",
      "Epoch 42: loss improved from 2736497.75000 to 2736497.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.2500\n",
      "Epoch 43/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737436.2500\n",
      "Epoch 43: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736499.0000\n",
      "Epoch 44/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2736655.0000\n",
      "Epoch 44: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.5000\n",
      "Epoch 45/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736820.2500\n",
      "Epoch 45: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.5000\n",
      "Epoch 46/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2736139.0000\n",
      "Epoch 46: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.2500\n",
      "Epoch 47/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 47: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736500.5000\n",
      "Epoch 48/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736787.2500\n",
      "Epoch 48: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736498.0000\n",
      "Epoch 49/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737603.0000\n",
      "Epoch 49: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736497.2500\n",
      "Epoch 50/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736243.0000\n",
      "Epoch 50: loss improved from 2736497.25000 to 2736497.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.0000\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.2500\n",
      "Epoch 51: loss did not improve from 2736497.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.2500\n",
      "Epoch 52/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736806.7500\n",
      "Epoch 52: loss did not improve from 2736497.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.5000\n",
      "Epoch 53/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735848.5000\n",
      "Epoch 53: loss improved from 2736497.00000 to 2736496.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.7500\n",
      "Epoch 54/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735036.0000\n",
      "Epoch 54: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736497.7500\n",
      "Epoch 55/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736498.2500\n",
      "Epoch 55: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.2500\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736333.0000\n",
      "Epoch 56: loss improved from 2736496.75000 to 2736494.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2734443.0000\n",
      "Epoch 57: loss did not improve from 2736494.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.0000\n",
      "Epoch 58/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2736984.5000\n",
      "Epoch 58: loss did not improve from 2736494.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 59/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736712.0000\n",
      "Epoch 59: loss improved from 2736494.50000 to 2736493.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 60/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736256.7500\n",
      "Epoch 60: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.5000\n",
      "Epoch 61/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736799.7500\n",
      "Epoch 61: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.2500\n",
      "Epoch 62/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736080.2500\n",
      "Epoch 62: loss improved from 2736493.75000 to 2736493.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.2500\n",
      "Epoch 63/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736248.0000\n",
      "Epoch 63: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 64/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2735757.0000\n",
      "Epoch 64: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.5000\n",
      "Epoch 65/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736496.5000\n",
      "Epoch 65: loss improved from 2736493.25000 to 2736492.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 66/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2734986.0000\n",
      "Epoch 66: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.2500\n",
      "Epoch 67/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737703.7500\n",
      "Epoch 67: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.2500\n",
      "Epoch 68/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736538.0000\n",
      "Epoch 68: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.5000\n",
      "Epoch 69/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737260.5000\n",
      "Epoch 69: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 70/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736638.5000\n",
      "Epoch 70: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.5000\n",
      "Epoch 71/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736823.5000\n",
      "Epoch 71: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.7500\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737168.0000\n",
      "Epoch 72: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 73/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734973.5000\n",
      "Epoch 73: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 74/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736562.7500\n",
      "Epoch 74: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.7500\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735665.7500\n",
      "Epoch 75: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.2500\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736494.7500\n",
      "Epoch 76: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2736780.2500\n",
      "Epoch 77: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 78/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736340.5000\n",
      "Epoch 78: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736476.2500\n",
      "Epoch 79: loss improved from 2736492.75000 to 2736491.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.2500\n",
      "Epoch 80/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2736499.7500\n",
      "Epoch 80: loss did not improve from 2736491.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.7500\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736742.0000\n",
      "Epoch 81: loss improved from 2736491.25000 to 2736490.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.0000\n",
      "Epoch 82/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736650.7500\n",
      "Epoch 82: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736496.2500\n",
      "Epoch 83/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736361.5000\n",
      "Epoch 83: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736495.0000\n",
      "Epoch 84/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2737532.7500\n",
      "Epoch 84: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 85/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 85: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 86/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737021.2500\n",
      "Epoch 86: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 87/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736519.5000\n",
      "Epoch 87: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.2500\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736493.5000\n",
      "Epoch 88: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.0000\n",
      "Epoch 89/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737232.0000\n",
      "Epoch 89: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735662.2500\n",
      "Epoch 90: loss improved from 2736490.00000 to 2736489.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736489.2500\n",
      "Epoch 91/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 91: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736491.7500\n",
      "Epoch 92/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736157.5000\n",
      "Epoch 92: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736491.7500\n",
      "Epoch 93/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2737485.5000\n",
      "Epoch 93: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 94/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736568.5000\n",
      "Epoch 94: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 95/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736269.5000\n",
      "Epoch 95: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 96/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737250.2500\n",
      "Epoch 96: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.5000\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736490.0000\n",
      "Epoch 97: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.0000\n",
      "Epoch 98/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2736710.2500\n",
      "Epoch 98: loss improved from 2736489.25000 to 2736488.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736488.0000\n",
      "Epoch 99/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737688.0000\n",
      "Epoch 99: loss did not improve from 2736488.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736489.5000\n",
      "Epoch 100/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737287.0000\n",
      "Epoch 100: loss did not improve from 2736488.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 02:32:29 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 02:32:29 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmptl308g4u\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmptl308g4u\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_5 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 25, 25, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 12, 12, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 6, 6, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d_15 (UpSamplin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_16 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_17 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 31:14 - loss: 1190771.8750WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0078s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0078s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737485.7500\n",
      "Epoch 1: loss improved from inf to 2736505.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736505.7500\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 2: loss improved from 2736505.75000 to 2736505.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.2500\n",
      "Epoch 3/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736879.5000\n",
      "Epoch 3: loss improved from 2736505.25000 to 2736504.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736504.7500\n",
      "Epoch 4/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736708.7500\n",
      "Epoch 4: loss improved from 2736504.75000 to 2736503.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.5000\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737004.0000\n",
      "Epoch 5: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.2500\n",
      "Epoch 6/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737117.2500\n",
      "Epoch 6: loss improved from 2736503.50000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.7500\n",
      "Epoch 7/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736350.7500\n",
      "Epoch 7: loss improved from 2736502.75000 to 2736500.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736500.5000\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735686.2500\n",
      "Epoch 8: loss improved from 2736500.50000 to 2736457.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2736457.2500\n",
      "Epoch 9/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2712281.7500\n",
      "Epoch 9: loss improved from 2736457.25000 to 2711936.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2711936.7500\n",
      "Epoch 10/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2592624.2500\n",
      "Epoch 10: loss improved from 2711936.75000 to 2591686.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2591686.7500\n",
      "Epoch 11/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2426374.0000\n",
      "Epoch 11: loss improved from 2591686.75000 to 2425918.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2425918.5000\n",
      "Epoch 12/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2313225.2500\n",
      "Epoch 12: loss improved from 2425918.50000 to 2314266.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2314266.5000\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2230242.2500\n",
      "Epoch 13: loss improved from 2314266.50000 to 2230752.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2230752.0000\n",
      "Epoch 14/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2158032.5000\n",
      "Epoch 14: loss improved from 2230752.00000 to 2157928.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2157928.0000\n",
      "Epoch 15/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2103704.0000\n",
      "Epoch 15: loss improved from 2157928.00000 to 2103756.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2103756.7500\n",
      "Epoch 16/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2054287.6250\n",
      "Epoch 16: loss improved from 2103756.75000 to 2054096.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2054096.8750\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2012018.1250\n",
      "Epoch 17: loss improved from 2054096.87500 to 2011955.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2011955.1250\n",
      "Epoch 18/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1974892.2500\n",
      "Epoch 18: loss improved from 2011955.12500 to 1975849.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 1975849.6250\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1945165.3750\n",
      "Epoch 19: loss improved from 1975849.62500 to 1944839.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 1944839.0000\n",
      "Epoch 20/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1916722.2500\n",
      "Epoch 20: loss improved from 1944839.00000 to 1917358.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1917358.1250\n",
      "Epoch 21/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1890127.7500\n",
      "Epoch 21: loss improved from 1917358.12500 to 1890463.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1890463.5000\n",
      "Epoch 22/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1864895.6250\n",
      "Epoch 22: loss improved from 1890463.50000 to 1865216.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 1865216.8750\n",
      "Epoch 23/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1842959.0000\n",
      "Epoch 23: loss improved from 1865216.87500 to 1843672.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1843672.1250\n",
      "Epoch 24/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1829169.2500\n",
      "Epoch 24: loss improved from 1843672.12500 to 1828230.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1828230.6250\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1805806.1250\n",
      "Epoch 25: loss improved from 1828230.62500 to 1806885.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1806885.0000\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1794375.6250\n",
      "Epoch 26: loss improved from 1806885.00000 to 1794375.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1794375.6250\n",
      "Epoch 27/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1775834.0000\n",
      "Epoch 27: loss improved from 1794375.62500 to 1776014.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1776014.5000\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1761623.6250\n",
      "Epoch 28: loss improved from 1776014.50000 to 1761623.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1761623.6250\n",
      "Epoch 29/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1748271.7500\n",
      "Epoch 29: loss improved from 1761623.62500 to 1748245.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1748245.3750\n",
      "Epoch 30/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1734180.8750\n",
      "Epoch 30: loss improved from 1748245.37500 to 1734419.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1734419.3750\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1725191.6250\n",
      "Epoch 31: loss improved from 1734419.37500 to 1725614.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1725614.0000\n",
      "Epoch 32/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1715219.6250\n",
      "Epoch 32: loss improved from 1725614.00000 to 1715040.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1715040.1250\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1703587.2500\n",
      "Epoch 33: loss improved from 1715040.12500 to 1703587.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1703587.2500\n",
      "Epoch 34/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1692979.1250\n",
      "Epoch 34: loss improved from 1703587.25000 to 1693043.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1693043.3750\n",
      "Epoch 35/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1685125.7500\n",
      "Epoch 35: loss improved from 1693043.37500 to 1685753.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1685753.0000\n",
      "Epoch 36/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1673600.1250\n",
      "Epoch 36: loss improved from 1685753.00000 to 1673367.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1673367.6250\n",
      "Epoch 37/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1665712.1250\n",
      "Epoch 37: loss improved from 1673367.62500 to 1666522.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1666522.3750\n",
      "Epoch 38/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1658402.1250\n",
      "Epoch 38: loss improved from 1666522.37500 to 1658514.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1658514.7500\n",
      "Epoch 39/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1651478.7500\n",
      "Epoch 39: loss improved from 1658514.75000 to 1651187.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1651187.0000\n",
      "Epoch 40/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1643982.3750\n",
      "Epoch 40: loss improved from 1651187.00000 to 1642858.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1642858.1250\n",
      "Epoch 41/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1637078.5000\n",
      "Epoch 41: loss improved from 1642858.12500 to 1636400.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1636400.3750\n",
      "Epoch 42/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1631546.1250\n",
      "Epoch 42: loss improved from 1636400.37500 to 1631824.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1631824.1250\n",
      "Epoch 43/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1626029.7500\n",
      "Epoch 43: loss improved from 1631824.12500 to 1625591.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1625591.7500\n",
      "Epoch 44/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1617191.3750\n",
      "Epoch 44: loss improved from 1625591.75000 to 1616923.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1616923.8750\n",
      "Epoch 45/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1611799.7500\n",
      "Epoch 45: loss improved from 1616923.87500 to 1611654.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1611654.3750\n",
      "Epoch 46/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1606607.8750\n",
      "Epoch 46: loss improved from 1611654.37500 to 1606062.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1606062.0000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1600622.0000\n",
      "Epoch 47: loss improved from 1606062.00000 to 1600622.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1600622.0000\n",
      "Epoch 48/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1598586.7500\n",
      "Epoch 48: loss improved from 1600622.00000 to 1597788.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1597788.2500\n",
      "Epoch 49/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1591242.3750\n",
      "Epoch 49: loss improved from 1597788.25000 to 1591440.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1591440.7500\n",
      "Epoch 50/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1587485.7500\n",
      "Epoch 50: loss improved from 1591440.75000 to 1587485.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1587485.7500\n",
      "Epoch 51/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1580974.2500\n",
      "Epoch 51: loss improved from 1587485.75000 to 1580570.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1580570.0000\n",
      "Epoch 52/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1576367.0000\n",
      "Epoch 52: loss improved from 1580570.00000 to 1576367.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1576367.0000\n",
      "Epoch 53/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1572101.6250\n",
      "Epoch 53: loss improved from 1576367.00000 to 1572326.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1572326.1250\n",
      "Epoch 54/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1564625.5000\n",
      "Epoch 54: loss improved from 1572326.12500 to 1564658.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1564658.2500\n",
      "Epoch 55/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1559151.8750\n",
      "Epoch 55: loss improved from 1564658.25000 to 1559404.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1559404.6250\n",
      "Epoch 56/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1555914.5000\n",
      "Epoch 56: loss improved from 1559404.62500 to 1555913.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1555913.8750\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1555625.5000\n",
      "Epoch 57: loss improved from 1555913.87500 to 1555351.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1555351.6250\n",
      "Epoch 58/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1552623.6250\n",
      "Epoch 58: loss improved from 1555351.62500 to 1552530.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1552530.2500\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1546322.0000\n",
      "Epoch 59: loss improved from 1552530.25000 to 1546095.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1546095.2500\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1542712.0000\n",
      "Epoch 60: loss improved from 1546095.25000 to 1542712.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1542712.0000\n",
      "Epoch 61/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1540378.8750\n",
      "Epoch 61: loss improved from 1542712.00000 to 1540301.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1540301.5000\n",
      "Epoch 62/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1535703.1250\n",
      "Epoch 62: loss improved from 1540301.50000 to 1535785.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1535785.5000\n",
      "Epoch 63/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1529692.2500\n",
      "Epoch 63: loss improved from 1535785.50000 to 1530005.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1530005.7500\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1530826.1250\n",
      "Epoch 64: loss did not improve from 1530005.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1530588.1250\n",
      "Epoch 65/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1525980.2500\n",
      "Epoch 65: loss improved from 1530005.75000 to 1525994.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1525994.8750\n",
      "Epoch 66/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1523983.5000\n",
      "Epoch 66: loss improved from 1525994.87500 to 1523359.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1523359.8750\n",
      "Epoch 67/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1517808.6250\n",
      "Epoch 67: loss improved from 1523359.87500 to 1518450.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1518450.1250\n",
      "Epoch 68/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1517530.2500\n",
      "Epoch 68: loss improved from 1518450.12500 to 1516932.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1516932.8750\n",
      "Epoch 69/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1513301.2500\n",
      "Epoch 69: loss improved from 1516932.87500 to 1512569.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1512569.8750\n",
      "Epoch 70/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1511622.3750\n",
      "Epoch 70: loss improved from 1512569.87500 to 1510849.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1510849.3750\n",
      "Epoch 71/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1505586.5000\n",
      "Epoch 71: loss improved from 1510849.37500 to 1505605.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1505605.7500\n",
      "Epoch 72/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1502768.1250\n",
      "Epoch 72: loss improved from 1505605.75000 to 1502960.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1502960.5000\n",
      "Epoch 73/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1498809.3750\n",
      "Epoch 73: loss improved from 1502960.50000 to 1500112.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1500112.7500\n",
      "Epoch 74/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1500857.8750\n",
      "Epoch 74: loss did not improve from 1500112.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1500185.0000\n",
      "Epoch 75/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1497160.1250\n",
      "Epoch 75: loss improved from 1500112.75000 to 1496521.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1496521.2500\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1492960.0000\n",
      "Epoch 76: loss improved from 1496521.25000 to 1492960.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1492960.0000\n",
      "Epoch 77/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1492493.7500\n",
      "Epoch 77: loss improved from 1492960.00000 to 1492463.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1492463.1250\n",
      "Epoch 78/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1488577.8750\n",
      "Epoch 78: loss improved from 1492463.12500 to 1488188.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1488188.8750\n",
      "Epoch 79/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1488041.3750\n",
      "Epoch 79: loss improved from 1488188.87500 to 1488013.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1488013.0000\n",
      "Epoch 80/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1482831.6250\n",
      "Epoch 80: loss improved from 1488013.00000 to 1482115.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1482115.0000\n",
      "Epoch 81/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1480339.0000\n",
      "Epoch 81: loss improved from 1482115.00000 to 1481972.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1481972.7500\n",
      "Epoch 82/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1478803.6250\n",
      "Epoch 82: loss improved from 1481972.75000 to 1478818.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1478818.0000\n",
      "Epoch 83/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1476501.8750\n",
      "Epoch 83: loss improved from 1478818.00000 to 1475697.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1475697.3750\n",
      "Epoch 84/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1475082.3750\n",
      "Epoch 84: loss improved from 1475697.37500 to 1475082.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1475082.3750\n",
      "Epoch 85/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1475018.0000\n",
      "Epoch 85: loss improved from 1475082.37500 to 1474342.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1474342.8750\n",
      "Epoch 86/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1473714.7500\n",
      "Epoch 86: loss improved from 1474342.87500 to 1473314.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1473314.7500\n",
      "Epoch 87/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1472092.1250\n",
      "Epoch 87: loss improved from 1473314.75000 to 1471350.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1471350.8750\n",
      "Epoch 88/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1469596.2500\n",
      "Epoch 88: loss improved from 1471350.87500 to 1468652.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1468652.2500\n",
      "Epoch 89/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1466246.0000\n",
      "Epoch 89: loss improved from 1468652.25000 to 1465703.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1465703.2500\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1460082.1250\n",
      "Epoch 90: loss improved from 1465703.25000 to 1460082.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1460082.1250\n",
      "Epoch 91/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1460518.2500\n",
      "Epoch 91: loss did not improve from 1460082.12500\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1460942.5000\n",
      "Epoch 92/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1459319.7500\n",
      "Epoch 92: loss improved from 1460082.12500 to 1459054.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1459054.3750\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1458140.3750\n",
      "Epoch 93: loss improved from 1459054.37500 to 1458140.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1458140.3750\n",
      "Epoch 94/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1455779.3750\n",
      "Epoch 94: loss improved from 1458140.37500 to 1455769.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1455769.3750\n",
      "Epoch 95/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1455885.8750\n",
      "Epoch 95: loss did not improve from 1455769.37500\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1455771.8750\n",
      "Epoch 96/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1450295.8750\n",
      "Epoch 96: loss improved from 1455769.37500 to 1450664.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1450664.3750\n",
      "Epoch 97/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1449873.2500\n",
      "Epoch 97: loss improved from 1450664.37500 to 1450626.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1450626.6250\n",
      "Epoch 98/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1448585.2500\n",
      "Epoch 98: loss improved from 1450626.62500 to 1448553.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1448553.5000\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1446657.6250\n",
      "Epoch 99: loss improved from 1448553.50000 to 1446657.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1446657.6250\n",
      "Epoch 100/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1447092.1250\n",
      "Epoch 100: loss did not improve from 1446657.62500\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1447092.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 02:59:19 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 02:59:19 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpb_92g71b\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpb_92g71b\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_6 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_48 (Conv2D)          (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 25, 25, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 25, 25, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 6, 6, 32)          18464     \n",
      "                                                                 \n",
      " up_sampling2d_18 (UpSamplin  (None, 12, 12, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_52 (Conv2D)          (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " up_sampling2d_19 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_20 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,132,005\n",
      "Trainable params: 6,132,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 2:32 - loss: 1759726.6250  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0200s vs `on_train_batch_end` time: 0.0257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0200s vs `on_train_batch_end` time: 0.0257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737587.2500\n",
      "Epoch 1: loss improved from inf to 2736502.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 34s 9ms/step - loss: 2736502.5000\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 2: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736504.5000\n",
      "Epoch 3/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737407.2500\n",
      "Epoch 3: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736503.0000\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 4: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736506.0000\n",
      "Epoch 5/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736430.2500\n",
      "Epoch 5: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736503.0000\n",
      "Epoch 6/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736722.2500\n",
      "Epoch 6: loss improved from 2736502.50000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736501.5000\n",
      "Epoch 7/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736917.0000\n",
      "Epoch 7: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736511.0000\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736972.7500\n",
      "Epoch 8: loss improved from 2736501.50000 to 2736490.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736490.2500\n",
      "Epoch 9/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737230.7500\n",
      "Epoch 9: loss improved from 2736490.25000 to 2736465.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736465.0000\n",
      "Epoch 10/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735948.0000\n",
      "Epoch 10: loss improved from 2736465.00000 to 2736157.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736157.5000\n",
      "Epoch 11/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2667340.0000\n",
      "Epoch 11: loss improved from 2736157.50000 to 2666695.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2666695.7500\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2412635.2500\n",
      "Epoch 12: loss improved from 2666695.75000 to 2412635.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2412635.2500\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2186094.5000\n",
      "Epoch 13: loss improved from 2412635.25000 to 2185657.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2185657.7500\n",
      "Epoch 14/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2046262.7500\n",
      "Epoch 14: loss improved from 2185657.75000 to 2046137.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2046137.8750\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1934974.3750\n",
      "Epoch 15: loss improved from 2046137.87500 to 1934852.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1934852.8750\n",
      "Epoch 16/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1839960.5000\n",
      "Epoch 16: loss improved from 1934852.87500 to 1839960.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1839960.5000\n",
      "Epoch 17/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1764416.6250\n",
      "Epoch 17: loss improved from 1839960.50000 to 1764455.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1764455.0000\n",
      "Epoch 18/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1697337.3750\n",
      "Epoch 18: loss improved from 1764455.00000 to 1697020.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1697020.5000\n",
      "Epoch 19/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1651575.2500\n",
      "Epoch 19: loss improved from 1697020.50000 to 1651536.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1651536.7500\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1603852.5000\n",
      "Epoch 20: loss improved from 1651536.75000 to 1603170.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1603170.0000\n",
      "Epoch 21/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1567191.2500\n",
      "Epoch 21: loss improved from 1603170.00000 to 1566833.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1566833.1250\n",
      "Epoch 22/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1530925.8750\n",
      "Epoch 22: loss improved from 1566833.12500 to 1531247.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1531247.2500\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1504048.1250\n",
      "Epoch 23: loss improved from 1531247.25000 to 1503683.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1503683.3750\n",
      "Epoch 24/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1475016.6250\n",
      "Epoch 24: loss improved from 1503683.37500 to 1475301.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1475301.6250\n",
      "Epoch 25/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1445265.8750\n",
      "Epoch 25: loss improved from 1475301.62500 to 1444888.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1444888.7500\n",
      "Epoch 26/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1424856.3750\n",
      "Epoch 26: loss improved from 1444888.75000 to 1424681.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1424681.6250\n",
      "Epoch 27/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1404147.0000\n",
      "Epoch 27: loss improved from 1424681.62500 to 1405311.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1405311.6250\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1386553.8750\n",
      "Epoch 28: loss improved from 1405311.62500 to 1385991.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1385991.3750\n",
      "Epoch 29/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1363698.1250\n",
      "Epoch 29: loss improved from 1385991.37500 to 1363883.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1363883.7500\n",
      "Epoch 30/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1354457.2500\n",
      "Epoch 30: loss improved from 1363883.75000 to 1354457.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1354457.2500\n",
      "Epoch 31/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1342219.3750\n",
      "Epoch 31: loss improved from 1354457.25000 to 1342349.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 1342349.7500\n",
      "Epoch 32/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1325160.0000\n",
      "Epoch 32: loss improved from 1342349.75000 to 1325148.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1325148.6250\n",
      "Epoch 33/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1315740.7500\n",
      "Epoch 33: loss improved from 1325148.62500 to 1316114.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1316114.2500\n",
      "Epoch 34/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1303959.6250\n",
      "Epoch 34: loss improved from 1316114.25000 to 1303264.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1303264.8750\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1291568.0000\n",
      "Epoch 35: loss improved from 1303264.87500 to 1291601.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1291601.5000\n",
      "Epoch 36/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1283220.7500\n",
      "Epoch 36: loss improved from 1291601.50000 to 1283009.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1283009.5000\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1274087.6250\n",
      "Epoch 37: loss improved from 1283009.50000 to 1274572.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1274572.5000\n",
      "Epoch 38/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1264495.6250\n",
      "Epoch 38: loss improved from 1274572.50000 to 1264732.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1264732.2500\n",
      "Epoch 39/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1255409.2500\n",
      "Epoch 39: loss improved from 1264732.25000 to 1255021.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1255021.8750\n",
      "Epoch 40/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1245696.0000\n",
      "Epoch 40: loss improved from 1255021.87500 to 1245782.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1245782.6250\n",
      "Epoch 41/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1236250.5000\n",
      "Epoch 41: loss improved from 1245782.62500 to 1235870.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1235870.8750\n",
      "Epoch 42/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1230480.6250\n",
      "Epoch 42: loss improved from 1235870.87500 to 1229958.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1229958.8750\n",
      "Epoch 43/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1227169.1250\n",
      "Epoch 43: loss improved from 1229958.87500 to 1227203.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1227203.8750\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1216095.8750\n",
      "Epoch 44: loss improved from 1227203.87500 to 1215798.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1215798.8750\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1209225.2500\n",
      "Epoch 45: loss improved from 1215798.87500 to 1208458.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1208458.8750\n",
      "Epoch 46/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1204873.0000\n",
      "Epoch 46: loss improved from 1208458.87500 to 1204918.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1204918.1250\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1198221.3750\n",
      "Epoch 47: loss improved from 1204918.12500 to 1199046.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1199046.3750\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1188430.1250\n",
      "Epoch 48: loss improved from 1199046.37500 to 1188206.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1188206.3750\n",
      "Epoch 49/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1186744.3750\n",
      "Epoch 49: loss improved from 1188206.37500 to 1186974.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1186974.1250\n",
      "Epoch 50/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1176911.8750\n",
      "Epoch 50: loss improved from 1186974.12500 to 1177166.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1177166.7500\n",
      "Epoch 51/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1173856.2500\n",
      "Epoch 51: loss improved from 1177166.75000 to 1173553.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1173553.2500\n",
      "Epoch 52/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1170965.0000\n",
      "Epoch 52: loss improved from 1173553.25000 to 1170943.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1170943.8750\n",
      "Epoch 53/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1162135.7500\n",
      "Epoch 53: loss improved from 1170943.87500 to 1161970.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1161970.1250\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1159902.5000\n",
      "Epoch 54: loss improved from 1161970.12500 to 1159467.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1159467.2500\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1156101.0000\n",
      "Epoch 55: loss improved from 1159467.25000 to 1155836.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1155836.5000\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1151116.0000\n",
      "Epoch 56: loss improved from 1155836.50000 to 1151213.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1151213.2500\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1146445.6250\n",
      "Epoch 57: loss improved from 1151213.25000 to 1146445.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1146445.6250\n",
      "Epoch 58/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1140343.5000\n",
      "Epoch 58: loss improved from 1146445.62500 to 1140853.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1140853.8750\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1139425.6250\n",
      "Epoch 59: loss improved from 1140853.87500 to 1139223.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1139223.1250\n",
      "Epoch 60/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1133151.2500\n",
      "Epoch 60: loss improved from 1139223.12500 to 1132825.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1132825.3750\n",
      "Epoch 61/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1130762.1250\n",
      "Epoch 61: loss improved from 1132825.37500 to 1130554.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1130554.6250\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1125948.1250\n",
      "Epoch 62: loss improved from 1130554.62500 to 1125948.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1125948.1250\n",
      "Epoch 63/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1122156.3750\n",
      "Epoch 63: loss improved from 1125948.12500 to 1122139.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1122139.1250\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1116826.1250\n",
      "Epoch 64: loss improved from 1122139.12500 to 1116694.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1116694.7500\n",
      "Epoch 65/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1121636.7500\n",
      "Epoch 65: loss did not improve from 1116694.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1121647.1250\n",
      "Epoch 66/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1108597.7500\n",
      "Epoch 66: loss improved from 1116694.75000 to 1108520.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1108520.5000\n",
      "Epoch 67/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1110740.5000\n",
      "Epoch 67: loss did not improve from 1108520.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1110534.1250\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1102590.0000\n",
      "Epoch 68: loss improved from 1108520.50000 to 1102519.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1102519.0000\n",
      "Epoch 69/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1103417.6250\n",
      "Epoch 69: loss did not improve from 1102519.00000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1103227.3750\n",
      "Epoch 70/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1102250.8750\n",
      "Epoch 70: loss improved from 1102519.00000 to 1101904.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1101904.5000\n",
      "Epoch 71/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1101039.0000\n",
      "Epoch 71: loss improved from 1101904.50000 to 1100642.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1100642.7500\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1093749.6250\n",
      "Epoch 72: loss improved from 1100642.75000 to 1093749.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1093749.6250\n",
      "Epoch 73/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1090338.8750\n",
      "Epoch 73: loss improved from 1093749.62500 to 1093145.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1093145.6250\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1090763.8750\n",
      "Epoch 74: loss improved from 1093145.62500 to 1090754.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1090754.8750\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1084689.8750\n",
      "Epoch 75: loss improved from 1090754.87500 to 1084732.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1084732.0000\n",
      "Epoch 76/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1081160.7500\n",
      "Epoch 76: loss improved from 1084732.00000 to 1081170.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1081170.6250\n",
      "Epoch 77/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1080737.0000\n",
      "Epoch 77: loss improved from 1081170.62500 to 1080595.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1080595.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1078029.6250\n",
      "Epoch 78: loss improved from 1080595.50000 to 1078117.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 1078117.7500\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1075496.8750\n",
      "Epoch 79: loss improved from 1078117.75000 to 1075863.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1075863.2500\n",
      "Epoch 80/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1070754.8750\n",
      "Epoch 80: loss improved from 1075863.25000 to 1070754.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1070754.8750\n",
      "Epoch 81/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1068430.2500\n",
      "Epoch 81: loss improved from 1070754.87500 to 1067887.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 1067887.6250\n",
      "Epoch 82/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1072490.8750\n",
      "Epoch 82: loss did not improve from 1067887.62500\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1072693.1250\n",
      "Epoch 83/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1066806.5000\n",
      "Epoch 83: loss improved from 1067887.62500 to 1066529.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1066529.6250\n",
      "Epoch 84/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1064955.5000\n",
      "Epoch 84: loss improved from 1066529.62500 to 1064955.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1064955.5000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1060883.8750\n",
      "Epoch 85: loss improved from 1064955.50000 to 1060435.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1060435.0000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1064520.3750\n",
      "Epoch 86: loss did not improve from 1060435.00000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1064520.3750\n",
      "Epoch 87/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1056904.7500\n",
      "Epoch 87: loss improved from 1060435.00000 to 1056673.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 1056673.3750\n",
      "Epoch 88/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1056260.6250\n",
      "Epoch 88: loss improved from 1056673.37500 to 1056184.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1056184.1250\n",
      "Epoch 89/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1054712.6250\n",
      "Epoch 89: loss improved from 1056184.12500 to 1054712.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1054712.6250\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1051810.0000\n",
      "Epoch 90: loss improved from 1054712.62500 to 1051810.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1051810.0000\n",
      "Epoch 91/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1051361.2500\n",
      "Epoch 91: loss did not improve from 1051810.00000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1051956.6250\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1049212.8750\n",
      "Epoch 92: loss improved from 1051810.00000 to 1049131.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1049131.7500\n",
      "Epoch 93/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1044642.7500\n",
      "Epoch 93: loss improved from 1049131.75000 to 1044709.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1044709.0000\n",
      "Epoch 94/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1047303.5000\n",
      "Epoch 94: loss did not improve from 1044709.00000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1047125.0625\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1043940.8750\n",
      "Epoch 95: loss improved from 1044709.00000 to 1043940.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1043940.8750\n",
      "Epoch 96/100\n",
      "2863/3189 [=========================>....] - ETA: 2s - loss: 1042831.1250"
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=3\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\"vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting activations and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 06:54:01 INFO mlflow.tracking.fluent: Experiment with name 'dnn3vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_111 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 2:01:38 - loss: 3577546.7500WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0381s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0381s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3189 [============================>.] - ETA: 0s - loss: 2687807.5000\n",
      "Epoch 1: loss improved from inf to 2687672.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 4ms/step - loss: 2687672.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2596229.2500\n",
      "Epoch 2: loss improved from 2687672.50000 to 2596205.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2596205.0000\n",
      "Epoch 3/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2538013.5000\n",
      "Epoch 3: loss improved from 2596205.00000 to 2537736.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2537736.7500\n",
      "Epoch 4/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2492994.5000\n",
      "Epoch 4: loss improved from 2537736.75000 to 2492932.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2492932.7500\n",
      "Epoch 5/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2452536.5000\n",
      "Epoch 5: loss improved from 2492932.75000 to 2452682.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2452682.2500\n",
      "Epoch 6/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2417745.0000\n",
      "Epoch 6: loss improved from 2452682.25000 to 2417568.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2417568.2500\n",
      "Epoch 7/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2388616.7500\n",
      "Epoch 7: loss improved from 2417568.25000 to 2388050.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2388050.7500\n",
      "Epoch 8/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2362039.5000\n",
      "Epoch 8: loss improved from 2388050.75000 to 2361705.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2361705.7500\n",
      "Epoch 9/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2336990.2500\n",
      "Epoch 9: loss improved from 2361705.75000 to 2338327.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2338327.7500\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2315327.5000\n",
      "Epoch 10: loss improved from 2338327.75000 to 2315918.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2315918.0000\n",
      "Epoch 11/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2296146.2500\n",
      "Epoch 11: loss improved from 2315918.00000 to 2295780.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2295780.0000\n",
      "Epoch 12/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2276998.5000\n",
      "Epoch 12: loss improved from 2295780.00000 to 2277084.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2277084.0000\n",
      "Epoch 13/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2258753.0000\n",
      "Epoch 13: loss improved from 2277084.00000 to 2258755.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2258755.2500\n",
      "Epoch 14/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2242001.0000\n",
      "Epoch 14: loss improved from 2258755.25000 to 2242001.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2242001.0000\n",
      "Epoch 15/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2226043.2500\n",
      "Epoch 15: loss improved from 2242001.00000 to 2225932.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2225932.2500\n",
      "Epoch 16/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2211845.5000\n",
      "Epoch 16: loss improved from 2225932.25000 to 2211667.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2211667.2500\n",
      "Epoch 17/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2197039.0000\n",
      "Epoch 17: loss improved from 2211667.25000 to 2197138.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2197138.7500\n",
      "Epoch 18/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2185156.2500\n",
      "Epoch 18: loss improved from 2197138.75000 to 2185959.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2185959.0000\n",
      "Epoch 19/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2173387.2500\n",
      "Epoch 19: loss improved from 2185959.00000 to 2173607.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2173607.2500\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2162170.0000\n",
      "Epoch 20: loss improved from 2173607.25000 to 2161214.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2161214.7500\n",
      "Epoch 21/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2152593.5000\n",
      "Epoch 21: loss improved from 2161214.75000 to 2153117.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2153117.2500\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2140848.0000\n",
      "Epoch 22: loss improved from 2153117.25000 to 2141345.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2141345.2500\n",
      "Epoch 23/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2132788.2500\n",
      "Epoch 23: loss improved from 2141345.25000 to 2132183.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2132183.5000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2122215.0000\n",
      "Epoch 24: loss improved from 2132183.50000 to 2123399.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2123399.2500\n",
      "Epoch 25/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2117789.2500\n",
      "Epoch 25: loss improved from 2123399.25000 to 2117526.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2117526.0000\n",
      "Epoch 26/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2107234.5000\n",
      "Epoch 26: loss improved from 2117526.00000 to 2108733.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2108733.2500\n",
      "Epoch 27/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2101747.2500\n",
      "Epoch 27: loss improved from 2108733.25000 to 2101903.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2101903.5000\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2096543.8750\n",
      "Epoch 28: loss improved from 2101903.50000 to 2096543.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2096543.8750\n",
      "Epoch 29/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2086963.5000\n",
      "Epoch 29: loss improved from 2096543.87500 to 2089090.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2089090.8750\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2083670.1250\n",
      "Epoch 30: loss improved from 2089090.87500 to 2083478.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2083478.3750\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2077010.7500\n",
      "Epoch 31: loss improved from 2083478.37500 to 2077516.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2077516.2500\n",
      "Epoch 32/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2069330.3750\n",
      "Epoch 32: loss improved from 2077516.25000 to 2070180.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2070180.1250\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2065142.2500\n",
      "Epoch 33: loss improved from 2070180.12500 to 2065142.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2065142.2500\n",
      "Epoch 34/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2060730.6250\n",
      "Epoch 34: loss improved from 2065142.25000 to 2060604.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2060604.6250\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2055208.3750\n",
      "Epoch 35: loss improved from 2060604.62500 to 2055208.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2055208.3750\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2050071.5000\n",
      "Epoch 36: loss improved from 2055208.37500 to 2049879.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2049879.7500\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2045495.5000\n",
      "Epoch 37: loss improved from 2049879.75000 to 2046359.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2046359.1250\n",
      "Epoch 38/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2040377.6250\n",
      "Epoch 38: loss improved from 2046359.12500 to 2040648.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2040648.0000\n",
      "Epoch 39/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2036224.3750\n",
      "Epoch 39: loss improved from 2040648.00000 to 2036267.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2036267.1250\n",
      "Epoch 40/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2030791.0000\n",
      "Epoch 40: loss improved from 2036267.12500 to 2031258.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2031258.7500\n",
      "Epoch 41/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2026679.2500\n",
      "Epoch 41: loss improved from 2031258.75000 to 2026790.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2026790.8750\n",
      "Epoch 42/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2022820.5000\n",
      "Epoch 42: loss improved from 2026790.87500 to 2023465.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2023465.1250\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2018969.6250\n",
      "Epoch 43: loss improved from 2023465.12500 to 2019996.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2019996.0000\n",
      "Epoch 44/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2017003.1250\n",
      "Epoch 44: loss improved from 2019996.00000 to 2016786.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2016786.1250\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2013293.7500\n",
      "Epoch 45: loss improved from 2016786.12500 to 2013293.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2013293.7500\n",
      "Epoch 46/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2009508.0000\n",
      "Epoch 46: loss improved from 2013293.75000 to 2009419.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2009419.5000\n",
      "Epoch 47/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2007460.8750\n",
      "Epoch 47: loss improved from 2009419.50000 to 2006738.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2006738.0000\n",
      "Epoch 48/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2003283.3750\n",
      "Epoch 48: loss improved from 2006738.00000 to 2003924.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2003924.6250\n",
      "Epoch 49/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2000731.6250\n",
      "Epoch 49: loss improved from 2003924.62500 to 2000765.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2000765.0000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1996815.5000\n",
      "Epoch 50: loss improved from 2000765.00000 to 1997808.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1997808.5000\n",
      "Epoch 51/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1992944.5000\n",
      "Epoch 51: loss improved from 1997808.50000 to 1993104.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1993104.5000\n",
      "Epoch 52/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1991359.6250\n",
      "Epoch 52: loss improved from 1993104.50000 to 1991190.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1991190.7500\n",
      "Epoch 53/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1989336.0000\n",
      "Epoch 53: loss improved from 1991190.75000 to 1987855.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1987855.1250\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1986433.5000\n",
      "Epoch 54: loss improved from 1987855.12500 to 1986630.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1986630.1250\n",
      "Epoch 55/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1981570.3750\n",
      "Epoch 55: loss improved from 1986630.12500 to 1981611.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1981611.0000\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1978213.5000\n",
      "Epoch 56: loss improved from 1981611.00000 to 1978434.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1978434.3750\n",
      "Epoch 57/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1977635.8750\n",
      "Epoch 57: loss improved from 1978434.37500 to 1978246.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1978246.5000\n",
      "Epoch 58/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1974048.8750\n",
      "Epoch 58: loss improved from 1978246.50000 to 1973310.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1973310.3750\n",
      "Epoch 59/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1972422.0000\n",
      "Epoch 59: loss improved from 1973310.37500 to 1972218.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1972218.2500\n",
      "Epoch 60/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1968857.5000\n",
      "Epoch 60: loss improved from 1972218.25000 to 1968803.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1968803.8750\n",
      "Epoch 61/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1966651.2500\n",
      "Epoch 61: loss improved from 1968803.87500 to 1966065.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1966065.7500\n",
      "Epoch 62/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1965104.7500\n",
      "Epoch 62: loss improved from 1966065.75000 to 1965393.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1965393.5000\n",
      "Epoch 63/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1962917.2500\n",
      "Epoch 63: loss improved from 1965393.50000 to 1962917.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1962917.2500\n",
      "Epoch 64/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1959908.0000\n",
      "Epoch 64: loss improved from 1962917.25000 to 1959630.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1959630.2500\n",
      "Epoch 65/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1959298.1250\n",
      "Epoch 65: loss improved from 1959630.25000 to 1958594.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1958594.3750\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1955499.5000\n",
      "Epoch 66: loss improved from 1958594.37500 to 1955499.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 1955499.5000\n",
      "Epoch 67/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1951248.0000\n",
      "Epoch 67: loss improved from 1955499.50000 to 1952766.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1952766.7500\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1952107.0000\n",
      "Epoch 68: loss improved from 1952766.75000 to 1951630.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1951630.3750\n",
      "Epoch 69/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1948620.3750\n",
      "Epoch 69: loss improved from 1951630.37500 to 1948855.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1948855.1250\n",
      "Epoch 70/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1946400.6250\n",
      "Epoch 70: loss improved from 1948855.12500 to 1947273.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1947273.0000\n",
      "Epoch 71/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1944955.1250\n",
      "Epoch 71: loss improved from 1947273.00000 to 1945437.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1945437.1250\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1942304.1250\n",
      "Epoch 72: loss improved from 1945437.12500 to 1942741.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1942741.5000\n",
      "Epoch 73/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1941499.5000\n",
      "Epoch 73: loss improved from 1942741.50000 to 1940844.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1940844.3750\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1937034.2500\n",
      "Epoch 74: loss improved from 1940844.37500 to 1937034.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1937034.2500\n",
      "Epoch 75/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1938589.0000\n",
      "Epoch 75: loss did not improve from 1937034.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1938348.7500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1936139.5000\n",
      "Epoch 76: loss improved from 1937034.25000 to 1935134.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1935134.7500\n",
      "Epoch 77/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1936239.0000\n",
      "Epoch 77: loss improved from 1935134.75000 to 1934523.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1934523.6250\n",
      "Epoch 78/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1932567.0000\n",
      "Epoch 78: loss improved from 1934523.62500 to 1930983.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1930983.1250\n",
      "Epoch 79/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1929309.6250\n",
      "Epoch 79: loss improved from 1930983.12500 to 1928756.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1928756.0000\n",
      "Epoch 80/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1929085.8750\n",
      "Epoch 80: loss did not improve from 1928756.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1928936.7500\n",
      "Epoch 81/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1926346.1250\n",
      "Epoch 81: loss improved from 1928756.00000 to 1926478.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1926478.1250\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1925085.5000\n",
      "Epoch 82: loss improved from 1926478.12500 to 1924823.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1924823.0000\n",
      "Epoch 83/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1922673.0000\n",
      "Epoch 83: loss improved from 1924823.00000 to 1922730.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1922730.7500\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1921605.8750\n",
      "Epoch 84: loss improved from 1922730.75000 to 1921263.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1921263.6250\n",
      "Epoch 85/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1920814.5000\n",
      "Epoch 85: loss improved from 1921263.62500 to 1920375.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1920375.0000\n",
      "Epoch 86/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1918135.5000\n",
      "Epoch 86: loss improved from 1920375.00000 to 1917819.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1917819.0000\n",
      "Epoch 87/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1916905.2500\n",
      "Epoch 87: loss improved from 1917819.00000 to 1917087.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1917087.5000\n",
      "Epoch 88/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1914907.8750\n",
      "Epoch 88: loss improved from 1917087.50000 to 1914451.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1914451.6250\n",
      "Epoch 89/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1914248.8750\n",
      "Epoch 89: loss did not improve from 1914451.62500\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1915215.6250\n",
      "Epoch 90/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1909997.5000\n",
      "Epoch 90: loss improved from 1914451.62500 to 1912434.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1912434.1250\n",
      "Epoch 91/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1911034.7500\n",
      "Epoch 91: loss improved from 1912434.12500 to 1909486.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1909486.5000\n",
      "Epoch 92/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1907229.8750\n",
      "Epoch 92: loss improved from 1909486.50000 to 1907013.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1907013.8750\n",
      "Epoch 93/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1908672.6250\n",
      "Epoch 93: loss did not improve from 1907013.87500\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1907233.7500\n",
      "Epoch 94/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1907616.3750\n",
      "Epoch 94: loss improved from 1907013.87500 to 1906599.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1906599.8750\n",
      "Epoch 95/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1904303.5000\n",
      "Epoch 95: loss improved from 1906599.87500 to 1903584.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1903584.3750\n",
      "Epoch 96/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1903716.2500\n",
      "Epoch 96: loss improved from 1903584.37500 to 1903318.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1903318.8750\n",
      "Epoch 97/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1903069.0000\n",
      "Epoch 97: loss did not improve from 1903318.87500\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1903446.6250\n",
      "Epoch 98/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1901589.7500\n",
      "Epoch 98: loss improved from 1903318.87500 to 1901067.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1901067.5000\n",
      "Epoch 99/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1900462.8750\n",
      "Epoch 99: loss improved from 1901067.50000 to 1900369.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1900369.1250\n",
      "Epoch 100/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1896863.3750\n",
      "Epoch 100: loss improved from 1900369.12500 to 1898245.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1898245.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 07:12:33 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 07:12:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp8fwp_c1d\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp8fwp_c1d\\model\\data\\model\\assets\n",
      "2023/05/28 07:12:55 INFO mlflow.tracking.fluent: Experiment with name 'dnn3vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_119 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 1:50:19 - loss: 4168649.2500WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0067s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0067s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184/3189 [============================>.] - ETA: 0s - loss: 2731933.5000\n",
      "Epoch 1: loss improved from inf to 2732545.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2732545.0000\n",
      "Epoch 2/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2716018.2500\n",
      "Epoch 2: loss improved from 2732545.00000 to 2716386.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2716386.7500\n",
      "Epoch 3/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2698345.0000\n",
      "Epoch 3: loss improved from 2716386.75000 to 2700120.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2700120.2500\n",
      "Epoch 4/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2687057.0000\n",
      "Epoch 4: loss improved from 2700120.25000 to 2688722.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2688722.5000\n",
      "Epoch 5/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2680513.7500\n",
      "Epoch 5: loss improved from 2688722.50000 to 2678877.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2678877.7500\n",
      "Epoch 6/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2670008.5000\n",
      "Epoch 6: loss improved from 2678877.75000 to 2670238.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2670238.7500\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2662010.5000\n",
      "Epoch 7: loss improved from 2670238.75000 to 2662010.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2662010.5000\n",
      "Epoch 8/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2657186.5000\n",
      "Epoch 8: loss improved from 2662010.50000 to 2655843.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2655843.5000\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2649745.7500\n",
      "Epoch 9: loss improved from 2655843.50000 to 2649460.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2649460.0000\n",
      "Epoch 10/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2642899.5000\n",
      "Epoch 10: loss improved from 2649460.00000 to 2642775.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2642775.0000\n",
      "Epoch 11/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2637387.5000\n",
      "Epoch 11: loss improved from 2642775.00000 to 2636516.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2636516.2500\n",
      "Epoch 12/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2627497.7500\n",
      "Epoch 12: loss improved from 2636516.25000 to 2631773.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2631773.2500\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2626645.7500\n",
      "Epoch 13: loss improved from 2631773.25000 to 2626645.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2626645.7500\n",
      "Epoch 14/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2621888.7500\n",
      "Epoch 14: loss improved from 2626645.75000 to 2621669.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2621669.2500\n",
      "Epoch 15/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2618362.7500\n",
      "Epoch 15: loss improved from 2621669.25000 to 2617985.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2617985.2500\n",
      "Epoch 16/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2611742.2500\n",
      "Epoch 16: loss improved from 2617985.25000 to 2612644.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2612644.0000\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2607311.5000\n",
      "Epoch 17: loss improved from 2612644.00000 to 2607534.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2607534.0000\n",
      "Epoch 18/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2605008.2500\n",
      "Epoch 18: loss improved from 2607534.00000 to 2604568.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2604568.2500\n",
      "Epoch 19/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2598718.0000\n",
      "Epoch 19: loss improved from 2604568.25000 to 2599752.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2599752.2500\n",
      "Epoch 20/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2593756.7500\n",
      "Epoch 20: loss improved from 2599752.25000 to 2595265.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2595265.2500\n",
      "Epoch 21/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2591288.2500\n",
      "Epoch 21: loss improved from 2595265.25000 to 2591173.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2591173.0000\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2587043.0000\n",
      "Epoch 22: loss improved from 2591173.00000 to 2587172.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2587172.0000\n",
      "Epoch 23/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2583164.2500\n",
      "Epoch 23: loss improved from 2587172.00000 to 2583755.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2583755.5000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2581924.2500\n",
      "Epoch 24: loss improved from 2583755.50000 to 2580774.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2580774.5000\n",
      "Epoch 25/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2576533.2500\n",
      "Epoch 25: loss improved from 2580774.50000 to 2576879.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2576879.2500\n",
      "Epoch 26/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2573689.2500\n",
      "Epoch 26: loss improved from 2576879.25000 to 2572837.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2572837.7500\n",
      "Epoch 27/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2571396.0000\n",
      "Epoch 27: loss improved from 2572837.75000 to 2569638.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2569638.5000\n",
      "Epoch 28/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2567429.0000\n",
      "Epoch 28: loss improved from 2569638.50000 to 2567041.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2567041.7500\n",
      "Epoch 29/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2562203.5000\n",
      "Epoch 29: loss improved from 2567041.75000 to 2564331.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2564331.2500\n",
      "Epoch 30/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2561242.2500\n",
      "Epoch 30: loss improved from 2564331.25000 to 2561242.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2561242.2500\n",
      "Epoch 31/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2558278.5000\n",
      "Epoch 31: loss improved from 2561242.25000 to 2557811.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2557811.7500\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2553575.5000\n",
      "Epoch 32: loss improved from 2557811.75000 to 2553575.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2553575.5000\n",
      "Epoch 33/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2549841.5000\n",
      "Epoch 33: loss improved from 2553575.50000 to 2550257.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550257.5000\n",
      "Epoch 34/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2548382.0000\n",
      "Epoch 34: loss improved from 2550257.50000 to 2549394.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2549394.5000\n",
      "Epoch 35/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2546140.5000\n",
      "Epoch 35: loss improved from 2549394.50000 to 2545918.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545918.5000\n",
      "Epoch 36/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2543423.5000\n",
      "Epoch 36: loss improved from 2545918.50000 to 2543207.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543207.2500\n",
      "Epoch 37/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2543452.0000\n",
      "Epoch 37: loss improved from 2543207.25000 to 2541771.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2541771.0000\n",
      "Epoch 38/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2538577.5000\n",
      "Epoch 38: loss improved from 2541771.00000 to 2537626.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2537626.7500\n",
      "Epoch 39/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2535034.0000\n",
      "Epoch 39: loss improved from 2537626.75000 to 2535079.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2535079.5000\n",
      "Epoch 40/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2533638.5000\n",
      "Epoch 40: loss improved from 2535079.50000 to 2533900.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533900.2500\n",
      "Epoch 41/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2529718.0000\n",
      "Epoch 41: loss improved from 2533900.25000 to 2529837.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2529837.7500\n",
      "Epoch 42/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2526275.7500\n",
      "Epoch 42: loss improved from 2529837.75000 to 2526903.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526903.7500\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2525542.2500\n",
      "Epoch 43: loss improved from 2526903.75000 to 2525445.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525445.7500\n",
      "Epoch 44/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2523321.2500\n",
      "Epoch 44: loss improved from 2525445.75000 to 2523139.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2523139.5000\n",
      "Epoch 45/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2522038.7500\n",
      "Epoch 45: loss improved from 2523139.50000 to 2520668.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2520668.7500\n",
      "Epoch 46/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2518455.0000\n",
      "Epoch 46: loss improved from 2520668.75000 to 2518229.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2518229.5000\n",
      "Epoch 47/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2513254.2500\n",
      "Epoch 47: loss improved from 2518229.50000 to 2514857.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2514857.0000\n",
      "Epoch 48/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2512243.0000\n",
      "Epoch 48: loss improved from 2514857.00000 to 2513127.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2513127.5000\n",
      "Epoch 49/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2511902.5000\n",
      "Epoch 49: loss improved from 2513127.50000 to 2511860.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2511860.5000\n",
      "Epoch 50/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2507940.5000\n",
      "Epoch 50: loss improved from 2511860.50000 to 2507950.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2507950.2500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2506169.7500\n",
      "Epoch 51: loss improved from 2507950.25000 to 2506169.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2506169.7500\n",
      "Epoch 52/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2504809.5000\n",
      "Epoch 52: loss improved from 2506169.75000 to 2506124.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2506124.5000\n",
      "Epoch 53/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2503734.2500\n",
      "Epoch 53: loss improved from 2506124.50000 to 2503375.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2503375.2500\n",
      "Epoch 54/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2498308.5000\n",
      "Epoch 54: loss improved from 2503375.25000 to 2499684.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2499684.2500\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2498890.5000\n",
      "Epoch 55: loss improved from 2499684.25000 to 2499097.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2499097.0000\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2496719.0000\n",
      "Epoch 56: loss improved from 2499097.00000 to 2496857.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2496857.2500\n",
      "Epoch 57/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2496748.7500\n",
      "Epoch 57: loss improved from 2496857.25000 to 2494883.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2494883.5000\n",
      "Epoch 58/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2492573.2500\n",
      "Epoch 58: loss improved from 2494883.50000 to 2492658.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2492658.0000\n",
      "Epoch 59/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2490885.0000\n",
      "Epoch 59: loss improved from 2492658.00000 to 2490004.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2490004.7500\n",
      "Epoch 60/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2490742.0000\n",
      "Epoch 60: loss improved from 2490004.75000 to 2489934.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2489934.2500\n",
      "Epoch 61/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2486240.5000\n",
      "Epoch 61: loss improved from 2489934.25000 to 2487566.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2487566.2500\n",
      "Epoch 62/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2488046.2500\n",
      "Epoch 62: loss improved from 2487566.25000 to 2486133.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2486133.0000\n",
      "Epoch 63/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2483601.7500\n",
      "Epoch 63: loss improved from 2486133.00000 to 2482786.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2482786.7500\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2482904.7500\n",
      "Epoch 64: loss did not improve from 2482786.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2483090.2500\n",
      "Epoch 65/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2480779.7500\n",
      "Epoch 65: loss improved from 2482786.75000 to 2480862.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2480862.2500\n",
      "Epoch 66/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2480065.0000\n",
      "Epoch 66: loss improved from 2480862.25000 to 2478372.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2478372.7500\n",
      "Epoch 67/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2476810.0000\n",
      "Epoch 67: loss improved from 2478372.75000 to 2477183.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2477183.0000\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2475000.5000\n",
      "Epoch 68: loss improved from 2477183.00000 to 2475000.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2475000.5000\n",
      "Epoch 69/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2473948.7500\n",
      "Epoch 69: loss improved from 2475000.50000 to 2473940.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2473940.7500\n",
      "Epoch 70/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2471788.2500\n",
      "Epoch 70: loss improved from 2473940.75000 to 2471660.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2471660.7500\n",
      "Epoch 71/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2472036.2500\n",
      "Epoch 71: loss did not improve from 2471660.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2471795.0000\n",
      "Epoch 72/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2468381.2500\n",
      "Epoch 72: loss improved from 2471660.75000 to 2468231.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2468231.5000\n",
      "Epoch 73/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2466314.7500\n",
      "Epoch 73: loss improved from 2468231.50000 to 2466080.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2466080.7500\n",
      "Epoch 74/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2467199.0000\n",
      "Epoch 74: loss improved from 2466080.75000 to 2465473.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2465473.2500\n",
      "Epoch 75/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2465356.7500\n",
      "Epoch 75: loss did not improve from 2465473.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2465752.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2462502.7500\n",
      "Epoch 76: loss improved from 2465473.25000 to 2462524.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2462524.7500\n",
      "Epoch 77/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2461704.7500\n",
      "Epoch 77: loss improved from 2462524.75000 to 2460571.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2460571.5000\n",
      "Epoch 78/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2458312.5000\n",
      "Epoch 78: loss improved from 2460571.50000 to 2458312.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2458312.5000\n",
      "Epoch 79/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2459141.0000\n",
      "Epoch 79: loss improved from 2458312.50000 to 2457908.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2457908.2500\n",
      "Epoch 80/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2456885.7500\n",
      "Epoch 80: loss did not improve from 2457908.25000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2458010.7500\n",
      "Epoch 81/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2454943.2500\n",
      "Epoch 81: loss improved from 2457908.25000 to 2455025.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2455025.2500\n",
      "Epoch 82/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2452839.2500\n",
      "Epoch 82: loss improved from 2455025.25000 to 2453296.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2453296.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2452443.5000\n",
      "Epoch 83: loss improved from 2453296.75000 to 2452382.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2452382.5000\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2449365.2500\n",
      "Epoch 84: loss improved from 2452382.50000 to 2451950.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2451950.2500\n",
      "Epoch 85/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2446725.7500\n",
      "Epoch 85: loss improved from 2451950.25000 to 2448936.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2448936.0000\n",
      "Epoch 86/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2448628.7500\n",
      "Epoch 86: loss improved from 2448936.00000 to 2448142.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2448142.7500\n",
      "Epoch 87/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2446137.5000\n",
      "Epoch 87: loss improved from 2448142.75000 to 2445701.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2445701.0000\n",
      "Epoch 88/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2445743.2500\n",
      "Epoch 88: loss improved from 2445701.00000 to 2445550.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2445550.0000\n",
      "Epoch 89/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2444241.0000\n",
      "Epoch 89: loss improved from 2445550.00000 to 2444243.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2444243.0000\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2443554.7500\n",
      "Epoch 90: loss improved from 2444243.00000 to 2443306.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2443306.5000\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2440927.5000\n",
      "Epoch 91: loss improved from 2443306.50000 to 2440311.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2440311.2500\n",
      "Epoch 92/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2440294.5000\n",
      "Epoch 92: loss improved from 2440311.25000 to 2440173.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2440173.0000\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2435878.7500\n",
      "Epoch 93: loss improved from 2440173.00000 to 2437962.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2437962.2500\n",
      "Epoch 94/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2438438.2500\n",
      "Epoch 94: loss improved from 2437962.25000 to 2437408.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2437408.5000\n",
      "Epoch 95/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2438917.7500\n",
      "Epoch 95: loss improved from 2437408.50000 to 2436881.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2436881.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2434674.2500\n",
      "Epoch 96: loss improved from 2436881.50000 to 2434674.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2434674.2500\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2435218.2500\n",
      "Epoch 97: loss did not improve from 2434674.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2434682.2500\n",
      "Epoch 98/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2433973.0000\n",
      "Epoch 98: loss improved from 2434674.25000 to 2433673.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2433673.2500\n",
      "Epoch 99/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2432353.5000\n",
      "Epoch 99: loss improved from 2433673.25000 to 2432637.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2432637.2500\n",
      "Epoch 100/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2430243.5000\n",
      "Epoch 100: loss improved from 2432637.25000 to 2431365.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2431365.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 07:33:28 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 07:33:28 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn2ol_3zo\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn2ol_3zo\\model\\data\\model\\assets\n",
      "2023/05/28 07:34:06 INFO mlflow.tracking.fluent: Experiment with name 'dnn4vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_127 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   6/3189 [..............................] - ETA: 2:22 - loss: 4538164.5000 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0338s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0338s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3187/3189 [============================>.] - ETA: 0s - loss: 2596019.2500\n",
      "Epoch 1: loss improved from inf to 2595454.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2595454.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2258064.2500\n",
      "Epoch 2: loss improved from 2595454.50000 to 2257566.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2257566.0000\n",
      "Epoch 3/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2086660.7500\n",
      "Epoch 3: loss improved from 2257566.00000 to 2085539.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2085539.3750\n",
      "Epoch 4/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1955290.5000\n",
      "Epoch 4: loss improved from 2085539.37500 to 1956205.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1956205.5000\n",
      "Epoch 5/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1862521.3750\n",
      "Epoch 5: loss improved from 1956205.50000 to 1862484.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1862484.2500\n",
      "Epoch 6/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1782995.8750\n",
      "Epoch 6: loss improved from 1862484.25000 to 1783050.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1783050.5000\n",
      "Epoch 7/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1723534.6250\n",
      "Epoch 7: loss improved from 1783050.50000 to 1722423.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1722423.3750\n",
      "Epoch 8/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1666057.3750\n",
      "Epoch 8: loss improved from 1722423.37500 to 1666180.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1666180.8750\n",
      "Epoch 9/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1621481.0000\n",
      "Epoch 9: loss improved from 1666180.87500 to 1621456.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1621456.3750\n",
      "Epoch 10/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1583270.2500\n",
      "Epoch 10: loss improved from 1621456.37500 to 1582380.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1582380.8750\n",
      "Epoch 11/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1547770.2500\n",
      "Epoch 11: loss improved from 1582380.87500 to 1547616.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1547616.1250\n",
      "Epoch 12/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1517979.3750\n",
      "Epoch 12: loss improved from 1547616.12500 to 1517921.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1517921.6250\n",
      "Epoch 13/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1490277.2500\n",
      "Epoch 13: loss improved from 1517921.62500 to 1490155.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1490155.0000\n",
      "Epoch 14/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1466089.5000\n",
      "Epoch 14: loss improved from 1490155.00000 to 1466244.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1466244.2500\n",
      "Epoch 15/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1442282.2500\n",
      "Epoch 15: loss improved from 1466244.25000 to 1442185.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1442185.3750\n",
      "Epoch 16/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1424607.2500\n",
      "Epoch 16: loss improved from 1442185.37500 to 1424366.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1424366.2500\n",
      "Epoch 17/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1406978.5000\n",
      "Epoch 17: loss improved from 1424366.25000 to 1407307.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1407307.1250\n",
      "Epoch 18/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1390248.8750\n",
      "Epoch 18: loss improved from 1407307.12500 to 1390494.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1390494.5000\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1376292.6250\n",
      "Epoch 19: loss improved from 1390494.50000 to 1376347.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1376347.2500\n",
      "Epoch 20/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1361699.8750\n",
      "Epoch 20: loss improved from 1376347.25000 to 1361866.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 39s 12ms/step - loss: 1361866.7500\n",
      "Epoch 21/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1346481.0000\n",
      "Epoch 21: loss improved from 1361866.75000 to 1346292.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1346292.8750\n",
      "Epoch 22/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1335795.5000\n",
      "Epoch 22: loss improved from 1346292.87500 to 1335928.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1335928.2500\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1326276.0000\n",
      "Epoch 23: loss improved from 1335928.25000 to 1325971.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1325971.5000\n",
      "Epoch 24/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1314146.2500\n",
      "Epoch 24: loss improved from 1325971.50000 to 1314289.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1314289.6250\n",
      "Epoch 25/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1301933.0000\n",
      "Epoch 25: loss improved from 1314289.62500 to 1302079.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1302079.2500\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1294151.2500\n",
      "Epoch 26: loss improved from 1302079.25000 to 1294151.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1294151.2500\n",
      "Epoch 27/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1284923.3750\n",
      "Epoch 27: loss improved from 1294151.25000 to 1284807.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1284807.6250\n",
      "Epoch 28/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1276406.1250\n",
      "Epoch 28: loss improved from 1284807.62500 to 1277011.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1277011.8750\n",
      "Epoch 29/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1266270.3750\n",
      "Epoch 29: loss improved from 1277011.87500 to 1266266.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1266266.0000\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1260004.6250\n",
      "Epoch 30: loss improved from 1266266.00000 to 1259951.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1259951.5000\n",
      "Epoch 31/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1255147.0000\n",
      "Epoch 31: loss improved from 1259951.50000 to 1255679.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1255679.6250\n",
      "Epoch 32/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1244789.0000\n",
      "Epoch 32: loss improved from 1255679.62500 to 1245009.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1245009.5000\n",
      "Epoch 33/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1240821.1250\n",
      "Epoch 33: loss improved from 1245009.50000 to 1240849.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1240849.0000\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1233538.0000\n",
      "Epoch 34: loss improved from 1240849.00000 to 1233538.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1233538.0000\n",
      "Epoch 35/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1227213.7500\n",
      "Epoch 35: loss improved from 1233538.00000 to 1226486.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1226486.7500\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1217893.3750\n",
      "Epoch 36: loss improved from 1226486.75000 to 1217981.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1217981.6250\n",
      "Epoch 37/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1212635.0000\n",
      "Epoch 37: loss improved from 1217981.62500 to 1212713.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1212713.2500\n",
      "Epoch 38/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1208750.3750\n",
      "Epoch 38: loss improved from 1212713.25000 to 1208890.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1208890.5000\n",
      "Epoch 39/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1203658.8750\n",
      "Epoch 39: loss improved from 1208890.50000 to 1203878.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1203878.0000\n",
      "Epoch 40/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1199557.3750\n",
      "Epoch 40: loss improved from 1203878.00000 to 1199720.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1199720.2500\n",
      "Epoch 41/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1194425.5000\n",
      "Epoch 41: loss improved from 1199720.25000 to 1194696.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1194696.3750\n",
      "Epoch 42/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1189404.3750\n",
      "Epoch 42: loss improved from 1194696.37500 to 1189153.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1189153.3750\n",
      "Epoch 43/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1185479.5000\n",
      "Epoch 43: loss improved from 1189153.37500 to 1184919.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1184919.7500\n",
      "Epoch 44/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1181446.6250\n",
      "Epoch 44: loss improved from 1184919.75000 to 1181339.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1181339.5000\n",
      "Epoch 45/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1175542.6250\n",
      "Epoch 45: loss improved from 1181339.50000 to 1175656.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1175656.0000\n",
      "Epoch 46/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1170546.3750\n",
      "Epoch 46: loss improved from 1175656.00000 to 1170643.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1170643.0000\n",
      "Epoch 47/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1169238.3750\n",
      "Epoch 47: loss improved from 1170643.00000 to 1169189.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1169189.7500\n",
      "Epoch 48/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1163786.8750\n",
      "Epoch 48: loss improved from 1169189.75000 to 1164751.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1164751.7500\n",
      "Epoch 49/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1160697.7500\n",
      "Epoch 49: loss improved from 1164751.75000 to 1161326.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1161326.6250\n",
      "Epoch 50/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1157621.1250\n",
      "Epoch 50: loss improved from 1161326.62500 to 1157435.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1157435.0000\n",
      "Epoch 51/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1154483.7500\n",
      "Epoch 51: loss improved from 1157435.00000 to 1154337.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1154337.5000\n",
      "Epoch 52/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1150819.1250\n",
      "Epoch 52: loss improved from 1154337.50000 to 1150606.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1150606.3750\n",
      "Epoch 53/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1146786.1250\n",
      "Epoch 53: loss improved from 1150606.37500 to 1146698.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1146698.0000\n",
      "Epoch 54/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1145573.5000\n",
      "Epoch 54: loss improved from 1146698.00000 to 1145508.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1145508.2500\n",
      "Epoch 55/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1142185.3750\n",
      "Epoch 55: loss improved from 1145508.25000 to 1141919.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1141919.2500\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1137603.7500\n",
      "Epoch 56: loss improved from 1141919.25000 to 1137425.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1137425.3750\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1135018.7500\n",
      "Epoch 57: loss improved from 1137425.37500 to 1134567.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1134567.0000\n",
      "Epoch 58/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1132322.8750\n",
      "Epoch 58: loss improved from 1134567.00000 to 1132379.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1132379.6250\n",
      "Epoch 59/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1129070.0000\n",
      "Epoch 59: loss improved from 1132379.62500 to 1129107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1129107.7500\n",
      "Epoch 60/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1126020.8750\n",
      "Epoch 60: loss improved from 1129107.75000 to 1126094.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1126094.8750\n",
      "Epoch 61/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1125416.7500\n",
      "Epoch 61: loss improved from 1126094.87500 to 1125352.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1125352.7500\n",
      "Epoch 62/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1121625.3750\n",
      "Epoch 62: loss improved from 1125352.75000 to 1121440.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1121440.2500\n",
      "Epoch 63/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1117533.5000\n",
      "Epoch 63: loss improved from 1121440.25000 to 1117380.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1117380.1250\n",
      "Epoch 64/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1114890.3750\n",
      "Epoch 64: loss improved from 1117380.12500 to 1115817.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1115817.1250\n",
      "Epoch 65/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1114267.5000\n",
      "Epoch 65: loss improved from 1115817.12500 to 1114048.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1114048.6250\n",
      "Epoch 66/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1110006.0000\n",
      "Epoch 66: loss improved from 1114048.62500 to 1109889.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1109889.7500\n",
      "Epoch 67/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1110212.2500\n",
      "Epoch 67: loss improved from 1109889.75000 to 1109481.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1109481.6250\n",
      "Epoch 68/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1105592.3750\n",
      "Epoch 68: loss improved from 1109481.62500 to 1105132.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1105132.0000\n",
      "Epoch 69/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1103052.7500\n",
      "Epoch 69: loss improved from 1105132.00000 to 1103196.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1103196.5000\n",
      "Epoch 70/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1100458.5000\n",
      "Epoch 70: loss improved from 1103196.50000 to 1100205.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1100205.2500\n",
      "Epoch 71/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1099404.5000\n",
      "Epoch 71: loss improved from 1100205.25000 to 1099513.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1099513.3750\n",
      "Epoch 72/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1096610.7500\n",
      "Epoch 72: loss improved from 1099513.37500 to 1096475.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1096475.7500\n",
      "Epoch 73/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1093604.0000\n",
      "Epoch 73: loss improved from 1096475.75000 to 1093727.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1093727.0000\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1091257.0000\n",
      "Epoch 74: loss improved from 1093727.00000 to 1091219.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1091219.2500\n",
      "Epoch 75/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1089011.5000\n",
      "Epoch 75: loss improved from 1091219.25000 to 1089671.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1089671.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1087367.7500\n",
      "Epoch 76: loss improved from 1089671.25000 to 1087714.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1087714.1250\n",
      "Epoch 77/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1086610.6250\n",
      "Epoch 77: loss improved from 1087714.12500 to 1086150.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1086150.8750\n",
      "Epoch 78/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1081321.8750\n",
      "Epoch 78: loss improved from 1086150.87500 to 1081203.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1081203.8750\n",
      "Epoch 79/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1080443.0000\n",
      "Epoch 79: loss improved from 1081203.87500 to 1080369.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1080369.5000\n",
      "Epoch 80/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1078001.0000\n",
      "Epoch 80: loss improved from 1080369.50000 to 1079598.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1079598.1250\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1077765.5000\n",
      "Epoch 81: loss improved from 1079598.12500 to 1077765.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1077765.5000\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1075415.0000\n",
      "Epoch 82: loss improved from 1077765.50000 to 1075450.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1075450.1250\n",
      "Epoch 83/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1073197.7500\n",
      "Epoch 83: loss improved from 1075450.12500 to 1073759.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1073759.6250\n",
      "Epoch 84/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1070943.2500\n",
      "Epoch 84: loss improved from 1073759.62500 to 1071547.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1071547.7500\n",
      "Epoch 85/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1069609.2500\n",
      "Epoch 85: loss improved from 1071547.75000 to 1069479.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1069479.7500\n",
      "Epoch 86/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1067331.8750\n",
      "Epoch 86: loss improved from 1069479.75000 to 1067390.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1067390.8750\n",
      "Epoch 87/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1066769.3750\n",
      "Epoch 87: loss improved from 1067390.87500 to 1066847.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1066847.2500\n",
      "Epoch 88/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1066327.5000\n",
      "Epoch 88: loss improved from 1066847.25000 to 1066168.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1066168.2500\n",
      "Epoch 89/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1063510.2500\n",
      "Epoch 89: loss improved from 1066168.25000 to 1063521.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1063521.7500\n",
      "Epoch 90/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1062636.0000\n",
      "Epoch 90: loss improved from 1063521.75000 to 1062495.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1062495.5000\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1059640.8750\n",
      "Epoch 91: loss improved from 1062495.50000 to 1059236.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1059236.1250\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1057793.6250\n",
      "Epoch 92: loss improved from 1059236.12500 to 1057693.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1057693.2500\n",
      "Epoch 93/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1056705.3750\n",
      "Epoch 93: loss improved from 1057693.25000 to 1056628.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1056628.7500\n",
      "Epoch 94/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1054557.0000\n",
      "Epoch 94: loss improved from 1056628.75000 to 1054474.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1054474.8750\n",
      "Epoch 95/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1054123.0000\n",
      "Epoch 95: loss improved from 1054474.87500 to 1053930.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1053930.6250\n",
      "Epoch 96/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1052081.6250\n",
      "Epoch 96: loss improved from 1053930.62500 to 1052010.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1052010.1250\n",
      "Epoch 97/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1052512.7500\n",
      "Epoch 97: loss did not improve from 1052010.12500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1052448.7500\n",
      "Epoch 98/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1047994.3750\n",
      "Epoch 98: loss improved from 1052010.12500 to 1048563.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1048563.0000\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1048900.0000\n",
      "Epoch 99: loss did not improve from 1048563.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1049010.6250\n",
      "Epoch 100/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1046036.0625\n",
      "Epoch 100: loss improved from 1048563.00000 to 1046479.93750, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1046479.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 07:54:45 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 07:54:45 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpk7933pww\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpk7933pww\\model\\data\\model\\assets\n",
      "2023/05/28 07:55:38 INFO mlflow.tracking.fluent: Experiment with name 'dnn4vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_131 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 2:21 - loss: 5275207.0000   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0694s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0694s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186/3189 [============================>.] - ETA: 0s - loss: 2744580.7500\n",
      "Epoch 1: loss improved from inf to 2744546.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 5ms/step - loss: 2744546.5000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2730445.5000\n",
      "Epoch 2: loss improved from 2744546.50000 to 2729755.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2729755.2500\n",
      "Epoch 3/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2714177.2500\n",
      "Epoch 3: loss improved from 2729755.25000 to 2718289.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2718289.7500\n",
      "Epoch 4/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2705589.2500\n",
      "Epoch 4: loss improved from 2718289.75000 to 2705956.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2705956.7500\n",
      "Epoch 5/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2694961.0000\n",
      "Epoch 5: loss improved from 2705956.75000 to 2694166.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2694166.2500\n",
      "Epoch 6/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2685449.2500\n",
      "Epoch 6: loss improved from 2694166.25000 to 2683515.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2683515.2500\n",
      "Epoch 7/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2672900.2500\n",
      "Epoch 7: loss improved from 2683515.25000 to 2673945.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2673945.0000\n",
      "Epoch 8/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2666520.0000\n",
      "Epoch 8: loss improved from 2673945.00000 to 2665490.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2665490.7500\n",
      "Epoch 9/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2656355.2500\n",
      "Epoch 9: loss improved from 2665490.75000 to 2657501.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2657501.2500\n",
      "Epoch 10/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2651514.0000\n",
      "Epoch 10: loss improved from 2657501.25000 to 2650220.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2650220.2500\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2644412.7500\n",
      "Epoch 11: loss improved from 2650220.25000 to 2643899.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2643899.7500\n",
      "Epoch 12/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2636702.7500\n",
      "Epoch 12: loss improved from 2643899.75000 to 2636612.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2636612.5000\n",
      "Epoch 13/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2632378.0000\n",
      "Epoch 13: loss improved from 2636612.50000 to 2632930.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2632930.0000\n",
      "Epoch 14/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2626944.2500\n",
      "Epoch 14: loss improved from 2632930.00000 to 2628328.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2628328.7500\n",
      "Epoch 15/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2622293.5000\n",
      "Epoch 15: loss improved from 2628328.75000 to 2621627.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2621627.5000\n",
      "Epoch 16/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2617423.0000\n",
      "Epoch 16: loss improved from 2621627.50000 to 2617848.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2617848.0000\n",
      "Epoch 17/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2614248.5000\n",
      "Epoch 17: loss improved from 2617848.00000 to 2613207.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2613207.5000\n",
      "Epoch 18/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2608608.5000\n",
      "Epoch 18: loss improved from 2613207.50000 to 2608798.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2608798.5000\n",
      "Epoch 19/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2603230.7500\n",
      "Epoch 19: loss improved from 2608798.50000 to 2603025.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2603025.5000\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2601806.7500\n",
      "Epoch 20: loss improved from 2603025.50000 to 2601503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2601503.0000\n",
      "Epoch 21/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2595322.7500\n",
      "Epoch 21: loss improved from 2601503.00000 to 2594941.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2594941.2500\n",
      "Epoch 22/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2592789.2500\n",
      "Epoch 22: loss improved from 2594941.25000 to 2592672.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2592672.0000\n",
      "Epoch 23/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2588383.2500\n",
      "Epoch 23: loss improved from 2592672.00000 to 2587828.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2587828.7500\n",
      "Epoch 24/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2584296.7500\n",
      "Epoch 24: loss improved from 2587828.75000 to 2584871.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2584871.5000\n",
      "Epoch 25/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2581976.7500\n",
      "Epoch 25: loss improved from 2584871.50000 to 2581531.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2581531.2500\n",
      "Epoch 26/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2579262.5000\n",
      "Epoch 26: loss improved from 2581531.25000 to 2578653.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2578653.5000\n",
      "Epoch 27/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2574937.7500\n",
      "Epoch 27: loss improved from 2578653.50000 to 2576337.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2576337.2500\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2571284.0000\n",
      "Epoch 28: loss improved from 2576337.25000 to 2571284.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2571284.0000\n",
      "Epoch 29/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2571038.5000\n",
      "Epoch 29: loss improved from 2571284.00000 to 2570229.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2570229.7500\n",
      "Epoch 30/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2568489.5000\n",
      "Epoch 30: loss improved from 2570229.75000 to 2566478.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2566478.7500\n",
      "Epoch 31/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2564926.5000\n",
      "Epoch 31: loss improved from 2566478.75000 to 2565971.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2565971.7500\n",
      "Epoch 32/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2564982.5000\n",
      "Epoch 32: loss improved from 2565971.75000 to 2565118.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2565118.2500\n",
      "Epoch 33/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2563339.7500\n",
      "Epoch 33: loss improved from 2565118.25000 to 2562294.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2562294.2500\n",
      "Epoch 34/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2558177.5000\n",
      "Epoch 34: loss improved from 2562294.25000 to 2557573.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2557573.7500\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2557381.0000\n",
      "Epoch 35: loss improved from 2557573.75000 to 2557521.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2557521.5000\n",
      "Epoch 36/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2555317.0000\n",
      "Epoch 36: loss improved from 2557521.50000 to 2554856.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2554856.2500\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2553423.0000\n",
      "Epoch 37: loss improved from 2554856.25000 to 2553408.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2553408.7500\n",
      "Epoch 38/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2552794.0000\n",
      "Epoch 38: loss improved from 2553408.75000 to 2552164.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2552164.5000\n",
      "Epoch 39/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2548950.0000\n",
      "Epoch 39: loss improved from 2552164.50000 to 2549294.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2549294.5000\n",
      "Epoch 40/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2550370.2500\n",
      "Epoch 40: loss did not improve from 2549294.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550890.7500\n",
      "Epoch 41/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2548474.7500\n",
      "Epoch 41: loss improved from 2549294.50000 to 2549032.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2549032.0000\n",
      "Epoch 42/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2545677.0000\n",
      "Epoch 42: loss improved from 2549032.00000 to 2546606.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2546606.0000\n",
      "Epoch 43/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2548640.0000\n",
      "Epoch 43: loss did not improve from 2546606.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2547613.7500\n",
      "Epoch 44/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2546245.5000\n",
      "Epoch 44: loss improved from 2546606.00000 to 2545800.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545800.7500\n",
      "Epoch 45/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2542167.0000\n",
      "Epoch 45: loss improved from 2545800.75000 to 2540884.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540884.7500\n",
      "Epoch 46/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2539308.0000\n",
      "Epoch 46: loss did not improve from 2540884.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542129.0000\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2543184.2500\n",
      "Epoch 47: loss did not improve from 2540884.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543038.7500\n",
      "Epoch 48/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2541575.2500\n",
      "Epoch 48: loss improved from 2540884.75000 to 2540370.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2540370.7500\n",
      "Epoch 49/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2540222.2500\n",
      "Epoch 49: loss improved from 2540370.75000 to 2539880.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539880.5000\n",
      "Epoch 50/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2536372.0000\n",
      "Epoch 50: loss improved from 2539880.50000 to 2537431.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537431.7500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2537127.0000\n",
      "Epoch 51: loss improved from 2537431.75000 to 2537127.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537127.0000\n",
      "Epoch 52/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2537358.2500\n",
      "Epoch 52: loss improved from 2537127.00000 to 2536961.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2536961.0000\n",
      "Epoch 53/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2538193.7500\n",
      "Epoch 53: loss did not improve from 2536961.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537806.0000\n",
      "Epoch 54/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2534101.2500\n",
      "Epoch 54: loss improved from 2536961.00000 to 2534601.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534601.2500\n",
      "Epoch 55/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2534482.2500\n",
      "Epoch 55: loss improved from 2534601.25000 to 2534469.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2534469.5000\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2536951.7500\n",
      "Epoch 56: loss did not improve from 2534469.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535633.0000\n",
      "Epoch 57/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2533193.5000\n",
      "Epoch 57: loss improved from 2534469.50000 to 2532725.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532725.5000\n",
      "Epoch 58/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2531907.2500\n",
      "Epoch 58: loss improved from 2532725.50000 to 2531753.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531753.7500\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2533147.7500\n",
      "Epoch 59: loss did not improve from 2531753.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533132.2500\n",
      "Epoch 60/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2530504.5000\n",
      "Epoch 60: loss improved from 2531753.75000 to 2531111.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531111.5000\n",
      "Epoch 61/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2531542.2500\n",
      "Epoch 61: loss did not improve from 2531111.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531542.2500\n",
      "Epoch 62/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2527048.7500\n",
      "Epoch 62: loss improved from 2531111.50000 to 2528086.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2528086.2500\n",
      "Epoch 63/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2528758.7500\n",
      "Epoch 63: loss did not improve from 2528086.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2528608.0000\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2528695.5000\n",
      "Epoch 64: loss did not improve from 2528086.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530309.2500\n",
      "Epoch 65/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2529019.5000\n",
      "Epoch 65: loss did not improve from 2528086.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2528166.0000\n",
      "Epoch 66/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2526811.5000\n",
      "Epoch 66: loss improved from 2528086.25000 to 2526671.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2526671.7500\n",
      "Epoch 67/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2527240.5000\n",
      "Epoch 67: loss did not improve from 2526671.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2527244.0000\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2525794.7500\n",
      "Epoch 68: loss improved from 2526671.75000 to 2525727.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525727.5000\n",
      "Epoch 69/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2526621.2500\n",
      "Epoch 69: loss did not improve from 2525727.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526220.0000\n",
      "Epoch 70/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2522168.5000\n",
      "Epoch 70: loss improved from 2525727.50000 to 2523203.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2523203.2500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2526377.5000\n",
      "Epoch 71: loss did not improve from 2523203.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526377.5000\n",
      "Epoch 72/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2525698.0000\n",
      "Epoch 72: loss did not improve from 2523203.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525745.5000\n",
      "Epoch 73/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2523711.7500\n",
      "Epoch 73: loss did not improve from 2523203.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2523339.0000\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2523054.0000\n",
      "Epoch 74: loss improved from 2523203.25000 to 2523190.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2523190.7500\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2520425.0000\n",
      "Epoch 75: loss improved from 2523190.75000 to 2521263.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2521263.2500\n",
      "Epoch 76/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2521457.5000\n",
      "Epoch 76: loss improved from 2521263.25000 to 2521122.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2521122.7500\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2521818.2500\n",
      "Epoch 77: loss did not improve from 2521122.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2521580.5000\n",
      "Epoch 78/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2519989.0000\n",
      "Epoch 78: loss improved from 2521122.75000 to 2520374.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 2520374.7500\n",
      "Epoch 79/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2518512.7500\n",
      "Epoch 79: loss improved from 2520374.75000 to 2519039.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2519039.5000\n",
      "Epoch 80/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2516507.5000\n",
      "Epoch 80: loss improved from 2519039.50000 to 2517639.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 2517639.2500\n",
      "Epoch 81/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2518443.5000\n",
      "Epoch 81: loss did not improve from 2517639.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2518568.0000\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2518111.0000\n",
      "Epoch 82: loss did not improve from 2517639.25000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2517969.5000\n",
      "Epoch 83/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2515816.2500\n",
      "Epoch 83: loss improved from 2517639.25000 to 2517192.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2517192.7500\n",
      "Epoch 84/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2516627.0000\n",
      "Epoch 84: loss did not improve from 2517192.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2518094.0000\n",
      "Epoch 85/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2514200.7500\n",
      "Epoch 85: loss improved from 2517192.75000 to 2516138.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2516138.5000\n",
      "Epoch 86/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2515188.5000\n",
      "Epoch 86: loss improved from 2516138.50000 to 2516014.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2516014.0000\n",
      "Epoch 87/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2515345.2500\n",
      "Epoch 87: loss improved from 2516014.00000 to 2514453.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2514453.5000\n",
      "Epoch 88/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2514922.2500\n",
      "Epoch 88: loss did not improve from 2514453.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514922.2500\n",
      "Epoch 89/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2514896.0000\n",
      "Epoch 89: loss did not improve from 2514453.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514679.5000\n",
      "Epoch 90/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2514171.2500\n",
      "Epoch 90: loss did not improve from 2514453.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514773.7500\n",
      "Epoch 91/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2513232.2500\n",
      "Epoch 91: loss did not improve from 2514453.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514657.2500\n",
      "Epoch 92/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2513786.0000\n",
      "Epoch 92: loss improved from 2514453.50000 to 2513807.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2513807.2500\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2511517.5000\n",
      "Epoch 93: loss improved from 2513807.25000 to 2511517.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2511517.5000\n",
      "Epoch 94/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2513577.0000\n",
      "Epoch 94: loss did not improve from 2511517.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2512581.0000\n",
      "Epoch 95/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2513918.0000\n",
      "Epoch 95: loss did not improve from 2511517.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2513652.0000\n",
      "Epoch 96/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2510747.2500\n",
      "Epoch 96: loss improved from 2511517.50000 to 2510609.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2510609.7500\n",
      "Epoch 97/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2512847.5000\n",
      "Epoch 97: loss did not improve from 2510609.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2511791.2500\n",
      "Epoch 98/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2511678.0000\n",
      "Epoch 98: loss improved from 2510609.75000 to 2509477.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509477.7500\n",
      "Epoch 99/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2508094.5000\n",
      "Epoch 99: loss did not improve from 2509477.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509635.5000\n",
      "Epoch 100/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2507879.0000\n",
      "Epoch 100: loss improved from 2509477.75000 to 2508397.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2508397.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 08:16:36 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 08:16:36 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpgdmh318z\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpgdmh318z\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_135 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 42s - loss: 1636397.5000    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0197s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0197s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3185/3189 [============================>.] - ETA: 0s - loss: 2684280.2500\n",
      "Epoch 1: loss improved from inf to 2684348.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 25s 6ms/step - loss: 2684348.2500\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2589847.2500\n",
      "Epoch 2: loss improved from 2684348.25000 to 2589847.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2589847.2500\n",
      "Epoch 3/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2531411.0000\n",
      "Epoch 3: loss improved from 2589847.25000 to 2531357.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2531357.2500\n",
      "Epoch 4/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2489526.7500\n",
      "Epoch 4: loss improved from 2531357.25000 to 2488829.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2488829.0000\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2452365.7500\n",
      "Epoch 5: loss improved from 2488829.00000 to 2452453.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2452453.5000\n",
      "Epoch 6/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2421078.0000\n",
      "Epoch 6: loss improved from 2452453.50000 to 2420373.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2420373.5000\n",
      "Epoch 7/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2391913.7500\n",
      "Epoch 7: loss improved from 2420373.50000 to 2391861.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2391861.7500\n",
      "Epoch 8/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2366266.5000\n",
      "Epoch 8: loss improved from 2391861.75000 to 2366065.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2366065.7500\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2345667.2500\n",
      "Epoch 9: loss improved from 2366065.75000 to 2344185.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2344185.7500\n",
      "Epoch 10/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2321719.5000\n",
      "Epoch 10: loss improved from 2344185.75000 to 2321942.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2321942.7500\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2302839.2500\n",
      "Epoch 11: loss improved from 2321942.75000 to 2302839.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2302839.2500\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2283568.2500\n",
      "Epoch 12: loss improved from 2302839.25000 to 2283810.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2283810.5000\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2269544.7500\n",
      "Epoch 13: loss improved from 2283810.50000 to 2269041.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2269041.5000\n",
      "Epoch 14/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2253141.5000\n",
      "Epoch 14: loss improved from 2269041.50000 to 2252151.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2252151.2500\n",
      "Epoch 15/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2235365.5000\n",
      "Epoch 15: loss improved from 2252151.25000 to 2237167.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2237167.2500\n",
      "Epoch 16/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2224045.0000\n",
      "Epoch 16: loss improved from 2237167.25000 to 2222453.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2222453.7500\n",
      "Epoch 17/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2208666.2500\n",
      "Epoch 17: loss improved from 2222453.75000 to 2208269.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2208269.5000\n",
      "Epoch 18/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2194260.7500\n",
      "Epoch 18: loss improved from 2208269.50000 to 2195171.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2195171.5000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2183049.2500\n",
      "Epoch 19: loss improved from 2195171.50000 to 2182995.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2182995.0000\n",
      "Epoch 20/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2171696.2500\n",
      "Epoch 20: loss improved from 2182995.00000 to 2170969.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2170969.7500\n",
      "Epoch 21/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2160245.2500\n",
      "Epoch 21: loss improved from 2170969.75000 to 2159648.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2159648.5000\n",
      "Epoch 22/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2150410.5000\n",
      "Epoch 22: loss improved from 2159648.50000 to 2150248.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2150248.5000\n",
      "Epoch 23/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2141194.7500\n",
      "Epoch 23: loss improved from 2150248.50000 to 2140486.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2140486.0000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2132215.2500\n",
      "Epoch 24: loss improved from 2140486.00000 to 2131364.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2131364.7500\n",
      "Epoch 25/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2124442.7500\n",
      "Epoch 25: loss improved from 2131364.75000 to 2122734.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2122734.2500\n",
      "Epoch 26/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2113597.2500\n",
      "Epoch 26: loss improved from 2122734.25000 to 2114659.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2114659.2500\n",
      "Epoch 27/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2105979.5000\n",
      "Epoch 27: loss improved from 2114659.25000 to 2105846.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2105846.5000\n",
      "Epoch 28/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2100748.5000\n",
      "Epoch 28: loss improved from 2105846.50000 to 2100746.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2100746.7500\n",
      "Epoch 29/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2092048.5000\n",
      "Epoch 29: loss improved from 2100746.75000 to 2091640.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2091640.2500\n",
      "Epoch 30/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2086637.0000\n",
      "Epoch 30: loss improved from 2091640.25000 to 2086468.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2086468.8750\n",
      "Epoch 31/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2079549.8750\n",
      "Epoch 31: loss improved from 2086468.87500 to 2079537.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2079537.3750\n",
      "Epoch 32/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2070146.7500\n",
      "Epoch 32: loss improved from 2079537.37500 to 2072630.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2072630.1250\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2066737.3750\n",
      "Epoch 33: loss improved from 2072630.12500 to 2066737.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2066737.3750\n",
      "Epoch 34/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2063707.0000\n",
      "Epoch 34: loss improved from 2066737.37500 to 2062521.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2062521.7500\n",
      "Epoch 35/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2056194.1250\n",
      "Epoch 35: loss improved from 2062521.75000 to 2056031.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2056031.0000\n",
      "Epoch 36/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2050772.1250\n",
      "Epoch 36: loss improved from 2056031.00000 to 2051571.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2051571.3750\n",
      "Epoch 37/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2044926.0000\n",
      "Epoch 37: loss improved from 2051571.37500 to 2045264.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2045264.6250\n",
      "Epoch 38/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2042667.1250\n",
      "Epoch 38: loss improved from 2045264.62500 to 2042679.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2042679.7500\n",
      "Epoch 39/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2036849.2500\n",
      "Epoch 39: loss improved from 2042679.75000 to 2036717.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2036717.5000\n",
      "Epoch 40/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2032348.0000\n",
      "Epoch 40: loss improved from 2036717.50000 to 2032336.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2032336.6250\n",
      "Epoch 41/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2029118.6250\n",
      "Epoch 41: loss improved from 2032336.62500 to 2028334.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2028334.3750\n",
      "Epoch 42/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2025081.3750\n",
      "Epoch 42: loss improved from 2028334.37500 to 2024806.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2024806.3750\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2020738.1250\n",
      "Epoch 43: loss improved from 2024806.37500 to 2020738.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2020738.1250\n",
      "Epoch 44/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2015454.8750\n",
      "Epoch 44: loss improved from 2020738.12500 to 2017321.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2017321.8750\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2014573.0000\n",
      "Epoch 45: loss improved from 2017321.87500 to 2014251.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2014251.8750\n",
      "Epoch 46/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2011940.5000\n",
      "Epoch 46: loss improved from 2014251.87500 to 2009734.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2009734.2500\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2007072.1250\n",
      "Epoch 47: loss improved from 2009734.25000 to 2007143.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2007143.3750\n",
      "Epoch 48/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2005846.3750\n",
      "Epoch 48: loss improved from 2007143.37500 to 2005349.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2005349.5000\n",
      "Epoch 49/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2002191.2500\n",
      "Epoch 49: loss improved from 2005349.50000 to 2000886.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2000886.0000\n",
      "Epoch 50/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1998599.0000\n",
      "Epoch 50: loss improved from 2000886.00000 to 1998835.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1998835.1250\n",
      "Epoch 51/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1994409.8750\n",
      "Epoch 51: loss improved from 1998835.12500 to 1993934.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1993934.8750\n",
      "Epoch 52/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1991472.7500\n",
      "Epoch 52: loss improved from 1993934.87500 to 1991763.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1991763.5000\n",
      "Epoch 53/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1988946.2500\n",
      "Epoch 53: loss improved from 1991763.50000 to 1987198.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1987198.1250\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1984530.1250\n",
      "Epoch 54: loss improved from 1987198.12500 to 1985022.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1985022.0000\n",
      "Epoch 55/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1983484.3750\n",
      "Epoch 55: loss improved from 1985022.00000 to 1983171.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1983171.6250\n",
      "Epoch 56/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1979255.3750\n",
      "Epoch 56: loss improved from 1983171.62500 to 1980326.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1980326.7500\n",
      "Epoch 57/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1978972.3750\n",
      "Epoch 57: loss improved from 1980326.75000 to 1979107.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1979107.1250\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1974345.2500\n",
      "Epoch 58: loss improved from 1979107.12500 to 1974345.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1974345.2500\n",
      "Epoch 59/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1971307.0000\n",
      "Epoch 59: loss improved from 1974345.25000 to 1972346.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1972346.1250\n",
      "Epoch 60/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1970502.1250\n",
      "Epoch 60: loss improved from 1972346.12500 to 1970004.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1970004.0000\n",
      "Epoch 61/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1967989.5000\n",
      "Epoch 61: loss improved from 1970004.00000 to 1967730.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1967730.0000\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1964638.1250\n",
      "Epoch 62: loss improved from 1967730.00000 to 1964638.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1964638.1250\n",
      "Epoch 63/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1962006.8750\n",
      "Epoch 63: loss improved from 1964638.12500 to 1963222.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1963222.6250\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1960613.5000\n",
      "Epoch 64: loss improved from 1963222.62500 to 1960445.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1960445.6250\n",
      "Epoch 65/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1959279.5000\n",
      "Epoch 65: loss improved from 1960445.62500 to 1959141.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1959141.1250\n",
      "Epoch 66/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1957508.7500\n",
      "Epoch 66: loss improved from 1959141.12500 to 1957297.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1957297.3750\n",
      "Epoch 67/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1956047.5000\n",
      "Epoch 67: loss improved from 1957297.37500 to 1955656.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1955656.5000\n",
      "Epoch 68/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1951119.3750\n",
      "Epoch 68: loss improved from 1955656.50000 to 1951615.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1951615.2500\n",
      "Epoch 69/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1950558.1250\n",
      "Epoch 69: loss improved from 1951615.25000 to 1950260.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1950260.8750\n",
      "Epoch 70/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1947469.7500\n",
      "Epoch 70: loss improved from 1950260.87500 to 1948463.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1948463.0000\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1947446.8750\n",
      "Epoch 71: loss improved from 1948463.00000 to 1947398.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1947398.7500\n",
      "Epoch 72/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1946292.0000\n",
      "Epoch 72: loss improved from 1947398.75000 to 1946029.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1946029.0000\n",
      "Epoch 73/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1942930.5000\n",
      "Epoch 73: loss improved from 1946029.00000 to 1942137.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1942137.1250\n",
      "Epoch 74/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1940911.3750\n",
      "Epoch 74: loss improved from 1942137.12500 to 1940873.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1940873.0000\n",
      "Epoch 75/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1937896.5000\n",
      "Epoch 75: loss improved from 1940873.00000 to 1938611.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1938611.3750\n",
      "Epoch 76/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1937869.0000\n",
      "Epoch 76: loss improved from 1938611.37500 to 1938059.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1938059.0000\n",
      "Epoch 77/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1934864.0000\n",
      "Epoch 77: loss improved from 1938059.00000 to 1935447.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1935447.2500\n",
      "Epoch 78/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1933820.3750\n",
      "Epoch 78: loss improved from 1935447.25000 to 1934664.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1934664.3750\n",
      "Epoch 79/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1932894.6250\n",
      "Epoch 79: loss improved from 1934664.37500 to 1932100.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1932100.1250\n",
      "Epoch 80/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1932422.2500\n",
      "Epoch 80: loss improved from 1932100.12500 to 1931124.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1931124.3750\n",
      "Epoch 81/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1930578.2500\n",
      "Epoch 81: loss improved from 1931124.37500 to 1929651.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1929651.2500\n",
      "Epoch 82/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1928529.0000\n",
      "Epoch 82: loss improved from 1929651.25000 to 1927682.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1927682.5000\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1926369.2500\n",
      "Epoch 83: loss improved from 1927682.50000 to 1925975.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1925975.0000\n",
      "Epoch 84/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1923975.5000\n",
      "Epoch 84: loss improved from 1925975.00000 to 1923735.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1923735.3750\n",
      "Epoch 85/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1922382.3750\n",
      "Epoch 85: loss improved from 1923735.37500 to 1923029.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1923029.0000\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1922770.2500\n",
      "Epoch 86: loss improved from 1923029.00000 to 1921945.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1921945.2500\n",
      "Epoch 87/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1918375.1250\n",
      "Epoch 87: loss improved from 1921945.25000 to 1919269.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1919269.0000\n",
      "Epoch 88/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1917874.5000\n",
      "Epoch 88: loss improved from 1919269.00000 to 1917677.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1917677.8750\n",
      "Epoch 89/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1918311.8750\n",
      "Epoch 89: loss improved from 1917677.87500 to 1916206.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1916206.5000\n",
      "Epoch 90/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1914892.0000\n",
      "Epoch 90: loss improved from 1916206.50000 to 1914526.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1914526.1250\n",
      "Epoch 91/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1913383.2500\n",
      "Epoch 91: loss improved from 1914526.12500 to 1913900.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1913900.6250\n",
      "Epoch 92/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1912368.7500\n",
      "Epoch 92: loss improved from 1913900.62500 to 1911888.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1911888.7500\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1911787.0000\n",
      "Epoch 93: loss improved from 1911888.75000 to 1911247.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1911247.5000\n",
      "Epoch 94/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1910505.8750\n",
      "Epoch 94: loss improved from 1911247.50000 to 1910054.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1910054.8750\n",
      "Epoch 95/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1908287.6250\n",
      "Epoch 95: loss improved from 1910054.87500 to 1907848.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1907848.0000\n",
      "Epoch 96/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1908438.2500\n",
      "Epoch 96: loss did not improve from 1907848.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1908245.1250\n",
      "Epoch 97/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1906379.1250\n",
      "Epoch 97: loss improved from 1907848.00000 to 1905876.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1905876.7500\n",
      "Epoch 98/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1904200.1250\n",
      "Epoch 98: loss improved from 1905876.75000 to 1904097.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1904097.8750\n",
      "Epoch 99/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1902223.5000\n",
      "Epoch 99: loss improved from 1904097.87500 to 1903222.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1903222.5000\n",
      "Epoch 100/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1902183.6250\n",
      "Epoch 100: loss improved from 1903222.50000 to 1901299.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1901299.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 08:35:44 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 08:35:44 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7r5no623\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7r5no623\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_143 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 2:40 - loss: 2898670.2500  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0232s vs `on_train_batch_end` time: 0.0334s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0232s vs `on_train_batch_end` time: 0.0334s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2733869.0000\n",
      "Epoch 1: loss improved from inf to 2733869.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 5ms/step - loss: 2733869.0000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2719079.0000\n",
      "Epoch 2: loss improved from 2733869.00000 to 2719731.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2719731.5000\n",
      "Epoch 3/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2707494.0000\n",
      "Epoch 3: loss improved from 2719731.50000 to 2706859.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2706859.5000\n",
      "Epoch 4/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2693064.5000\n",
      "Epoch 4: loss improved from 2706859.50000 to 2692324.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2692324.5000\n",
      "Epoch 5/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2678055.5000\n",
      "Epoch 5: loss improved from 2692324.50000 to 2679575.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2679575.7500\n",
      "Epoch 6/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2669756.5000\n",
      "Epoch 6: loss improved from 2679575.75000 to 2668242.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2668242.0000\n",
      "Epoch 7/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2661007.2500\n",
      "Epoch 7: loss improved from 2668242.00000 to 2659759.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2659759.2500\n",
      "Epoch 8/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2652872.5000\n",
      "Epoch 8: loss improved from 2659759.25000 to 2652450.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2652450.7500\n",
      "Epoch 9/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2644733.2500\n",
      "Epoch 9: loss improved from 2652450.75000 to 2644439.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2644439.7500\n",
      "Epoch 10/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2637453.7500\n",
      "Epoch 10: loss improved from 2644439.75000 to 2638224.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2638224.5000\n",
      "Epoch 11/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2632009.7500\n",
      "Epoch 11: loss improved from 2638224.50000 to 2631834.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2631834.0000\n",
      "Epoch 12/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2626567.7500\n",
      "Epoch 12: loss improved from 2631834.00000 to 2625640.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2625640.2500\n",
      "Epoch 13/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2621149.7500\n",
      "Epoch 13: loss improved from 2625640.25000 to 2620708.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2620708.7500\n",
      "Epoch 14/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2616285.7500\n",
      "Epoch 14: loss improved from 2620708.75000 to 2614857.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2614857.7500\n",
      "Epoch 15/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2606662.5000\n",
      "Epoch 15: loss improved from 2614857.75000 to 2610142.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2610142.5000\n",
      "Epoch 16/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2606553.5000\n",
      "Epoch 16: loss improved from 2610142.50000 to 2606874.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2606874.2500\n",
      "Epoch 17/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2602722.5000\n",
      "Epoch 17: loss improved from 2606874.25000 to 2602123.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2602123.2500\n",
      "Epoch 18/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2597547.5000\n",
      "Epoch 18: loss improved from 2602123.25000 to 2597658.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2597658.5000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2596344.5000\n",
      "Epoch 19: loss improved from 2597658.50000 to 2595439.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2595439.7500\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2589556.2500\n",
      "Epoch 20: loss improved from 2595439.75000 to 2591351.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2591351.5000\n",
      "Epoch 21/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2587522.5000\n",
      "Epoch 21: loss improved from 2591351.50000 to 2586589.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2586589.5000\n",
      "Epoch 22/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2584341.7500\n",
      "Epoch 22: loss improved from 2586589.50000 to 2583650.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2583650.5000\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2581467.0000\n",
      "Epoch 23: loss improved from 2583650.50000 to 2581584.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2581584.0000\n",
      "Epoch 24/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2577587.5000\n",
      "Epoch 24: loss improved from 2581584.00000 to 2576806.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2576806.2500\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2573724.0000\n",
      "Epoch 25: loss improved from 2576806.25000 to 2573876.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2573876.2500\n",
      "Epoch 26/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2570599.5000\n",
      "Epoch 26: loss improved from 2573876.25000 to 2570160.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2570160.5000\n",
      "Epoch 27/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2567234.5000\n",
      "Epoch 27: loss improved from 2570160.50000 to 2567051.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2567051.7500\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2563993.5000\n",
      "Epoch 28: loss improved from 2567051.75000 to 2563924.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2563924.5000\n",
      "Epoch 29/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2561099.7500\n",
      "Epoch 29: loss improved from 2563924.50000 to 2562090.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2562090.2500\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2556562.0000\n",
      "Epoch 30: loss improved from 2562090.25000 to 2557276.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2557276.0000\n",
      "Epoch 31/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2555675.5000\n",
      "Epoch 31: loss improved from 2557276.00000 to 2554999.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2554999.2500\n",
      "Epoch 32/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2550290.0000\n",
      "Epoch 32: loss improved from 2554999.25000 to 2550984.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2550984.7500\n",
      "Epoch 33/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2549676.0000\n",
      "Epoch 33: loss improved from 2550984.75000 to 2549038.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2549038.0000\n",
      "Epoch 34/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2546032.0000\n",
      "Epoch 34: loss improved from 2549038.00000 to 2546000.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2546000.5000\n",
      "Epoch 35/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2542175.2500\n",
      "Epoch 35: loss improved from 2546000.50000 to 2542434.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2542434.0000\n",
      "Epoch 36/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2541080.0000\n",
      "Epoch 36: loss improved from 2542434.00000 to 2539732.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539732.7500\n",
      "Epoch 37/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2536927.5000\n",
      "Epoch 37: loss improved from 2539732.75000 to 2536805.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2536805.2500\n",
      "Epoch 38/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2534972.5000\n",
      "Epoch 38: loss improved from 2536805.25000 to 2534270.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2534270.5000\n",
      "Epoch 39/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2532367.5000\n",
      "Epoch 39: loss improved from 2534270.50000 to 2532327.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2532327.0000\n",
      "Epoch 40/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2530657.2500\n",
      "Epoch 40: loss improved from 2532327.00000 to 2529126.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2529126.5000\n",
      "Epoch 41/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2528210.0000\n",
      "Epoch 41: loss improved from 2529126.50000 to 2527547.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2527547.7500\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2523625.0000\n",
      "Epoch 42: loss improved from 2527547.75000 to 2524568.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2524568.2500\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2521127.2500\n",
      "Epoch 43: loss improved from 2524568.25000 to 2521127.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2521127.2500\n",
      "Epoch 44/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2519904.5000\n",
      "Epoch 44: loss improved from 2521127.25000 to 2519852.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2519852.5000\n",
      "Epoch 45/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2520361.7500\n",
      "Epoch 45: loss improved from 2519852.50000 to 2518675.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2518675.5000\n",
      "Epoch 46/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2516360.7500\n",
      "Epoch 46: loss improved from 2518675.50000 to 2515383.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2515383.7500\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2512836.2500\n",
      "Epoch 47: loss improved from 2515383.75000 to 2512296.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2512296.7500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2509898.2500\n",
      "Epoch 48: loss improved from 2512296.75000 to 2510324.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2510324.0000\n",
      "Epoch 49/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2508856.5000\n",
      "Epoch 49: loss improved from 2510324.00000 to 2508694.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2508694.2500\n",
      "Epoch 50/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2507518.2500\n",
      "Epoch 50: loss improved from 2508694.25000 to 2506418.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2506418.2500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2503463.7500\n",
      "Epoch 51: loss improved from 2506418.25000 to 2503463.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2503463.7500\n",
      "Epoch 52/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2502798.7500\n",
      "Epoch 52: loss improved from 2503463.75000 to 2502580.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2502580.7500\n",
      "Epoch 53/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2499054.2500\n",
      "Epoch 53: loss improved from 2502580.75000 to 2498379.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2498379.5000\n",
      "Epoch 54/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2497048.7500\n",
      "Epoch 54: loss improved from 2498379.50000 to 2497532.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2497532.2500\n",
      "Epoch 55/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2497388.5000\n",
      "Epoch 55: loss improved from 2497532.25000 to 2496709.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2496709.5000\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2491869.0000\n",
      "Epoch 56: loss improved from 2496709.50000 to 2492360.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 5ms/step - loss: 2492360.0000\n",
      "Epoch 57/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2491984.7500\n",
      "Epoch 57: loss improved from 2492360.00000 to 2491680.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2491680.2500\n",
      "Epoch 58/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2490333.7500\n",
      "Epoch 58: loss improved from 2491680.25000 to 2490576.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2490576.5000\n",
      "Epoch 59/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2487651.0000\n",
      "Epoch 59: loss improved from 2490576.50000 to 2487382.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2487382.2500\n",
      "Epoch 60/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2486921.7500\n",
      "Epoch 60: loss improved from 2487382.25000 to 2486698.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2486698.2500\n",
      "Epoch 61/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2484504.5000\n",
      "Epoch 61: loss improved from 2486698.25000 to 2485102.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2485102.7500\n",
      "Epoch 62/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2480525.0000\n",
      "Epoch 62: loss improved from 2485102.75000 to 2481802.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2481802.7500\n",
      "Epoch 63/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2480702.5000\n",
      "Epoch 63: loss improved from 2481802.75000 to 2481182.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2481182.0000\n",
      "Epoch 64/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2477457.0000\n",
      "Epoch 64: loss improved from 2481182.00000 to 2479988.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2479988.5000\n",
      "Epoch 65/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2477838.5000\n",
      "Epoch 65: loss improved from 2479988.50000 to 2477938.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2477938.7500\n",
      "Epoch 66/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2476670.7500\n",
      "Epoch 66: loss improved from 2477938.75000 to 2476981.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2476981.2500\n",
      "Epoch 67/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2474269.2500\n",
      "Epoch 67: loss improved from 2476981.25000 to 2474566.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2474566.0000\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2472793.2500\n",
      "Epoch 68: loss improved from 2474566.00000 to 2471928.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2471928.7500\n",
      "Epoch 69/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2471380.7500\n",
      "Epoch 69: loss improved from 2471928.75000 to 2471391.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2471391.0000\n",
      "Epoch 70/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2470703.5000\n",
      "Epoch 70: loss improved from 2471391.00000 to 2469989.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2469989.2500\n",
      "Epoch 71/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2466078.2500\n",
      "Epoch 71: loss improved from 2469989.25000 to 2466873.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2466873.7500\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2466689.7500\n",
      "Epoch 72: loss improved from 2466873.75000 to 2466777.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2466777.5000\n",
      "Epoch 73/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2466787.7500\n",
      "Epoch 73: loss did not improve from 2466777.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2467305.7500\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2463539.0000\n",
      "Epoch 74: loss improved from 2466777.50000 to 2465448.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2465448.0000\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2463013.0000\n",
      "Epoch 75: loss improved from 2465448.00000 to 2463425.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2463425.5000\n",
      "Epoch 76/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2458983.7500\n",
      "Epoch 76: loss improved from 2463425.50000 to 2461788.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2461788.7500\n",
      "Epoch 77/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2460945.5000\n",
      "Epoch 77: loss improved from 2461788.75000 to 2461094.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2461094.2500\n",
      "Epoch 78/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2458444.5000\n",
      "Epoch 78: loss improved from 2461094.25000 to 2459284.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2459284.5000\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2457370.7500\n",
      "Epoch 79: loss improved from 2459284.50000 to 2457671.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2457671.5000\n",
      "Epoch 80/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2456821.2500\n",
      "Epoch 80: loss did not improve from 2457671.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2458161.0000\n",
      "Epoch 81/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2455298.7500\n",
      "Epoch 81: loss improved from 2457671.50000 to 2455236.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2455236.0000\n",
      "Epoch 82/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2454136.2500\n",
      "Epoch 82: loss improved from 2455236.00000 to 2454041.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2454041.5000\n",
      "Epoch 83/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2454036.0000\n",
      "Epoch 83: loss improved from 2454041.50000 to 2453716.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2453716.2500\n",
      "Epoch 84/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2450147.0000\n",
      "Epoch 84: loss improved from 2453716.25000 to 2451062.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2451062.2500\n",
      "Epoch 85/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2450843.0000\n",
      "Epoch 85: loss improved from 2451062.25000 to 2450634.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2450634.5000\n",
      "Epoch 86/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2450965.0000\n",
      "Epoch 86: loss improved from 2450634.50000 to 2450291.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2450291.2500\n",
      "Epoch 87/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2447400.5000\n",
      "Epoch 87: loss improved from 2450291.25000 to 2447577.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2447577.5000\n",
      "Epoch 88/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2448215.2500\n",
      "Epoch 88: loss improved from 2447577.50000 to 2446780.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2446780.0000\n",
      "Epoch 89/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2446080.2500\n",
      "Epoch 89: loss improved from 2446780.00000 to 2445531.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2445531.7500\n",
      "Epoch 90/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2445374.2500\n",
      "Epoch 90: loss improved from 2445531.75000 to 2445190.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2445190.2500\n",
      "Epoch 91/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2444488.0000\n",
      "Epoch 91: loss improved from 2445190.25000 to 2443283.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2443283.2500\n",
      "Epoch 92/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2441762.7500\n",
      "Epoch 92: loss improved from 2443283.25000 to 2441434.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2441434.5000\n",
      "Epoch 93/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2440610.2500\n",
      "Epoch 93: loss improved from 2441434.50000 to 2441107.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2441107.0000\n",
      "Epoch 94/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2440740.5000\n",
      "Epoch 94: loss improved from 2441107.00000 to 2439720.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2439720.0000\n",
      "Epoch 95/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2440469.2500\n",
      "Epoch 95: loss did not improve from 2439720.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2440147.5000\n",
      "Epoch 96/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2436652.7500\n",
      "Epoch 96: loss improved from 2439720.00000 to 2437852.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2437852.2500\n",
      "Epoch 97/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2438526.5000\n",
      "Epoch 97: loss did not improve from 2437852.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2437855.5000\n",
      "Epoch 98/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2436016.7500\n",
      "Epoch 98: loss improved from 2437852.25000 to 2436249.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2436249.7500\n",
      "Epoch 99/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2433304.7500\n",
      "Epoch 99: loss improved from 2436249.75000 to 2434332.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2434332.5000\n",
      "Epoch 100/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2433745.0000\n",
      "Epoch 100: loss improved from 2434332.50000 to 2434084.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2434084.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 08:54:31 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 08:54:31 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0j3t08rr\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0j3t08rr\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_151 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   2/3189 [..............................] - ETA: 4:06 - loss: 2908771.5000    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3180/3189 [============================>.] - ETA: 0s - loss: 2492098.7500\n",
      "Epoch 1: loss improved from inf to 2490334.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 67s 4ms/step - loss: 2490334.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2172377.5000\n",
      "Epoch 2: loss improved from 2490334.50000 to 2172394.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2172394.0000\n",
      "Epoch 3/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2013795.6250\n",
      "Epoch 3: loss improved from 2172394.00000 to 2013390.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2013390.6250\n",
      "Epoch 4/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1902048.0000\n",
      "Epoch 4: loss improved from 2013390.62500 to 1901825.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1901825.0000\n",
      "Epoch 5/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1815171.1250\n",
      "Epoch 5: loss improved from 1901825.00000 to 1815171.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1815171.1250\n",
      "Epoch 6/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1743666.0000\n",
      "Epoch 6: loss improved from 1815171.12500 to 1743567.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1743567.2500\n",
      "Epoch 7/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1681679.0000\n",
      "Epoch 7: loss improved from 1743567.25000 to 1681579.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1681579.8750\n",
      "Epoch 8/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1631756.3750\n",
      "Epoch 8: loss improved from 1681579.87500 to 1631855.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1631855.7500\n",
      "Epoch 9/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1583708.8750\n",
      "Epoch 9: loss improved from 1631855.75000 to 1583477.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1583477.8750\n",
      "Epoch 10/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1549069.7500\n",
      "Epoch 10: loss improved from 1583477.87500 to 1549004.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1549004.6250\n",
      "Epoch 11/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1515883.7500\n",
      "Epoch 11: loss improved from 1549004.62500 to 1515512.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1515512.8750\n",
      "Epoch 12/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1484364.2500\n",
      "Epoch 12: loss improved from 1515512.87500 to 1484491.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1484491.2500\n",
      "Epoch 13/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1459169.8750\n",
      "Epoch 13: loss improved from 1484491.25000 to 1458834.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1458834.5000\n",
      "Epoch 14/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1436601.8750\n",
      "Epoch 14: loss improved from 1458834.50000 to 1436044.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1436044.5000\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1416217.7500\n",
      "Epoch 15: loss improved from 1436044.50000 to 1416356.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1416356.5000\n",
      "Epoch 16/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1399013.7500\n",
      "Epoch 16: loss improved from 1416356.50000 to 1399192.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 1399192.1250\n",
      "Epoch 17/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1378609.3750\n",
      "Epoch 17: loss improved from 1399192.12500 to 1380462.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1380462.6250\n",
      "Epoch 18/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1366760.1250\n",
      "Epoch 18: loss improved from 1380462.62500 to 1365725.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1365725.5000\n",
      "Epoch 19/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1351340.0000\n",
      "Epoch 19: loss improved from 1365725.50000 to 1351681.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1351681.5000\n",
      "Epoch 20/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1335454.8750\n",
      "Epoch 20: loss improved from 1351681.50000 to 1336582.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1336582.1250\n",
      "Epoch 21/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1327464.8750\n",
      "Epoch 21: loss improved from 1336582.12500 to 1327359.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1327359.8750\n",
      "Epoch 22/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1315090.5000\n",
      "Epoch 22: loss improved from 1327359.87500 to 1315287.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1315287.3750\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1303672.2500\n",
      "Epoch 23: loss improved from 1315287.37500 to 1303271.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1303271.0000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1292233.8750\n",
      "Epoch 24: loss improved from 1303271.00000 to 1292262.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1292262.5000\n",
      "Epoch 25/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1284592.8750\n",
      "Epoch 25: loss improved from 1292262.50000 to 1283932.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1283932.0000\n",
      "Epoch 26/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1275510.8750\n",
      "Epoch 26: loss improved from 1283932.00000 to 1274652.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1274652.8750\n",
      "Epoch 27/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1267807.2500\n",
      "Epoch 27: loss improved from 1274652.87500 to 1267373.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1267373.2500\n",
      "Epoch 28/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1258919.7500\n",
      "Epoch 28: loss improved from 1267373.25000 to 1258630.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1258630.2500\n",
      "Epoch 29/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1251302.0000\n",
      "Epoch 29: loss improved from 1258630.25000 to 1251279.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1251279.2500\n",
      "Epoch 30/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1244794.6250\n",
      "Epoch 30: loss improved from 1251279.25000 to 1244110.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1244110.3750\n",
      "Epoch 31/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1237876.7500\n",
      "Epoch 31: loss improved from 1244110.37500 to 1237854.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1237854.1250\n",
      "Epoch 32/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1229172.8750\n",
      "Epoch 32: loss improved from 1237854.12500 to 1229340.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1229340.3750\n",
      "Epoch 33/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1224365.6250\n",
      "Epoch 33: loss improved from 1229340.37500 to 1224873.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1224873.7500\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1217782.1250\n",
      "Epoch 34: loss improved from 1224873.75000 to 1217942.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1217942.6250\n",
      "Epoch 35/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1209942.0000\n",
      "Epoch 35: loss improved from 1217942.62500 to 1210198.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1210198.2500\n",
      "Epoch 36/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1204267.3750\n",
      "Epoch 36: loss improved from 1210198.25000 to 1204106.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1204106.7500\n",
      "Epoch 37/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1199810.8750\n",
      "Epoch 37: loss improved from 1204106.75000 to 1199850.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1199850.6250\n",
      "Epoch 38/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1194766.8750\n",
      "Epoch 38: loss improved from 1199850.62500 to 1195183.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1195183.1250\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1189315.6250\n",
      "Epoch 39: loss improved from 1195183.12500 to 1189315.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1189315.6250\n",
      "Epoch 40/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1181172.6250\n",
      "Epoch 40: loss improved from 1189315.62500 to 1183024.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1183024.6250\n",
      "Epoch 41/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1177401.3750\n",
      "Epoch 41: loss improved from 1183024.62500 to 1177297.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1177297.8750\n",
      "Epoch 42/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1176090.7500\n",
      "Epoch 42: loss improved from 1177297.87500 to 1175933.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1175933.3750\n",
      "Epoch 43/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1171622.1250\n",
      "Epoch 43: loss improved from 1175933.37500 to 1171433.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1171433.3750\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1166565.1250\n",
      "Epoch 44: loss improved from 1171433.37500 to 1167992.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1167992.3750\n",
      "Epoch 45/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1159475.7500\n",
      "Epoch 45: loss improved from 1167992.37500 to 1158960.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1158960.6250\n",
      "Epoch 46/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1157357.7500\n",
      "Epoch 46: loss improved from 1158960.62500 to 1156730.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1156730.5000\n",
      "Epoch 47/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1154796.5000\n",
      "Epoch 47: loss improved from 1156730.50000 to 1154661.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1154661.7500\n",
      "Epoch 48/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1149300.7500\n",
      "Epoch 48: loss improved from 1154661.75000 to 1150484.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1150484.3750\n",
      "Epoch 49/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1147533.1250\n",
      "Epoch 49: loss improved from 1150484.37500 to 1147228.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1147228.3750\n",
      "Epoch 50/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1141915.2500\n",
      "Epoch 50: loss improved from 1147228.37500 to 1142101.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1142101.1250\n",
      "Epoch 51/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1138110.2500\n",
      "Epoch 51: loss improved from 1142101.12500 to 1137478.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1137478.5000\n",
      "Epoch 52/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1135935.5000\n",
      "Epoch 52: loss improved from 1137478.50000 to 1136291.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1136291.3750\n",
      "Epoch 53/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1130016.6250\n",
      "Epoch 53: loss improved from 1136291.37500 to 1130140.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1130140.2500\n",
      "Epoch 54/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1130470.1250\n",
      "Epoch 54: loss improved from 1130140.25000 to 1130065.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1130065.3750\n",
      "Epoch 55/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1127123.2500\n",
      "Epoch 55: loss improved from 1130065.37500 to 1127321.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1127321.6250\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1123671.7500\n",
      "Epoch 56: loss improved from 1127321.62500 to 1123296.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1123296.0000\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1118589.1250\n",
      "Epoch 57: loss improved from 1123296.00000 to 1118664.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1118664.6250\n",
      "Epoch 58/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1116256.1250\n",
      "Epoch 58: loss improved from 1118664.62500 to 1116188.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1116188.7500\n",
      "Epoch 59/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1112727.2500\n",
      "Epoch 59: loss improved from 1116188.75000 to 1113668.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1113668.0000\n",
      "Epoch 60/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1109203.6250\n",
      "Epoch 60: loss improved from 1113668.00000 to 1109478.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1109478.1250\n",
      "Epoch 61/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1106581.8750\n",
      "Epoch 61: loss improved from 1109478.12500 to 1106883.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1106883.1250\n",
      "Epoch 62/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1105445.0000\n",
      "Epoch 62: loss improved from 1106883.12500 to 1105279.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1105279.0000\n",
      "Epoch 63/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1099632.7500\n",
      "Epoch 63: loss improved from 1105279.00000 to 1099396.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1099396.6250\n",
      "Epoch 64/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1098059.2500\n",
      "Epoch 64: loss improved from 1099396.62500 to 1098007.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1098007.3750\n",
      "Epoch 65/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1097476.6250\n",
      "Epoch 65: loss improved from 1098007.37500 to 1097543.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1097543.1250\n",
      "Epoch 66/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1096437.6250\n",
      "Epoch 66: loss improved from 1097543.12500 to 1096202.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 24s 7ms/step - loss: 1096202.8750\n",
      "Epoch 67/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1092394.5000\n",
      "Epoch 67: loss improved from 1096202.87500 to 1091996.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 1091996.1250\n",
      "Epoch 68/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1092284.0000\n",
      "Epoch 68: loss did not improve from 1091996.12500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1092465.5000\n",
      "Epoch 69/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1088517.5000\n",
      "Epoch 69: loss improved from 1091996.12500 to 1088393.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1088393.7500\n",
      "Epoch 70/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1087271.0000\n",
      "Epoch 70: loss improved from 1088393.75000 to 1087018.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1087018.0000\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1085913.5000\n",
      "Epoch 71: loss improved from 1087018.00000 to 1085288.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1085288.3750\n",
      "Epoch 72/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1082696.2500\n",
      "Epoch 72: loss improved from 1085288.37500 to 1082585.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1082585.7500\n",
      "Epoch 73/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1080433.6250\n",
      "Epoch 73: loss improved from 1082585.75000 to 1079958.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1079958.5000\n",
      "Epoch 74/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1076701.8750\n",
      "Epoch 74: loss improved from 1079958.50000 to 1076828.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1076828.2500\n",
      "Epoch 75/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1075884.1250\n",
      "Epoch 75: loss improved from 1076828.25000 to 1075839.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1075839.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1071677.3750\n",
      "Epoch 76: loss improved from 1075839.25000 to 1072478.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1072478.7500\n",
      "Epoch 77/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1072389.5000\n",
      "Epoch 77: loss improved from 1072478.75000 to 1071959.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1071959.5000\n",
      "Epoch 78/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1071490.7500\n",
      "Epoch 78: loss improved from 1071959.50000 to 1071516.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1071516.0000\n",
      "Epoch 79/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1067196.6250\n",
      "Epoch 79: loss improved from 1071516.00000 to 1067099.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1067099.3750\n",
      "Epoch 80/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1065093.0000\n",
      "Epoch 80: loss improved from 1067099.37500 to 1064785.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1064785.0000\n",
      "Epoch 81/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1062386.3750\n",
      "Epoch 81: loss improved from 1064785.00000 to 1063997.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1063997.8750\n",
      "Epoch 82/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1061925.6250\n",
      "Epoch 82: loss improved from 1063997.87500 to 1062317.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1062317.7500\n",
      "Epoch 83/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1060160.3750\n",
      "Epoch 83: loss improved from 1062317.75000 to 1059882.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1059882.0000\n",
      "Epoch 84/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1058103.6250\n",
      "Epoch 84: loss improved from 1059882.00000 to 1057944.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1057944.1250\n",
      "Epoch 85/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1058032.8750\n",
      "Epoch 85: loss did not improve from 1057944.12500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1057986.5000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1058202.0000\n",
      "Epoch 86: loss did not improve from 1057944.12500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1058202.0000\n",
      "Epoch 87/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1053124.1250\n",
      "Epoch 87: loss improved from 1057944.12500 to 1053044.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1053044.0000\n",
      "Epoch 88/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1053193.6250\n",
      "Epoch 88: loss did not improve from 1053044.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1053178.0000\n",
      "Epoch 89/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1050823.3750\n",
      "Epoch 89: loss improved from 1053044.00000 to 1051366.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1051366.6250\n",
      "Epoch 90/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1049506.5000\n",
      "Epoch 90: loss improved from 1051366.62500 to 1050373.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1050373.6250\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1049246.0000\n",
      "Epoch 91: loss improved from 1050373.62500 to 1049108.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1049108.0000\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1047152.8750\n",
      "Epoch 92: loss improved from 1049108.00000 to 1047118.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1047118.5000\n",
      "Epoch 93/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1042762.6875\n",
      "Epoch 93: loss improved from 1047118.50000 to 1042930.81250, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1042930.8125\n",
      "Epoch 94/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1043505.9375\n",
      "Epoch 94: loss did not improve from 1042930.81250\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1043634.5000\n",
      "Epoch 95/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1040299.8125\n",
      "Epoch 95: loss improved from 1042930.81250 to 1041123.18750, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1041123.1875\n",
      "Epoch 96/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1040647.0000\n",
      "Epoch 96: loss improved from 1041123.18750 to 1040519.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1040519.1250\n",
      "Epoch 97/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1037906.3125\n",
      "Epoch 97: loss improved from 1040519.12500 to 1037751.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1037751.6250\n",
      "Epoch 98/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1038860.9375\n",
      "Epoch 98: loss did not improve from 1037751.62500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1038657.9375\n",
      "Epoch 99/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1035472.8125\n",
      "Epoch 99: loss improved from 1037751.62500 to 1035480.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1035480.2500\n",
      "Epoch 100/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1033418.3125\n",
      "Epoch 100: loss improved from 1035480.25000 to 1033604.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1033604.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 09:16:02 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 09:16:02 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp9d5h88vu\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp9d5h88vu\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_155 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   6/3189 [..............................] - ETA: 1:57 - loss: 5000311.5000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3177/3189 [============================>.] - ETA: 0s - loss: 2740867.0000\n",
      "Epoch 1: loss improved from inf to 2742079.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2742079.5000\n",
      "Epoch 2/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2730893.0000\n",
      "Epoch 2: loss improved from 2742079.50000 to 2730907.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2730907.0000\n",
      "Epoch 3/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2722665.7500\n",
      "Epoch 3: loss improved from 2730907.00000 to 2722665.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2722665.7500\n",
      "Epoch 4/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2713851.2500\n",
      "Epoch 4: loss improved from 2722665.75000 to 2713389.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2713389.2500\n",
      "Epoch 5/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2702796.2500\n",
      "Epoch 5: loss improved from 2713389.25000 to 2702418.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2702418.7500\n",
      "Epoch 6/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2692640.2500\n",
      "Epoch 6: loss improved from 2702418.75000 to 2692021.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2692021.0000\n",
      "Epoch 7/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2683032.2500\n",
      "Epoch 7: loss improved from 2692021.00000 to 2681494.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2681494.5000\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2671854.7500\n",
      "Epoch 8: loss improved from 2681494.50000 to 2671668.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2671668.2500\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2659646.7500\n",
      "Epoch 9: loss improved from 2671668.25000 to 2661593.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2661593.2500\n",
      "Epoch 10/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2654799.0000\n",
      "Epoch 10: loss improved from 2661593.25000 to 2654799.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2654799.0000\n",
      "Epoch 11/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2644282.5000\n",
      "Epoch 11: loss improved from 2654799.00000 to 2645417.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2645417.7500\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2638505.2500\n",
      "Epoch 12: loss improved from 2645417.75000 to 2638430.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2638430.0000\n",
      "Epoch 13/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2632201.7500\n",
      "Epoch 13: loss improved from 2638430.00000 to 2632150.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2632150.5000\n",
      "Epoch 14/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2624758.5000\n",
      "Epoch 14: loss improved from 2632150.50000 to 2625596.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2625596.0000\n",
      "Epoch 15/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2620814.7500\n",
      "Epoch 15: loss improved from 2625596.00000 to 2621316.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2621316.2500\n",
      "Epoch 16/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2615545.5000\n",
      "Epoch 16: loss improved from 2621316.25000 to 2615688.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2615688.7500\n",
      "Epoch 17/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2609488.0000\n",
      "Epoch 17: loss improved from 2615688.75000 to 2609658.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2609658.7500\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2606560.5000\n",
      "Epoch 18: loss improved from 2609658.75000 to 2606093.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2606093.5000\n",
      "Epoch 19/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2599352.0000\n",
      "Epoch 19: loss improved from 2606093.50000 to 2598870.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2598870.2500\n",
      "Epoch 20/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2594218.0000\n",
      "Epoch 20: loss improved from 2598870.25000 to 2594369.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2594369.5000\n",
      "Epoch 21/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2586428.2500\n",
      "Epoch 21: loss improved from 2594369.50000 to 2589606.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2589606.5000\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2585265.2500\n",
      "Epoch 22: loss improved from 2589606.50000 to 2585672.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2585672.5000\n",
      "Epoch 23/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2581118.2500\n",
      "Epoch 23: loss improved from 2585672.50000 to 2580579.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2580579.7500\n",
      "Epoch 24/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2576080.2500\n",
      "Epoch 24: loss improved from 2580579.75000 to 2576167.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2576167.0000\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2572053.0000\n",
      "Epoch 25: loss improved from 2576167.00000 to 2571974.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2571974.7500\n",
      "Epoch 26/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2568058.2500\n",
      "Epoch 26: loss improved from 2571974.75000 to 2566866.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2566866.0000\n",
      "Epoch 27/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2564664.2500\n",
      "Epoch 27: loss improved from 2566866.00000 to 2563773.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2563773.5000\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2559874.0000\n",
      "Epoch 28: loss improved from 2563773.50000 to 2560175.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2560175.2500\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2556762.0000\n",
      "Epoch 29: loss improved from 2560175.25000 to 2556762.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2556762.0000\n",
      "Epoch 30/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2550582.7500\n",
      "Epoch 30: loss improved from 2556762.00000 to 2551152.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2551152.7500\n",
      "Epoch 31/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2548436.5000\n",
      "Epoch 31: loss improved from 2551152.75000 to 2547715.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2547715.2500\n",
      "Epoch 32/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2548869.5000\n",
      "Epoch 32: loss did not improve from 2547715.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2548447.5000\n",
      "Epoch 33/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2543602.2500\n",
      "Epoch 33: loss improved from 2547715.25000 to 2544980.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2544980.2500\n",
      "Epoch 34/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2542531.0000\n",
      "Epoch 34: loss improved from 2544980.25000 to 2541514.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2541514.0000\n",
      "Epoch 35/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2536092.7500\n",
      "Epoch 35: loss improved from 2541514.00000 to 2536265.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 23s 7ms/step - loss: 2536265.0000\n",
      "Epoch 36/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2535829.2500\n",
      "Epoch 36: loss improved from 2536265.00000 to 2535321.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2535321.5000\n",
      "Epoch 37/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2532310.5000\n",
      "Epoch 37: loss improved from 2535321.50000 to 2533872.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533872.2500\n",
      "Epoch 38/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2529981.0000\n",
      "Epoch 38: loss improved from 2533872.25000 to 2530850.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530850.2500\n",
      "Epoch 39/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2526231.5000\n",
      "Epoch 39: loss improved from 2530850.25000 to 2526471.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526471.5000\n",
      "Epoch 40/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2525717.7500\n",
      "Epoch 40: loss improved from 2526471.50000 to 2525803.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525803.5000\n",
      "Epoch 41/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2526101.2500\n",
      "Epoch 41: loss improved from 2525803.50000 to 2525661.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525661.2500\n",
      "Epoch 42/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2523557.0000\n",
      "Epoch 42: loss improved from 2525661.25000 to 2524040.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2524040.7500\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2519247.7500\n",
      "Epoch 43: loss improved from 2524040.75000 to 2519466.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2519466.2500\n",
      "Epoch 44/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2519231.0000\n",
      "Epoch 44: loss improved from 2519466.25000 to 2518954.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2518954.2500\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2517181.2500\n",
      "Epoch 45: loss improved from 2518954.25000 to 2517181.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2517181.2500\n",
      "Epoch 46/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2515688.5000\n",
      "Epoch 46: loss improved from 2517181.25000 to 2514840.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514840.0000\n",
      "Epoch 47/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2514720.0000\n",
      "Epoch 47: loss did not improve from 2514840.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2515664.7500\n",
      "Epoch 48/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2512887.2500\n",
      "Epoch 48: loss improved from 2514840.00000 to 2513109.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2513109.5000\n",
      "Epoch 49/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2511348.7500\n",
      "Epoch 49: loss improved from 2513109.50000 to 2511258.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2511258.0000\n",
      "Epoch 50/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2512285.5000\n",
      "Epoch 50: loss did not improve from 2511258.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2512379.2500\n",
      "Epoch 51/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2509464.7500\n",
      "Epoch 51: loss improved from 2511258.00000 to 2509758.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509758.7500\n",
      "Epoch 52/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2510429.0000\n",
      "Epoch 52: loss improved from 2509758.75000 to 2509064.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509064.0000\n",
      "Epoch 53/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2508939.0000\n",
      "Epoch 53: loss did not improve from 2509064.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509242.2500\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2507603.0000\n",
      "Epoch 54: loss improved from 2509064.00000 to 2507631.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2507631.7500\n",
      "Epoch 55/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2505484.0000\n",
      "Epoch 55: loss improved from 2507631.75000 to 2505597.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2505597.2500\n",
      "Epoch 56/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2503041.0000\n",
      "Epoch 56: loss improved from 2505597.25000 to 2503597.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2503597.0000\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2505594.0000\n",
      "Epoch 57: loss did not improve from 2503597.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2505479.7500\n",
      "Epoch 58/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2502868.2500\n",
      "Epoch 58: loss improved from 2503597.00000 to 2503407.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2503407.2500\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2503537.7500\n",
      "Epoch 59: loss improved from 2503407.25000 to 2502670.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2502670.5000\n",
      "Epoch 60/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2501615.5000\n",
      "Epoch 60: loss did not improve from 2502670.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2503521.2500\n",
      "Epoch 61/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2499644.5000\n",
      "Epoch 61: loss improved from 2502670.50000 to 2500581.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2500581.7500\n",
      "Epoch 62/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2499738.7500\n",
      "Epoch 62: loss improved from 2500581.75000 to 2500441.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2500441.7500\n",
      "Epoch 63/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2499116.7500\n",
      "Epoch 63: loss improved from 2500441.75000 to 2499188.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2499188.7500\n",
      "Epoch 64/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2497379.0000\n",
      "Epoch 64: loss improved from 2499188.75000 to 2498296.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2498296.2500\n",
      "Epoch 65/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2497672.5000\n",
      "Epoch 65: loss did not improve from 2498296.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2498419.2500\n",
      "Epoch 66/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2498261.5000\n",
      "Epoch 66: loss improved from 2498296.25000 to 2495977.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2495977.5000\n",
      "Epoch 67/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2497812.2500\n",
      "Epoch 67: loss did not improve from 2495977.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2496936.7500\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2494618.7500\n",
      "Epoch 68: loss improved from 2495977.50000 to 2494987.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2494987.5000\n",
      "Epoch 69/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2495800.7500\n",
      "Epoch 69: loss did not improve from 2494987.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2496021.7500\n",
      "Epoch 70/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2495613.7500\n",
      "Epoch 70: loss did not improve from 2494987.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2496359.2500\n",
      "Epoch 71/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2494298.2500\n",
      "Epoch 71: loss improved from 2494987.50000 to 2494708.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2494708.0000\n",
      "Epoch 72/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2496695.0000\n",
      "Epoch 72: loss did not improve from 2494708.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2495770.0000\n",
      "Epoch 73/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2495900.0000\n",
      "Epoch 73: loss did not improve from 2494708.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2495850.7500\n",
      "Epoch 74/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2492890.5000\n",
      "Epoch 74: loss improved from 2494708.00000 to 2492452.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2492452.7500\n",
      "Epoch 75/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2494341.0000\n",
      "Epoch 75: loss improved from 2492452.75000 to 2491862.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491862.0000\n",
      "Epoch 76/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2493009.0000\n",
      "Epoch 76: loss did not improve from 2491862.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2492145.2500\n",
      "Epoch 77/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2490390.7500\n",
      "Epoch 77: loss improved from 2491862.00000 to 2491324.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491324.2500\n",
      "Epoch 78/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2489592.5000\n",
      "Epoch 78: loss improved from 2491324.25000 to 2491096.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491096.7500\n",
      "Epoch 79/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2488854.0000\n",
      "Epoch 79: loss did not improve from 2491096.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491265.5000\n",
      "Epoch 80/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2492033.0000\n",
      "Epoch 80: loss did not improve from 2491096.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491934.5000\n",
      "Epoch 81/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2492384.2500\n",
      "Epoch 81: loss did not improve from 2491096.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2492429.0000\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2491065.5000\n",
      "Epoch 82: loss improved from 2491096.75000 to 2491065.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491065.5000\n",
      "Epoch 83/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2488175.2500\n",
      "Epoch 83: loss improved from 2491065.50000 to 2489561.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2489561.2500\n",
      "Epoch 84/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2491245.5000\n",
      "Epoch 84: loss did not improve from 2489561.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2490294.2500\n",
      "Epoch 85/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2486395.7500\n",
      "Epoch 85: loss improved from 2489561.25000 to 2486448.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2486448.5000\n",
      "Epoch 86/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2487833.5000\n",
      "Epoch 86: loss did not improve from 2486448.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2487525.0000\n",
      "Epoch 87/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2486477.7500\n",
      "Epoch 87: loss did not improve from 2486448.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2487378.0000\n",
      "Epoch 88/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2487053.5000\n",
      "Epoch 88: loss did not improve from 2486448.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2488123.5000\n",
      "Epoch 89/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2485827.0000\n",
      "Epoch 89: loss did not improve from 2486448.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2486616.0000\n",
      "Epoch 90/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2484874.7500\n",
      "Epoch 90: loss improved from 2486448.50000 to 2484201.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2484201.0000\n",
      "Epoch 91/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2487592.5000\n",
      "Epoch 91: loss did not improve from 2484201.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2485041.2500\n",
      "Epoch 92/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2485037.7500\n",
      "Epoch 92: loss did not improve from 2484201.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2485441.2500\n",
      "Epoch 93/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2484863.2500\n",
      "Epoch 93: loss did not improve from 2484201.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2484419.2500\n",
      "Epoch 94/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2482500.2500\n",
      "Epoch 94: loss improved from 2484201.00000 to 2483496.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2483496.5000\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2485154.5000\n",
      "Epoch 95: loss did not improve from 2483496.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2484965.7500\n",
      "Epoch 96/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2483790.7500\n",
      "Epoch 96: loss improved from 2483496.50000 to 2483189.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2483189.0000\n",
      "Epoch 97/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2482010.5000\n",
      "Epoch 97: loss improved from 2483189.00000 to 2481995.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2481995.2500\n",
      "Epoch 98/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2482495.5000\n",
      "Epoch 98: loss did not improve from 2481995.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2482466.7500\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2481950.7500\n",
      "Epoch 99: loss improved from 2481995.25000 to 2481635.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2481635.2500\n",
      "Epoch 100/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2481331.2500\n",
      "Epoch 100: loss improved from 2481635.25000 to 2481425.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2481425.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 09:36:27 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 09:36:27 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpwob7zk0s\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpwob7zk0s\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_159 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_160 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 1:01 - loss: 4607608.5000   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0091s vs `on_train_batch_end` time: 0.0291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0091s vs `on_train_batch_end` time: 0.0291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3189 [============================>.] - ETA: 0s - loss: 2688613.7500\n",
      "Epoch 1: loss improved from inf to 2688449.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 3ms/step - loss: 2688449.7500\n",
      "Epoch 2/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2597669.7500\n",
      "Epoch 2: loss improved from 2688449.75000 to 2598079.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2598079.0000\n",
      "Epoch 3/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2534536.0000\n",
      "Epoch 3: loss improved from 2598079.00000 to 2535251.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2535251.5000\n",
      "Epoch 4/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2491567.5000\n",
      "Epoch 4: loss improved from 2535251.50000 to 2489790.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2489790.0000\n",
      "Epoch 5/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2452731.7500\n",
      "Epoch 5: loss improved from 2489790.00000 to 2454005.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2454005.5000\n",
      "Epoch 6/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2420832.0000\n",
      "Epoch 6: loss improved from 2454005.50000 to 2420120.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2420120.7500\n",
      "Epoch 7/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2390480.5000\n",
      "Epoch 7: loss improved from 2420120.75000 to 2389174.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2389174.0000\n",
      "Epoch 8/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2360899.0000\n",
      "Epoch 8: loss improved from 2389174.00000 to 2360899.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2360899.0000\n",
      "Epoch 9/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2335179.2500\n",
      "Epoch 9: loss improved from 2360899.00000 to 2335130.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2335130.2500\n",
      "Epoch 10/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2315201.7500\n",
      "Epoch 10: loss improved from 2335130.25000 to 2314802.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2314802.5000\n",
      "Epoch 11/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2293483.0000\n",
      "Epoch 11: loss improved from 2314802.50000 to 2295283.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2295283.0000\n",
      "Epoch 12/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2275554.2500\n",
      "Epoch 12: loss improved from 2295283.00000 to 2275681.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2275681.7500\n",
      "Epoch 13/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2257365.5000\n",
      "Epoch 13: loss improved from 2275681.75000 to 2257563.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2257563.7500\n",
      "Epoch 14/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2241712.0000\n",
      "Epoch 14: loss improved from 2257563.75000 to 2240889.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2240889.2500\n",
      "Epoch 15/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2229905.5000\n",
      "Epoch 15: loss improved from 2240889.25000 to 2227277.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2227277.2500\n",
      "Epoch 16/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2212966.2500\n",
      "Epoch 16: loss improved from 2227277.25000 to 2212838.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2212838.2500\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2200909.0000\n",
      "Epoch 17: loss improved from 2212838.25000 to 2200545.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2200545.5000\n",
      "Epoch 18/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2189601.5000\n",
      "Epoch 18: loss improved from 2200545.50000 to 2189385.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2189385.2500\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2178493.0000\n",
      "Epoch 19: loss improved from 2189385.25000 to 2178642.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2178642.0000\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2165905.7500\n",
      "Epoch 20: loss improved from 2178642.00000 to 2166107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2166107.7500\n",
      "Epoch 21/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2159144.0000\n",
      "Epoch 21: loss improved from 2166107.75000 to 2158823.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2158823.7500\n",
      "Epoch 22/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2146992.7500\n",
      "Epoch 22: loss improved from 2158823.75000 to 2148081.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2148081.2500\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2138309.2500\n",
      "Epoch 23: loss improved from 2148081.25000 to 2140080.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2140080.0000\n",
      "Epoch 24/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2133915.2500\n",
      "Epoch 24: loss improved from 2140080.00000 to 2132543.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2132543.7500\n",
      "Epoch 25/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2122603.5000\n",
      "Epoch 25: loss improved from 2132543.75000 to 2122734.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2122734.5000\n",
      "Epoch 26/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2119063.7500\n",
      "Epoch 26: loss improved from 2122734.50000 to 2117776.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2117776.2500\n",
      "Epoch 27/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2108160.7500\n",
      "Epoch 27: loss improved from 2117776.25000 to 2109652.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2109652.5000\n",
      "Epoch 28/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2102782.0000\n",
      "Epoch 28: loss improved from 2109652.50000 to 2103978.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2103978.7500\n",
      "Epoch 29/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2096567.6250\n",
      "Epoch 29: loss improved from 2103978.75000 to 2096909.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2096909.5000\n",
      "Epoch 30/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2092016.7500\n",
      "Epoch 30: loss improved from 2096909.50000 to 2091152.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2091152.0000\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2083473.6250\n",
      "Epoch 31: loss improved from 2091152.00000 to 2083628.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2083628.8750\n",
      "Epoch 32/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2079298.8750\n",
      "Epoch 32: loss improved from 2083628.87500 to 2079415.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2079415.6250\n",
      "Epoch 33/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2072706.1250\n",
      "Epoch 33: loss improved from 2079415.62500 to 2072646.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2072646.1250\n",
      "Epoch 34/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2068057.2500\n",
      "Epoch 34: loss improved from 2072646.12500 to 2067413.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2067413.0000\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2063402.6250\n",
      "Epoch 35: loss improved from 2067413.00000 to 2063402.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2063402.6250\n",
      "Epoch 36/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2057613.3750\n",
      "Epoch 36: loss improved from 2063402.62500 to 2058576.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2058576.5000\n",
      "Epoch 37/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2052693.7500\n",
      "Epoch 37: loss improved from 2058576.50000 to 2053518.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2053518.1250\n",
      "Epoch 38/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2049932.8750\n",
      "Epoch 38: loss improved from 2053518.12500 to 2049082.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2049082.1250\n",
      "Epoch 39/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2047322.2500\n",
      "Epoch 39: loss improved from 2049082.12500 to 2046069.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2046069.6250\n",
      "Epoch 40/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2040495.6250\n",
      "Epoch 40: loss improved from 2046069.62500 to 2040664.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2040664.7500\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2036216.6250\n",
      "Epoch 41: loss improved from 2040664.75000 to 2036216.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2036216.6250\n",
      "Epoch 42/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2032657.6250\n",
      "Epoch 42: loss improved from 2036216.62500 to 2032668.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2032668.0000\n",
      "Epoch 43/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2029901.7500\n",
      "Epoch 43: loss improved from 2032668.00000 to 2028660.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2028660.5000\n",
      "Epoch 44/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2024230.6250\n",
      "Epoch 44: loss improved from 2028660.50000 to 2023643.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2023643.1250\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2020804.8750\n",
      "Epoch 45: loss improved from 2023643.12500 to 2020354.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2020354.5000\n",
      "Epoch 46/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2018301.6250\n",
      "Epoch 46: loss improved from 2020354.50000 to 2017568.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2017568.7500\n",
      "Epoch 47/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2014114.6250\n",
      "Epoch 47: loss improved from 2017568.75000 to 2014057.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2014057.6250\n",
      "Epoch 48/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2010640.0000\n",
      "Epoch 48: loss improved from 2014057.62500 to 2009960.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2009960.7500\n",
      "Epoch 49/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2006132.7500\n",
      "Epoch 49: loss improved from 2009960.75000 to 2008105.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2008105.2500\n",
      "Epoch 50/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2003622.5000\n",
      "Epoch 50: loss improved from 2008105.25000 to 2003876.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2003876.6250\n",
      "Epoch 51/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2002407.7500\n",
      "Epoch 51: loss improved from 2003876.62500 to 2001284.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2001284.0000\n",
      "Epoch 52/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1996448.6250\n",
      "Epoch 52: loss improved from 2001284.00000 to 1996717.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1996717.2500\n",
      "Epoch 53/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1994147.5000\n",
      "Epoch 53: loss improved from 1996717.25000 to 1994177.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1994177.0000\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1992002.8750\n",
      "Epoch 54: loss improved from 1994177.00000 to 1991562.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1991562.1250\n",
      "Epoch 55/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1987012.1250\n",
      "Epoch 55: loss improved from 1991562.12500 to 1988460.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1988460.0000\n",
      "Epoch 56/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1984770.6250\n",
      "Epoch 56: loss improved from 1988460.00000 to 1985227.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1985227.3750\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1982924.2500\n",
      "Epoch 57: loss improved from 1985227.37500 to 1982418.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1982418.1250\n",
      "Epoch 58/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1982029.1250\n",
      "Epoch 58: loss improved from 1982418.12500 to 1982154.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1982154.1250\n",
      "Epoch 59/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1978609.0000\n",
      "Epoch 59: loss improved from 1982154.12500 to 1978609.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1978609.0000\n",
      "Epoch 60/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1976548.1250\n",
      "Epoch 60: loss improved from 1978609.00000 to 1976307.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1976307.3750\n",
      "Epoch 61/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1974021.5000\n",
      "Epoch 61: loss improved from 1976307.37500 to 1973420.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1973420.1250\n",
      "Epoch 62/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1972320.1250\n",
      "Epoch 62: loss improved from 1973420.12500 to 1971926.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1971926.8750\n",
      "Epoch 63/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1967612.7500\n",
      "Epoch 63: loss improved from 1971926.87500 to 1968455.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1968455.0000\n",
      "Epoch 64/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1968148.8750\n",
      "Epoch 64: loss improved from 1968455.00000 to 1967268.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1967268.2500\n",
      "Epoch 65/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1962776.3750\n",
      "Epoch 65: loss improved from 1967268.25000 to 1963791.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1963791.8750\n",
      "Epoch 66/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1960877.2500\n",
      "Epoch 66: loss improved from 1963791.87500 to 1960862.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1960862.1250\n",
      "Epoch 67/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1959269.1250\n",
      "Epoch 67: loss improved from 1960862.12500 to 1959173.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1959173.3750\n",
      "Epoch 68/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1956500.3750\n",
      "Epoch 68: loss improved from 1959173.37500 to 1957124.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1957124.7500\n",
      "Epoch 69/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1954648.7500\n",
      "Epoch 69: loss improved from 1957124.75000 to 1955483.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1955483.8750\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1953034.1250\n",
      "Epoch 70: loss improved from 1955483.87500 to 1953034.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1953034.1250\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1952503.8750\n",
      "Epoch 71: loss improved from 1953034.12500 to 1952503.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1952503.8750\n",
      "Epoch 72/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1949619.3750\n",
      "Epoch 72: loss improved from 1952503.87500 to 1949781.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1949781.2500\n",
      "Epoch 73/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1947514.7500\n",
      "Epoch 73: loss improved from 1949781.25000 to 1947778.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1947778.0000\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1945973.7500\n",
      "Epoch 74: loss improved from 1947778.00000 to 1945973.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1945973.7500\n",
      "Epoch 75/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1944573.5000\n",
      "Epoch 75: loss improved from 1945973.75000 to 1944628.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1944628.8750\n",
      "Epoch 76/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1942456.7500\n",
      "Epoch 76: loss improved from 1944628.87500 to 1942740.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1942740.3750\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1940717.5000\n",
      "Epoch 77: loss improved from 1942740.37500 to 1940717.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1940717.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1940652.2500\n",
      "Epoch 78: loss did not improve from 1940717.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1940811.6250\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1935786.2500\n",
      "Epoch 79: loss improved from 1940717.50000 to 1935825.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1935825.7500\n",
      "Epoch 80/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1935998.7500\n",
      "Epoch 80: loss improved from 1935825.75000 to 1935439.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1935439.3750\n",
      "Epoch 81/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1933649.1250\n",
      "Epoch 81: loss improved from 1935439.37500 to 1933506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1933506.0000\n",
      "Epoch 82/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1933123.5000\n",
      "Epoch 82: loss improved from 1933506.00000 to 1932682.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 5ms/step - loss: 1932682.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1930701.6250\n",
      "Epoch 83: loss improved from 1932682.75000 to 1931554.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1931554.1250\n",
      "Epoch 84/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1928319.7500\n",
      "Epoch 84: loss improved from 1931554.12500 to 1928150.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1928150.5000\n",
      "Epoch 85/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1926803.2500\n",
      "Epoch 85: loss improved from 1928150.50000 to 1926809.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1926809.5000\n",
      "Epoch 86/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1926778.7500\n",
      "Epoch 86: loss improved from 1926809.50000 to 1926746.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1926746.7500\n",
      "Epoch 87/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1923953.8750\n",
      "Epoch 87: loss improved from 1926746.75000 to 1923699.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1923699.3750\n",
      "Epoch 88/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1922538.5000\n",
      "Epoch 88: loss improved from 1923699.37500 to 1922549.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1922549.1250\n",
      "Epoch 89/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1920721.6250\n",
      "Epoch 89: loss improved from 1922549.12500 to 1921228.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1921228.6250\n",
      "Epoch 90/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1919337.7500\n",
      "Epoch 90: loss improved from 1921228.62500 to 1918905.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1918905.8750\n",
      "Epoch 91/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1917422.1250\n",
      "Epoch 91: loss improved from 1918905.87500 to 1916861.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1916861.3750\n",
      "Epoch 92/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1917683.1250\n",
      "Epoch 92: loss did not improve from 1916861.37500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1917110.7500\n",
      "Epoch 93/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1915662.6250\n",
      "Epoch 93: loss improved from 1916861.37500 to 1914861.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1914861.8750\n",
      "Epoch 94/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1913519.6250\n",
      "Epoch 94: loss improved from 1914861.87500 to 1912834.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1912834.1250\n",
      "Epoch 95/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1912545.1250\n",
      "Epoch 95: loss improved from 1912834.12500 to 1912676.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1912676.2500\n",
      "Epoch 96/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1911290.5000\n",
      "Epoch 96: loss improved from 1912676.25000 to 1911151.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1911151.0000\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1909618.3750\n",
      "Epoch 97: loss improved from 1911151.00000 to 1909335.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1909335.3750\n",
      "Epoch 98/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1908108.7500\n",
      "Epoch 98: loss improved from 1909335.37500 to 1908730.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1908730.0000\n",
      "Epoch 99/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1907395.5000\n",
      "Epoch 99: loss improved from 1908730.00000 to 1907623.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1907623.8750\n",
      "Epoch 100/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1907192.1250\n",
      "Epoch 100: loss improved from 1907623.87500 to 1906601.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1906601.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 09:57:35 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 09:57:35 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_8bgncsy\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_8bgncsy\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_167 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 2:12:45 - loss: 3587569.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0170s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0170s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2733883.5000\n",
      "Epoch 1: loss improved from inf to 2733883.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 6ms/step - loss: 2733883.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2718458.0000\n",
      "Epoch 2: loss improved from 2733883.50000 to 2721411.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2721411.7500\n",
      "Epoch 3/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2702427.5000\n",
      "Epoch 3: loss improved from 2721411.75000 to 2703988.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2703988.5000\n",
      "Epoch 4/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2691040.5000\n",
      "Epoch 4: loss improved from 2703988.50000 to 2690043.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2690043.2500\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2678784.7500\n",
      "Epoch 5: loss improved from 2690043.25000 to 2678939.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2678939.0000\n",
      "Epoch 6/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2670106.7500\n",
      "Epoch 6: loss improved from 2678939.00000 to 2669967.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2669967.7500\n",
      "Epoch 7/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2659933.0000\n",
      "Epoch 7: loss improved from 2669967.75000 to 2662699.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2662699.2500\n",
      "Epoch 8/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2653547.5000\n",
      "Epoch 8: loss improved from 2662699.25000 to 2653711.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2653711.7500\n",
      "Epoch 9/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2645895.2500\n",
      "Epoch 9: loss improved from 2653711.75000 to 2645578.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2645578.0000\n",
      "Epoch 10/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2638647.7500\n",
      "Epoch 10: loss improved from 2645578.00000 to 2638712.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2638712.5000\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2629644.7500\n",
      "Epoch 11: loss improved from 2638712.50000 to 2632204.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2632204.2500\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2627085.7500\n",
      "Epoch 12: loss improved from 2632204.25000 to 2627085.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2627085.7500\n",
      "Epoch 13/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2621130.0000\n",
      "Epoch 13: loss improved from 2627085.75000 to 2621397.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2621397.0000\n",
      "Epoch 14/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2615215.7500\n",
      "Epoch 14: loss improved from 2621397.00000 to 2615710.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2615710.7500\n",
      "Epoch 15/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2612507.0000\n",
      "Epoch 15: loss improved from 2615710.75000 to 2612408.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2612408.0000\n",
      "Epoch 16/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2606623.5000\n",
      "Epoch 16: loss improved from 2612408.00000 to 2607137.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2607137.7500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2603102.5000\n",
      "Epoch 17: loss improved from 2607137.75000 to 2603088.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2603088.7500\n",
      "Epoch 18/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2599596.7500\n",
      "Epoch 18: loss improved from 2603088.75000 to 2599118.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2599118.0000\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2594606.5000\n",
      "Epoch 19: loss improved from 2599118.00000 to 2594182.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2594182.7500\n",
      "Epoch 20/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2592096.5000\n",
      "Epoch 20: loss improved from 2594182.75000 to 2590732.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2590732.5000\n",
      "Epoch 21/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2586288.2500\n",
      "Epoch 21: loss improved from 2590732.50000 to 2587975.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2587975.0000\n",
      "Epoch 22/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2584061.2500\n",
      "Epoch 22: loss improved from 2587975.00000 to 2583106.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2583106.2500\n",
      "Epoch 23/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2581107.5000\n",
      "Epoch 23: loss improved from 2583106.25000 to 2580109.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2580109.5000\n",
      "Epoch 24/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2576768.2500\n",
      "Epoch 24: loss improved from 2580109.50000 to 2576989.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2576989.7500\n",
      "Epoch 25/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2574576.5000\n",
      "Epoch 25: loss improved from 2576989.75000 to 2574791.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2574791.2500\n",
      "Epoch 26/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2571171.2500\n",
      "Epoch 26: loss improved from 2574791.25000 to 2570890.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2570890.7500\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2568013.2500\n",
      "Epoch 27: loss improved from 2570890.75000 to 2568075.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2568075.0000\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2565331.5000\n",
      "Epoch 28: loss improved from 2568075.00000 to 2565033.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2565033.0000\n",
      "Epoch 29/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2560741.5000\n",
      "Epoch 29: loss improved from 2565033.00000 to 2560528.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2560528.0000\n",
      "Epoch 30/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2558841.5000\n",
      "Epoch 30: loss improved from 2560528.00000 to 2558295.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2558295.0000\n",
      "Epoch 31/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2556092.2500\n",
      "Epoch 31: loss improved from 2558295.00000 to 2555525.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2555525.5000\n",
      "Epoch 32/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2553341.7500\n",
      "Epoch 32: loss improved from 2555525.50000 to 2552990.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2552990.2500\n",
      "Epoch 33/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2551571.7500\n",
      "Epoch 33: loss improved from 2552990.25000 to 2551989.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2551989.0000\n",
      "Epoch 34/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2548061.2500\n",
      "Epoch 34: loss improved from 2551989.00000 to 2548033.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2548033.0000\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2544845.5000\n",
      "Epoch 35: loss improved from 2548033.00000 to 2544845.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2544845.5000\n",
      "Epoch 36/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2542779.0000\n",
      "Epoch 36: loss improved from 2544845.50000 to 2542424.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542424.5000\n",
      "Epoch 37/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2539711.2500\n",
      "Epoch 37: loss improved from 2542424.50000 to 2540751.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2540751.0000\n",
      "Epoch 38/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537229.7500\n",
      "Epoch 38: loss improved from 2540751.00000 to 2538240.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2538240.0000\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2536433.2500\n",
      "Epoch 39: loss improved from 2538240.00000 to 2535541.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2535541.2500\n",
      "Epoch 40/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2534076.0000\n",
      "Epoch 40: loss improved from 2535541.25000 to 2533822.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2533822.7500\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2530220.7500\n",
      "Epoch 41: loss improved from 2533822.75000 to 2530220.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2530220.7500\n",
      "Epoch 42/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2529664.5000\n",
      "Epoch 42: loss improved from 2530220.75000 to 2528804.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2528804.7500\n",
      "Epoch 43/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2527783.0000\n",
      "Epoch 43: loss improved from 2528804.75000 to 2527009.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2527009.7500\n",
      "Epoch 44/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2527179.7500\n",
      "Epoch 44: loss improved from 2527009.75000 to 2524934.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2524934.2500\n",
      "Epoch 45/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2523134.5000\n",
      "Epoch 45: loss improved from 2524934.25000 to 2523228.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2523228.0000\n",
      "Epoch 46/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2521188.2500\n",
      "Epoch 46: loss improved from 2523228.00000 to 2521107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2521107.7500\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2519152.7500\n",
      "Epoch 47: loss improved from 2521107.75000 to 2518806.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2518806.2500\n",
      "Epoch 48/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2517286.0000\n",
      "Epoch 48: loss improved from 2518806.25000 to 2517286.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2517286.0000\n",
      "Epoch 49/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2513477.0000\n",
      "Epoch 49: loss improved from 2517286.00000 to 2514499.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2514499.5000\n",
      "Epoch 50/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2512294.5000\n",
      "Epoch 50: loss improved from 2514499.50000 to 2512939.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2512939.0000\n",
      "Epoch 51/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2512130.0000\n",
      "Epoch 51: loss improved from 2512939.00000 to 2511482.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2511482.0000\n",
      "Epoch 52/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2508756.5000\n",
      "Epoch 52: loss improved from 2511482.00000 to 2507589.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2507589.7500\n",
      "Epoch 53/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2507228.2500\n",
      "Epoch 53: loss improved from 2507589.75000 to 2507228.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2507228.2500\n",
      "Epoch 54/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2505727.5000\n",
      "Epoch 54: loss improved from 2507228.25000 to 2505081.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2505081.5000\n",
      "Epoch 55/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2503359.7500\n",
      "Epoch 55: loss improved from 2505081.50000 to 2502407.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2502407.0000\n",
      "Epoch 56/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2499507.2500\n",
      "Epoch 56: loss improved from 2502407.00000 to 2500879.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2500879.5000\n",
      "Epoch 57/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2500119.5000\n",
      "Epoch 57: loss improved from 2500879.50000 to 2498676.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2498676.0000\n",
      "Epoch 58/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2496416.7500\n",
      "Epoch 58: loss improved from 2498676.00000 to 2497208.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2497208.7500\n",
      "Epoch 59/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2495820.7500\n",
      "Epoch 59: loss improved from 2497208.75000 to 2495301.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2495301.7500\n",
      "Epoch 60/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2495025.0000\n",
      "Epoch 60: loss improved from 2495301.75000 to 2494357.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2494357.0000\n",
      "Epoch 61/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2492463.0000\n",
      "Epoch 61: loss improved from 2494357.00000 to 2491832.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2491832.2500\n",
      "Epoch 62/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2491554.2500\n",
      "Epoch 62: loss improved from 2491832.25000 to 2491810.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491810.0000\n",
      "Epoch 63/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2489817.7500\n",
      "Epoch 63: loss improved from 2491810.00000 to 2489483.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2489483.7500\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2489193.2500\n",
      "Epoch 64: loss improved from 2489483.75000 to 2487629.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2487629.7500\n",
      "Epoch 65/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2483124.0000\n",
      "Epoch 65: loss improved from 2487629.75000 to 2485384.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2485384.5000\n",
      "Epoch 66/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2484021.5000\n",
      "Epoch 66: loss improved from 2485384.50000 to 2484708.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2484708.7500\n",
      "Epoch 67/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2485066.2500\n",
      "Epoch 67: loss improved from 2484708.75000 to 2484281.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2484281.7500\n",
      "Epoch 68/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2479974.5000\n",
      "Epoch 68: loss improved from 2484281.75000 to 2480905.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2480905.0000\n",
      "Epoch 69/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2480214.7500\n",
      "Epoch 69: loss improved from 2480905.00000 to 2478861.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2478861.5000\n",
      "Epoch 70/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2477915.7500\n",
      "Epoch 70: loss improved from 2478861.50000 to 2477291.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2477291.5000\n",
      "Epoch 71/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2477812.2500\n",
      "Epoch 71: loss improved from 2477291.50000 to 2477082.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2477082.0000\n",
      "Epoch 72/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2475209.7500\n",
      "Epoch 72: loss improved from 2477082.00000 to 2475649.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2475649.7500\n",
      "Epoch 73/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2476496.5000\n",
      "Epoch 73: loss improved from 2475649.75000 to 2475299.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2475299.0000\n",
      "Epoch 74/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2474015.0000\n",
      "Epoch 74: loss improved from 2475299.00000 to 2473796.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2473796.2500\n",
      "Epoch 75/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2471990.7500\n",
      "Epoch 75: loss improved from 2473796.25000 to 2471075.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2471075.0000\n",
      "Epoch 76/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2469991.2500\n",
      "Epoch 76: loss improved from 2471075.00000 to 2469664.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2469664.0000\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2468972.0000\n",
      "Epoch 77: loss improved from 2469664.00000 to 2468519.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2468519.5000\n",
      "Epoch 78/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2466808.7500\n",
      "Epoch 78: loss improved from 2468519.50000 to 2466273.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2466273.5000\n",
      "Epoch 79/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2465803.7500\n",
      "Epoch 79: loss improved from 2466273.50000 to 2465803.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2465803.7500\n",
      "Epoch 80/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2464981.7500\n",
      "Epoch 80: loss improved from 2465803.75000 to 2464286.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2464286.2500\n",
      "Epoch 81/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2464632.0000\n",
      "Epoch 81: loss improved from 2464286.25000 to 2464259.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2464259.2500\n",
      "Epoch 82/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2462198.5000\n",
      "Epoch 82: loss improved from 2464259.25000 to 2462263.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2462263.0000\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2461205.2500\n",
      "Epoch 83: loss improved from 2462263.00000 to 2459888.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2459888.0000\n",
      "Epoch 84/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2461548.7500\n",
      "Epoch 84: loss did not improve from 2459888.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2460516.0000\n",
      "Epoch 85/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2457995.7500\n",
      "Epoch 85: loss improved from 2459888.00000 to 2457917.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2457917.7500\n",
      "Epoch 86/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2457840.2500\n",
      "Epoch 86: loss improved from 2457917.75000 to 2457649.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2457649.5000\n",
      "Epoch 87/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2455908.7500\n",
      "Epoch 87: loss improved from 2457649.50000 to 2456545.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2456545.5000\n",
      "Epoch 88/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2454998.5000\n",
      "Epoch 88: loss improved from 2456545.50000 to 2455124.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2455124.0000\n",
      "Epoch 89/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2455438.2500\n",
      "Epoch 89: loss did not improve from 2455124.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2455407.5000\n",
      "Epoch 90/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2453206.2500\n",
      "Epoch 90: loss improved from 2455124.00000 to 2453239.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2453239.7500\n",
      "Epoch 91/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2452841.0000\n",
      "Epoch 91: loss improved from 2453239.75000 to 2453141.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2453141.0000\n",
      "Epoch 92/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2451509.2500\n",
      "Epoch 92: loss improved from 2453141.00000 to 2451975.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2451975.2500\n",
      "Epoch 93/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2448795.5000\n",
      "Epoch 93: loss improved from 2451975.25000 to 2450111.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2450111.7500\n",
      "Epoch 94/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2449808.0000\n",
      "Epoch 94: loss improved from 2450111.75000 to 2449020.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2449020.5000\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2446569.2500\n",
      "Epoch 95: loss improved from 2449020.50000 to 2446569.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2446569.2500\n",
      "Epoch 96/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2447525.7500\n",
      "Epoch 96: loss did not improve from 2446569.25000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2447039.5000\n",
      "Epoch 97/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2445170.7500\n",
      "Epoch 97: loss improved from 2446569.25000 to 2445358.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2445358.7500\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2445295.7500\n",
      "Epoch 98: loss improved from 2445358.75000 to 2445295.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2445295.7500\n",
      "Epoch 99/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2443057.2500\n",
      "Epoch 99: loss improved from 2445295.75000 to 2443904.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2443904.0000\n",
      "Epoch 100/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2443061.7500\n",
      "Epoch 100: loss improved from 2443904.00000 to 2442846.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2442846.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 10:17:23 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 10:17:23 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpx0xvhsqe\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpx0xvhsqe\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_175 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_176 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_60 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 4:11 - loss: 2515843.5000 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0388s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0388s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188/3189 [============================>.] - ETA: 0s - loss: 2635109.2500\n",
      "Epoch 1: loss improved from inf to 2634729.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 38s 10ms/step - loss: 2634729.0000\n",
      "Epoch 2/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2442418.0000\n",
      "Epoch 2: loss improved from 2634729.00000 to 2442260.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2442260.7500\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2299516.5000\n",
      "Epoch 3: loss improved from 2442260.75000 to 2299239.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2299239.2500\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2185210.5000\n",
      "Epoch 4: loss improved from 2299239.25000 to 2185210.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2185210.5000\n",
      "Epoch 5/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2092447.0000\n",
      "Epoch 5: loss improved from 2185210.50000 to 2092579.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2092579.8750\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2020678.8750\n",
      "Epoch 6: loss improved from 2092579.87500 to 2020678.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2020678.8750\n",
      "Epoch 7/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1955592.7500\n",
      "Epoch 7: loss improved from 2020678.87500 to 1956911.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1956911.8750\n",
      "Epoch 8/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1905399.8750\n",
      "Epoch 8: loss improved from 1956911.87500 to 1905463.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1905463.7500\n",
      "Epoch 9/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1853760.0000\n",
      "Epoch 9: loss improved from 1905463.75000 to 1853273.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1853273.2500\n",
      "Epoch 10/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1806829.6250\n",
      "Epoch 10: loss improved from 1853273.25000 to 1805944.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1805944.0000\n",
      "Epoch 11/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1766627.8750\n",
      "Epoch 11: loss improved from 1805944.00000 to 1765768.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1765768.2500\n",
      "Epoch 12/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1726744.1250\n",
      "Epoch 12: loss improved from 1765768.25000 to 1725306.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1725306.3750\n",
      "Epoch 13/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1688739.1250\n",
      "Epoch 13: loss improved from 1725306.37500 to 1688610.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1688610.6250\n",
      "Epoch 14/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1651921.5000\n",
      "Epoch 14: loss improved from 1688610.62500 to 1652331.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1652331.0000\n",
      "Epoch 15/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1624204.5000\n",
      "Epoch 15: loss improved from 1652331.00000 to 1623846.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1623846.5000\n",
      "Epoch 16/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1593998.5000\n",
      "Epoch 16: loss improved from 1623846.50000 to 1594102.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1594102.0000\n",
      "Epoch 17/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1562571.5000\n",
      "Epoch 17: loss improved from 1594102.00000 to 1563032.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1563032.2500\n",
      "Epoch 18/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1539520.0000\n",
      "Epoch 18: loss improved from 1563032.25000 to 1539889.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1539889.3750\n",
      "Epoch 19/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1519099.3750\n",
      "Epoch 19: loss improved from 1539889.37500 to 1519283.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1519283.7500\n",
      "Epoch 20/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1498105.5000\n",
      "Epoch 20: loss improved from 1519283.75000 to 1497409.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1497409.0000\n",
      "Epoch 21/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1478258.6250\n",
      "Epoch 21: loss improved from 1497409.00000 to 1478770.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1478770.3750\n",
      "Epoch 22/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1459514.7500\n",
      "Epoch 22: loss improved from 1478770.37500 to 1460446.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 1460446.2500\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1443645.1250\n",
      "Epoch 23: loss improved from 1460446.25000 to 1443070.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1443070.1250\n",
      "Epoch 24/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1431084.2500\n",
      "Epoch 24: loss improved from 1443070.12500 to 1431672.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1431672.6250\n",
      "Epoch 25/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1417895.1250\n",
      "Epoch 25: loss improved from 1431672.62500 to 1417581.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1417581.1250\n",
      "Epoch 26/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1405108.1250\n",
      "Epoch 26: loss improved from 1417581.12500 to 1405030.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1405030.7500\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1392282.2500\n",
      "Epoch 27: loss improved from 1405030.75000 to 1392259.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1392259.1250\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1379666.5000\n",
      "Epoch 28: loss improved from 1392259.12500 to 1379404.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1379404.2500\n",
      "Epoch 29/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1370843.6250\n",
      "Epoch 29: loss improved from 1379404.25000 to 1370943.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1370943.8750\n",
      "Epoch 30/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1359399.1250\n",
      "Epoch 30: loss improved from 1370943.87500 to 1359670.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1359670.3750\n",
      "Epoch 31/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1348841.2500\n",
      "Epoch 31: loss improved from 1359670.37500 to 1348507.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1348507.0000\n",
      "Epoch 32/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1340114.1250\n",
      "Epoch 32: loss improved from 1348507.00000 to 1339835.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1339835.8750\n",
      "Epoch 33/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1333588.0000\n",
      "Epoch 33: loss improved from 1339835.87500 to 1332937.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1332937.1250\n",
      "Epoch 34/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1322555.2500\n",
      "Epoch 34: loss improved from 1332937.12500 to 1322510.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1322510.1250\n",
      "Epoch 35/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1317515.3750\n",
      "Epoch 35: loss improved from 1322510.12500 to 1317350.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1317350.8750\n",
      "Epoch 36/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1308369.3750\n",
      "Epoch 36: loss improved from 1317350.87500 to 1308686.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1308686.8750\n",
      "Epoch 37/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1302400.3750\n",
      "Epoch 37: loss improved from 1308686.87500 to 1301961.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1301961.3750\n",
      "Epoch 38/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1296633.6250\n",
      "Epoch 38: loss improved from 1301961.37500 to 1296671.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1296671.7500\n",
      "Epoch 39/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1290871.1250\n",
      "Epoch 39: loss improved from 1296671.75000 to 1290632.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1290632.8750\n",
      "Epoch 40/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1284409.2500\n",
      "Epoch 40: loss improved from 1290632.87500 to 1284215.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1284215.5000\n",
      "Epoch 41/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1277005.6250\n",
      "Epoch 41: loss improved from 1284215.50000 to 1277525.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1277525.1250\n",
      "Epoch 42/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1269750.8750\n",
      "Epoch 42: loss improved from 1277525.12500 to 1269461.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1269461.1250\n",
      "Epoch 43/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1265444.3750\n",
      "Epoch 43: loss improved from 1269461.12500 to 1265397.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1265397.0000\n",
      "Epoch 44/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1255752.3750\n",
      "Epoch 44: loss improved from 1265397.00000 to 1257000.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1257000.5000\n",
      "Epoch 45/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1255348.0000\n",
      "Epoch 45: loss improved from 1257000.50000 to 1254927.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1254927.0000\n",
      "Epoch 46/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1245233.2500\n",
      "Epoch 46: loss improved from 1254927.00000 to 1245784.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1245784.0000\n",
      "Epoch 47/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1242054.6250\n",
      "Epoch 47: loss improved from 1245784.00000 to 1241351.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1241351.6250\n",
      "Epoch 48/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1235340.6250\n",
      "Epoch 48: loss improved from 1241351.62500 to 1235419.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1235419.6250\n",
      "Epoch 49/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1232081.1250\n",
      "Epoch 49: loss improved from 1235419.62500 to 1231894.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1231894.7500\n",
      "Epoch 50/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1224178.1250\n",
      "Epoch 50: loss improved from 1231894.75000 to 1226333.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1226333.8750\n",
      "Epoch 51/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1219966.0000\n",
      "Epoch 51: loss improved from 1226333.87500 to 1220191.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1220191.1250\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1218849.2500\n",
      "Epoch 52: loss improved from 1220191.12500 to 1218531.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1218531.6250\n",
      "Epoch 53/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1213663.7500\n",
      "Epoch 53: loss improved from 1218531.62500 to 1213690.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1213690.5000\n",
      "Epoch 54/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1210905.5000\n",
      "Epoch 54: loss improved from 1213690.50000 to 1210725.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1210725.8750\n",
      "Epoch 55/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1205464.0000\n",
      "Epoch 55: loss improved from 1210725.87500 to 1205629.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1205629.3750\n",
      "Epoch 56/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1200941.0000\n",
      "Epoch 56: loss improved from 1205629.37500 to 1200874.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1200874.1250\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1198491.2500\n",
      "Epoch 57: loss improved from 1200874.12500 to 1198301.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1198301.6250\n",
      "Epoch 58/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1192054.3750\n",
      "Epoch 58: loss improved from 1198301.62500 to 1192072.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1192072.3750\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1190817.7500\n",
      "Epoch 59: loss improved from 1192072.37500 to 1191195.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1191195.1250\n",
      "Epoch 60/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1185598.7500\n",
      "Epoch 60: loss improved from 1191195.12500 to 1185135.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1185135.8750\n",
      "Epoch 61/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1182854.6250\n",
      "Epoch 61: loss improved from 1185135.87500 to 1183299.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1183299.7500\n",
      "Epoch 62/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1178446.1250\n",
      "Epoch 62: loss improved from 1183299.75000 to 1178372.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1178372.8750\n",
      "Epoch 63/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1176351.5000\n",
      "Epoch 63: loss improved from 1178372.87500 to 1176547.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1176547.2500\n",
      "Epoch 64/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1171627.0000\n",
      "Epoch 64: loss improved from 1176547.25000 to 1172020.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1172020.8750\n",
      "Epoch 65/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1166995.7500\n",
      "Epoch 65: loss improved from 1172020.87500 to 1168657.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1168657.3750\n",
      "Epoch 66/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1165756.1250\n",
      "Epoch 66: loss improved from 1168657.37500 to 1165708.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1165708.1250\n",
      "Epoch 67/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1161927.3750\n",
      "Epoch 67: loss improved from 1165708.12500 to 1163052.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1163052.3750\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1160909.2500\n",
      "Epoch 68: loss improved from 1163052.37500 to 1160909.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1160909.2500\n",
      "Epoch 69/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1155681.6250\n",
      "Epoch 69: loss improved from 1160909.25000 to 1155681.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1155681.6250\n",
      "Epoch 70/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1154227.5000\n",
      "Epoch 70: loss improved from 1155681.62500 to 1153969.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1153969.7500\n",
      "Epoch 71/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1149364.2500\n",
      "Epoch 71: loss improved from 1153969.75000 to 1149751.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1149751.0000\n",
      "Epoch 72/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1145721.2500\n",
      "Epoch 72: loss improved from 1149751.00000 to 1147090.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1147090.3750\n",
      "Epoch 73/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1145323.3750\n",
      "Epoch 73: loss improved from 1147090.37500 to 1145354.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1145354.1250\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1143328.1250\n",
      "Epoch 74: loss improved from 1145354.12500 to 1142961.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1142961.8750\n",
      "Epoch 75/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1141862.1250\n",
      "Epoch 75: loss improved from 1142961.87500 to 1141439.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1141439.2500\n",
      "Epoch 76/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1137488.7500\n",
      "Epoch 76: loss improved from 1141439.25000 to 1137601.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1137601.7500\n",
      "Epoch 77/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1135145.0000\n",
      "Epoch 77: loss improved from 1137601.75000 to 1135079.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1135079.3750\n",
      "Epoch 78/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1130282.3750\n",
      "Epoch 78: loss improved from 1135079.37500 to 1130469.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1130469.2500\n",
      "Epoch 79/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1129186.3750\n",
      "Epoch 79: loss improved from 1130469.25000 to 1130142.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1130142.6250\n",
      "Epoch 80/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1128164.8750\n",
      "Epoch 80: loss improved from 1130142.62500 to 1127920.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1127920.8750\n",
      "Epoch 81/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1124261.1250\n",
      "Epoch 81: loss improved from 1127920.87500 to 1125452.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1125452.1250\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1123155.0000\n",
      "Epoch 82: loss improved from 1125452.12500 to 1123263.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1123263.3750\n",
      "Epoch 83/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1122599.5000\n",
      "Epoch 83: loss improved from 1123263.37500 to 1122721.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1122721.2500\n",
      "Epoch 84/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1119254.8750\n",
      "Epoch 84: loss improved from 1122721.25000 to 1119703.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1119703.5000\n",
      "Epoch 85/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1115245.7500\n",
      "Epoch 85: loss improved from 1119703.50000 to 1115658.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1115658.0000\n",
      "Epoch 86/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1115621.7500\n",
      "Epoch 86: loss improved from 1115658.00000 to 1115555.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1115555.5000\n",
      "Epoch 87/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1111681.1250\n",
      "Epoch 87: loss improved from 1115555.50000 to 1111752.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1111752.6250\n",
      "Epoch 88/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1109154.7500\n",
      "Epoch 88: loss improved from 1111752.62500 to 1108917.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1108917.7500\n",
      "Epoch 89/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1108111.3750\n",
      "Epoch 89: loss improved from 1108917.75000 to 1107320.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1107320.2500\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1106002.3750\n",
      "Epoch 90: loss improved from 1107320.25000 to 1106312.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1106312.6250\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1104422.6250\n",
      "Epoch 91: loss improved from 1106312.62500 to 1103952.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1103952.0000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1102547.3750\n",
      "Epoch 92: loss improved from 1103952.00000 to 1102248.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1102248.7500\n",
      "Epoch 93/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1103449.8750\n",
      "Epoch 93: loss did not improve from 1102248.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1103395.0000\n",
      "Epoch 94/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1100058.2500\n",
      "Epoch 94: loss improved from 1102248.75000 to 1099525.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1099525.2500\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1095799.8750\n",
      "Epoch 95: loss improved from 1099525.25000 to 1095642.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1095642.5000\n",
      "Epoch 96/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1093604.6250\n",
      "Epoch 96: loss improved from 1095642.50000 to 1093732.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 1093732.2500\n",
      "Epoch 97/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1094167.2500\n",
      "Epoch 97: loss did not improve from 1093732.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1094309.0000\n",
      "Epoch 98/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1091117.1250\n",
      "Epoch 98: loss improved from 1093732.25000 to 1090864.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1090864.6250\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1089336.3750\n",
      "Epoch 99: loss improved from 1090864.62500 to 1089336.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1089336.3750\n",
      "Epoch 100/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1088231.5000\n",
      "Epoch 100: loss improved from 1089336.37500 to 1088231.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1088231.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 10:41:33 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 10:41:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmptr9upp74\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmptr9upp74\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_179 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_61 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_180 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_62 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_181 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_63 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   3/3189 [..............................] - ETA: 5:50 - loss: 6657601.5000  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0649s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0649s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3181/3189 [============================>.] - ETA: 0s - loss: 2742201.0000\n",
      "Epoch 1: loss improved from inf to 2742196.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 26s 6ms/step - loss: 2742196.5000\n",
      "Epoch 2/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2734212.5000\n",
      "Epoch 2: loss improved from 2742196.50000 to 2733304.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2733304.2500\n",
      "Epoch 3/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2726029.7500\n",
      "Epoch 3: loss improved from 2733304.25000 to 2724733.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2724733.5000\n",
      "Epoch 4/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2713569.7500\n",
      "Epoch 4: loss improved from 2724733.50000 to 2713321.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2713321.5000\n",
      "Epoch 5/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2700022.0000\n",
      "Epoch 5: loss improved from 2713321.50000 to 2700624.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2700624.5000\n",
      "Epoch 6/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2690975.7500\n",
      "Epoch 6: loss improved from 2700624.50000 to 2690050.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2690050.2500\n",
      "Epoch 7/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2679574.2500\n",
      "Epoch 7: loss improved from 2690050.25000 to 2681922.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2681922.2500\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2675977.7500\n",
      "Epoch 8: loss improved from 2681922.25000 to 2674465.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2674465.5000\n",
      "Epoch 9/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2666715.2500\n",
      "Epoch 9: loss improved from 2674465.50000 to 2666684.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2666684.2500\n",
      "Epoch 10/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2658008.5000\n",
      "Epoch 10: loss improved from 2666684.25000 to 2658974.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2658974.5000\n",
      "Epoch 11/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2650403.2500\n",
      "Epoch 11: loss improved from 2658974.50000 to 2650575.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2650575.5000\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2643477.7500\n",
      "Epoch 12: loss improved from 2650575.50000 to 2643477.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2643477.7500\n",
      "Epoch 13/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2634687.2500\n",
      "Epoch 13: loss improved from 2643477.75000 to 2633138.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2633138.5000\n",
      "Epoch 14/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2629574.7500\n",
      "Epoch 14: loss improved from 2633138.50000 to 2628863.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2628863.5000\n",
      "Epoch 15/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2621799.2500\n",
      "Epoch 15: loss improved from 2628863.50000 to 2621724.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2621724.0000\n",
      "Epoch 16/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2614603.5000\n",
      "Epoch 16: loss improved from 2621724.00000 to 2612409.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2612409.7500\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2606193.5000\n",
      "Epoch 17: loss improved from 2612409.75000 to 2606176.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2606176.7500\n",
      "Epoch 18/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2600611.0000\n",
      "Epoch 18: loss improved from 2606176.75000 to 2599502.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2599502.0000\n",
      "Epoch 19/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2593661.0000\n",
      "Epoch 19: loss improved from 2599502.00000 to 2593700.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2593700.5000\n",
      "Epoch 20/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2586782.5000\n",
      "Epoch 20: loss improved from 2593700.50000 to 2587626.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2587626.7500\n",
      "Epoch 21/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2583200.7500\n",
      "Epoch 21: loss improved from 2587626.75000 to 2582357.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2582357.7500\n",
      "Epoch 22/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2576315.5000\n",
      "Epoch 22: loss improved from 2582357.75000 to 2576360.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2576360.5000\n",
      "Epoch 23/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2569040.2500\n",
      "Epoch 23: loss improved from 2576360.50000 to 2570783.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2570783.0000\n",
      "Epoch 24/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2567529.2500\n",
      "Epoch 24: loss improved from 2570783.00000 to 2566497.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2566497.0000\n",
      "Epoch 25/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2561988.5000\n",
      "Epoch 25: loss improved from 2566497.00000 to 2562064.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2562064.5000\n",
      "Epoch 26/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2557140.5000\n",
      "Epoch 26: loss improved from 2562064.50000 to 2559055.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2559055.0000\n",
      "Epoch 27/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2554838.5000\n",
      "Epoch 27: loss improved from 2559055.00000 to 2554782.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2554782.7500\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2549291.2500\n",
      "Epoch 28: loss improved from 2554782.75000 to 2549879.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2549879.5000\n",
      "Epoch 29/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2546827.7500\n",
      "Epoch 29: loss improved from 2549879.50000 to 2547008.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2547008.0000\n",
      "Epoch 30/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2543550.0000\n",
      "Epoch 30: loss improved from 2547008.00000 to 2542412.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542412.7500\n",
      "Epoch 31/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2540931.0000\n",
      "Epoch 31: loss improved from 2542412.75000 to 2540764.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540764.5000\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2540063.2500\n",
      "Epoch 32: loss improved from 2540764.50000 to 2540063.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540063.2500\n",
      "Epoch 33/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2533774.2500\n",
      "Epoch 33: loss improved from 2540063.25000 to 2534275.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534275.0000\n",
      "Epoch 34/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2532512.2500\n",
      "Epoch 34: loss improved from 2534275.00000 to 2532891.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532891.5000\n",
      "Epoch 35/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2532054.2500\n",
      "Epoch 35: loss improved from 2532891.50000 to 2530622.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530622.2500\n",
      "Epoch 36/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2525722.5000\n",
      "Epoch 36: loss improved from 2530622.25000 to 2527727.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2527727.2500\n",
      "Epoch 37/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2524289.2500\n",
      "Epoch 37: loss improved from 2527727.25000 to 2525044.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525044.0000\n",
      "Epoch 38/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2522243.0000\n",
      "Epoch 38: loss improved from 2525044.00000 to 2522243.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2522243.0000\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2519845.0000\n",
      "Epoch 39: loss improved from 2522243.00000 to 2520224.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2520224.5000\n",
      "Epoch 40/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2520726.5000\n",
      "Epoch 40: loss improved from 2520224.50000 to 2518946.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2518946.5000\n",
      "Epoch 41/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2515908.5000\n",
      "Epoch 41: loss improved from 2518946.50000 to 2514761.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514761.7500\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2513596.2500\n",
      "Epoch 42: loss improved from 2514761.75000 to 2514240.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514240.5000\n",
      "Epoch 43/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2514244.2500\n",
      "Epoch 43: loss improved from 2514240.50000 to 2514129.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514129.0000\n",
      "Epoch 44/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2510652.2500\n",
      "Epoch 44: loss improved from 2514129.00000 to 2510416.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2510416.5000\n",
      "Epoch 45/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2509930.5000\n",
      "Epoch 45: loss improved from 2510416.50000 to 2508946.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2508946.5000\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2510771.2500\n",
      "Epoch 46: loss did not improve from 2508946.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2509188.5000\n",
      "Epoch 47/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2507281.5000\n",
      "Epoch 47: loss improved from 2508946.50000 to 2507176.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2507176.7500\n",
      "Epoch 48/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2506035.0000\n",
      "Epoch 48: loss improved from 2507176.75000 to 2505541.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2505541.2500\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2503869.5000\n",
      "Epoch 49: loss improved from 2505541.25000 to 2503869.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2503869.5000\n",
      "Epoch 50/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2504740.7500\n",
      "Epoch 50: loss improved from 2503869.50000 to 2502921.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2502921.5000\n",
      "Epoch 51/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2500779.5000\n",
      "Epoch 51: loss improved from 2502921.50000 to 2501698.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2501698.2500\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2502441.2500\n",
      "Epoch 52: loss improved from 2501698.25000 to 2501649.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2501649.5000\n",
      "Epoch 53/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2500090.7500\n",
      "Epoch 53: loss improved from 2501649.50000 to 2499299.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2499299.5000\n",
      "Epoch 54/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2498549.7500\n",
      "Epoch 54: loss improved from 2499299.50000 to 2498981.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2498981.7500\n",
      "Epoch 55/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2497326.2500\n",
      "Epoch 55: loss improved from 2498981.75000 to 2496508.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2496508.2500\n",
      "Epoch 56/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2493502.2500\n",
      "Epoch 56: loss improved from 2496508.25000 to 2495921.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2495921.5000\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2493861.5000\n",
      "Epoch 57: loss improved from 2495921.50000 to 2494021.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2494021.7500\n",
      "Epoch 58/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2495178.5000\n",
      "Epoch 58: loss did not improve from 2494021.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2494811.5000\n",
      "Epoch 59/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2491371.5000\n",
      "Epoch 59: loss improved from 2494021.75000 to 2491653.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491653.7500\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2492293.5000\n",
      "Epoch 60: loss did not improve from 2491653.75000\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2492629.7500\n",
      "Epoch 61/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2488832.2500\n",
      "Epoch 61: loss improved from 2491653.75000 to 2488837.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 70s 22ms/step - loss: 2488837.0000\n",
      "Epoch 62/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2488990.7500\n",
      "Epoch 62: loss did not improve from 2488837.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2489000.5000\n",
      "Epoch 63/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2487317.5000\n",
      "Epoch 63: loss improved from 2488837.00000 to 2488430.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2488430.0000\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2488220.2500\n",
      "Epoch 64: loss improved from 2488430.00000 to 2488013.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2488013.7500\n",
      "Epoch 65/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2486430.5000\n",
      "Epoch 65: loss improved from 2488013.75000 to 2486480.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2486480.7500\n",
      "Epoch 66/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2485677.2500\n",
      "Epoch 66: loss improved from 2486480.75000 to 2485611.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2485611.7500\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2483597.5000\n",
      "Epoch 67: loss improved from 2485611.75000 to 2483546.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2483546.5000\n",
      "Epoch 68/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2483147.2500\n",
      "Epoch 68: loss did not improve from 2483546.50000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2483632.0000\n",
      "Epoch 69/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2483393.7500\n",
      "Epoch 69: loss improved from 2483546.50000 to 2483393.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2483393.0000\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2484713.5000\n",
      "Epoch 70: loss did not improve from 2483393.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2484713.5000\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2481088.5000\n",
      "Epoch 71: loss improved from 2483393.00000 to 2480123.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2480123.5000\n",
      "Epoch 72/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2482293.7500\n",
      "Epoch 72: loss did not improve from 2480123.50000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2481663.5000\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2479156.0000\n",
      "Epoch 73: loss improved from 2480123.50000 to 2479156.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2479156.0000\n",
      "Epoch 74/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2479876.0000\n",
      "Epoch 74: loss improved from 2479156.00000 to 2478773.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2478773.2500\n",
      "Epoch 75/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2480046.5000\n",
      "Epoch 75: loss improved from 2478773.25000 to 2478544.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2478544.2500\n",
      "Epoch 76/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2474579.2500\n",
      "Epoch 76: loss improved from 2478544.25000 to 2474722.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2474722.7500\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2476292.5000\n",
      "Epoch 77: loss did not improve from 2474722.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2476292.5000\n",
      "Epoch 78/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2477938.7500\n",
      "Epoch 78: loss did not improve from 2474722.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2476745.7500\n",
      "Epoch 79/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2475416.0000\n",
      "Epoch 79: loss did not improve from 2474722.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2475073.2500\n",
      "Epoch 80/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2473880.0000\n",
      "Epoch 80: loss improved from 2474722.75000 to 2473574.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2473574.5000\n",
      "Epoch 81/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2472788.0000\n",
      "Epoch 81: loss improved from 2473574.50000 to 2472580.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2472580.7500\n",
      "Epoch 82/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2472239.2500\n",
      "Epoch 82: loss improved from 2472580.75000 to 2471645.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2471645.5000\n",
      "Epoch 83/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2469647.7500\n",
      "Epoch 83: loss improved from 2471645.50000 to 2469378.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2469378.2500\n",
      "Epoch 84/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2472392.2500\n",
      "Epoch 84: loss did not improve from 2469378.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2472401.0000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2468860.2500\n",
      "Epoch 85: loss did not improve from 2469378.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2470334.5000\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2471238.2500\n",
      "Epoch 86: loss did not improve from 2469378.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2470849.7500\n",
      "Epoch 87/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2468701.0000\n",
      "Epoch 87: loss improved from 2469378.25000 to 2468210.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2468210.0000\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2466444.5000\n",
      "Epoch 88: loss improved from 2468210.00000 to 2467046.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2467046.5000\n",
      "Epoch 89/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2467282.0000\n",
      "Epoch 89: loss improved from 2467046.50000 to 2466781.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2466781.0000\n",
      "Epoch 90/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2465822.5000\n",
      "Epoch 90: loss did not improve from 2466781.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2467279.7500\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2467435.7500\n",
      "Epoch 91: loss improved from 2466781.00000 to 2466474.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2466474.5000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2464480.0000\n",
      "Epoch 92: loss improved from 2466474.50000 to 2464846.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2464846.5000\n",
      "Epoch 93/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2463978.2500\n",
      "Epoch 93: loss improved from 2464846.50000 to 2463111.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2463111.2500\n",
      "Epoch 94/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2462192.7500\n",
      "Epoch 94: loss improved from 2463111.25000 to 2462667.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2462667.2500\n",
      "Epoch 95/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2459363.5000\n",
      "Epoch 95: loss improved from 2462667.25000 to 2460749.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2460749.5000\n",
      "Epoch 96/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2461676.5000\n",
      "Epoch 96: loss did not improve from 2460749.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2462451.2500\n",
      "Epoch 97/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2462221.0000\n",
      "Epoch 97: loss did not improve from 2460749.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2461937.0000\n",
      "Epoch 98/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2461474.2500\n",
      "Epoch 98: loss did not improve from 2460749.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2462265.5000\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2461500.5000\n",
      "Epoch 99: loss did not improve from 2460749.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2461259.2500\n",
      "Epoch 100/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2458583.5000\n",
      "Epoch 100: loss improved from 2460749.50000 to 2459603.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2459603.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 11:04:28 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 11:04:28 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp092qqeyg\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp092qqeyg\\model\\data\\model\\assets\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "MODELS=[\"dnn3\",\"dnn4\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=3\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\"vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacked models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 11:51:41 INFO mlflow.tracking.fluent: Experiment with name 'dnn4_stacked2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_183 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_64 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_65 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_185 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_186 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_67 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,019,000\n",
      "Trainable params: 42,019,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2695908.2500\n",
      "Epoch 1: loss improved from inf to 2695908.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 241s 47ms/step - loss: 2695908.2500\n",
      "Epoch 2/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2564129.0000\n",
      "Epoch 2: loss improved from 2695908.25000 to 2563457.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 27s 9ms/step - loss: 2563457.7500\n",
      "Epoch 3/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2442386.7500\n",
      "Epoch 3: loss improved from 2563457.75000 to 2441821.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2441821.2500\n",
      "Epoch 4/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2353157.5000\n",
      "Epoch 4: loss improved from 2441821.25000 to 2352661.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2352661.5000\n",
      "Epoch 5/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2276082.2500\n",
      "Epoch 5: loss improved from 2352661.50000 to 2276490.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2276490.5000\n",
      "Epoch 6/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2213613.0000\n",
      "Epoch 6: loss improved from 2276490.50000 to 2213955.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2213955.7500\n",
      "Epoch 7/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2155751.0000\n",
      "Epoch 7: loss improved from 2213955.75000 to 2157890.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2157890.7500\n",
      "Epoch 8/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2110838.2500\n",
      "Epoch 8: loss improved from 2157890.75000 to 2111316.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2111316.5000\n",
      "Epoch 9/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2070104.8750\n",
      "Epoch 9: loss improved from 2111316.50000 to 2070131.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2070131.1250\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2035317.0000\n",
      "Epoch 10: loss improved from 2070131.12500 to 2034482.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2034482.2500\n",
      "Epoch 11/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2001981.1250\n",
      "Epoch 11: loss improved from 2034482.25000 to 2000821.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2000821.8750\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1970421.3750\n",
      "Epoch 12: loss improved from 2000821.87500 to 1970279.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 1970279.6250\n",
      "Epoch 13/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1943153.2500\n",
      "Epoch 13: loss improved from 1970279.62500 to 1944494.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 1944494.5000\n",
      "Epoch 14/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1917307.2500\n",
      "Epoch 14: loss improved from 1944494.50000 to 1917307.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 1917307.2500\n",
      "Epoch 15/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1893951.8750\n",
      "Epoch 15: loss improved from 1917307.25000 to 1893654.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1893654.5000\n",
      "Epoch 16/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1870771.0000\n",
      "Epoch 16: loss improved from 1893654.50000 to 1870745.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 1870745.7500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1848495.2500\n",
      "Epoch 17: loss improved from 1870745.75000 to 1848726.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1848726.3750\n",
      "Epoch 18/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1825666.7500\n",
      "Epoch 18: loss improved from 1848726.37500 to 1825666.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 1825666.7500\n",
      "Epoch 19/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1807303.6250\n",
      "Epoch 19: loss improved from 1825666.75000 to 1806843.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 1806843.3750\n",
      "Epoch 20/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1789153.8750\n",
      "Epoch 20: loss improved from 1806843.37500 to 1788514.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 1788514.7500\n",
      "Epoch 21/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1770197.8750\n",
      "Epoch 21: loss improved from 1788514.75000 to 1769903.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1769903.5000\n",
      "Epoch 22/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1750666.3750\n",
      "Epoch 22: loss improved from 1769903.50000 to 1750666.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1750666.3750\n",
      "Epoch 23/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1733776.1250\n",
      "Epoch 23: loss improved from 1750666.37500 to 1733776.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1733776.1250\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1719277.3750\n",
      "Epoch 24: loss improved from 1733776.12500 to 1718607.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1718607.0000\n",
      "Epoch 25/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1701899.6250\n",
      "Epoch 25: loss improved from 1718607.00000 to 1701706.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1701706.6250\n",
      "Epoch 26/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1685710.8750\n",
      "Epoch 26: loss improved from 1701706.62500 to 1686436.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1686436.3750\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1672720.7500\n",
      "Epoch 27: loss improved from 1686436.37500 to 1672720.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1672720.7500\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1658046.5000\n",
      "Epoch 28: loss improved from 1672720.75000 to 1657824.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1657824.6250\n",
      "Epoch 29/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1645875.0000\n",
      "Epoch 29: loss improved from 1657824.62500 to 1645631.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1645631.5000\n",
      "Epoch 30/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1633411.2500\n",
      "Epoch 30: loss improved from 1645631.50000 to 1632943.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1632943.3750\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1620917.0000\n",
      "Epoch 31: loss improved from 1632943.37500 to 1621426.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1621426.8750\n",
      "Epoch 32/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1607996.0000\n",
      "Epoch 32: loss improved from 1621426.87500 to 1609347.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1609347.0000\n",
      "Epoch 33/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1596605.0000\n",
      "Epoch 33: loss improved from 1609347.00000 to 1596924.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1596924.1250\n",
      "Epoch 34/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1586075.3750\n",
      "Epoch 34: loss improved from 1596924.12500 to 1586615.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1586615.7500\n",
      "Epoch 35/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1576515.2500\n",
      "Epoch 35: loss improved from 1586615.75000 to 1577011.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1577011.5000\n",
      "Epoch 36/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1566023.0000\n",
      "Epoch 36: loss improved from 1577011.50000 to 1566155.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1566155.0000\n",
      "Epoch 37/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1556008.8750\n",
      "Epoch 37: loss improved from 1566155.00000 to 1555976.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1555976.1250\n",
      "Epoch 38/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1546220.6250\n",
      "Epoch 38: loss improved from 1555976.12500 to 1546433.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1546433.1250\n",
      "Epoch 39/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1538218.3750\n",
      "Epoch 39: loss improved from 1546433.12500 to 1538082.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1538082.7500\n",
      "Epoch 40/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1529200.3750\n",
      "Epoch 40: loss improved from 1538082.75000 to 1528572.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1528572.7500\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1521607.1250\n",
      "Epoch 41: loss improved from 1528572.75000 to 1521607.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1521607.1250\n",
      "Epoch 42/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1512864.6250\n",
      "Epoch 42: loss improved from 1521607.12500 to 1512914.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1512914.2500\n",
      "Epoch 43/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1504102.8750\n",
      "Epoch 43: loss improved from 1512914.25000 to 1504160.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1504160.7500\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1497370.3750\n",
      "Epoch 44: loss improved from 1504160.75000 to 1497059.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1497059.1250\n",
      "Epoch 45/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1489554.2500\n",
      "Epoch 45: loss improved from 1497059.12500 to 1489183.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1489183.0000\n",
      "Epoch 46/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1485037.3750\n",
      "Epoch 46: loss improved from 1489183.00000 to 1484769.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1484769.1250\n",
      "Epoch 47/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1475516.0000\n",
      "Epoch 47: loss improved from 1484769.12500 to 1475412.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1475412.5000\n",
      "Epoch 48/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1466944.0000\n",
      "Epoch 48: loss improved from 1475412.50000 to 1466644.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1466644.8750\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1463265.5000\n",
      "Epoch 49: loss improved from 1466644.87500 to 1463265.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1463265.5000\n",
      "Epoch 50/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1452890.6250\n",
      "Epoch 50: loss improved from 1463265.50000 to 1452796.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1452796.1250\n",
      "Epoch 51/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1447425.5000\n",
      "Epoch 51: loss improved from 1452796.12500 to 1447324.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1447324.7500\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1444023.7500\n",
      "Epoch 52: loss improved from 1447324.75000 to 1443099.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1443099.8750\n",
      "Epoch 53/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1434903.3750\n",
      "Epoch 53: loss improved from 1443099.87500 to 1434863.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1434863.5000\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1430617.2500\n",
      "Epoch 54: loss improved from 1434863.50000 to 1430812.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1430812.3750\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1426070.3750\n",
      "Epoch 55: loss improved from 1430812.37500 to 1425985.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1425985.8750\n",
      "Epoch 56/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1418311.0000\n",
      "Epoch 56: loss improved from 1425985.87500 to 1418551.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1418551.2500\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1413482.8750\n",
      "Epoch 57: loss improved from 1418551.25000 to 1413482.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1413482.8750\n",
      "Epoch 58/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1407223.1250\n",
      "Epoch 58: loss improved from 1413482.87500 to 1406705.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1406705.3750\n",
      "Epoch 59/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1404036.2500\n",
      "Epoch 59: loss improved from 1406705.37500 to 1404036.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1404036.2500\n",
      "Epoch 60/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1398098.6250\n",
      "Epoch 60: loss improved from 1404036.25000 to 1398017.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1398017.8750\n",
      "Epoch 61/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1394879.7500\n",
      "Epoch 61: loss improved from 1398017.87500 to 1394842.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1394842.2500\n",
      "Epoch 62/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1387464.6250\n",
      "Epoch 62: loss improved from 1394842.25000 to 1387358.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1387358.1250\n",
      "Epoch 63/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1383036.3750\n",
      "Epoch 63: loss improved from 1387358.12500 to 1383149.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1383149.0000\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1380367.2500\n",
      "Epoch 64: loss improved from 1383149.00000 to 1380789.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1380789.5000\n",
      "Epoch 65/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1374321.0000\n",
      "Epoch 65: loss improved from 1380789.50000 to 1374661.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1374661.2500\n",
      "Epoch 66/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1371739.7500\n",
      "Epoch 66: loss improved from 1374661.25000 to 1371831.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1371831.0000\n",
      "Epoch 67/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1369666.5000\n",
      "Epoch 67: loss improved from 1371831.00000 to 1369473.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1369473.0000\n",
      "Epoch 68/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1365390.0000\n",
      "Epoch 68: loss improved from 1369473.00000 to 1365346.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1365346.0000\n",
      "Epoch 69/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1359806.5000\n",
      "Epoch 69: loss improved from 1365346.00000 to 1360293.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1360293.0000\n",
      "Epoch 70/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1355312.6250\n",
      "Epoch 70: loss improved from 1360293.00000 to 1355208.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1355208.8750\n",
      "Epoch 71/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1352385.2500\n",
      "Epoch 71: loss improved from 1355208.87500 to 1352668.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1352668.3750\n",
      "Epoch 72/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1349234.5000\n",
      "Epoch 72: loss improved from 1352668.37500 to 1349138.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1349138.0000\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1343872.3750\n",
      "Epoch 73: loss improved from 1349138.00000 to 1344042.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1344042.2500\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1340278.7500\n",
      "Epoch 74: loss improved from 1344042.25000 to 1340445.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1340445.0000\n",
      "Epoch 75/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1339013.0000\n",
      "Epoch 75: loss improved from 1340445.00000 to 1339060.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1339060.1250\n",
      "Epoch 76/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1335922.3750\n",
      "Epoch 76: loss improved from 1339060.12500 to 1335032.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1335032.1250\n",
      "Epoch 77/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1330148.1250\n",
      "Epoch 77: loss improved from 1335032.12500 to 1330767.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1330767.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1331648.3750\n",
      "Epoch 78: loss did not improve from 1330767.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1331503.5000\n",
      "Epoch 79/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1324147.7500\n",
      "Epoch 79: loss improved from 1330767.50000 to 1323987.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1323987.3750\n",
      "Epoch 80/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1321036.0000\n",
      "Epoch 80: loss improved from 1323987.37500 to 1321563.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1321563.3750\n",
      "Epoch 81/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1318143.6250\n",
      "Epoch 81: loss improved from 1321563.37500 to 1317858.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1317858.8750\n",
      "Epoch 82/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1315830.7500\n",
      "Epoch 82: loss improved from 1317858.87500 to 1315869.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1315869.1250\n",
      "Epoch 83/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1313889.3750\n",
      "Epoch 83: loss improved from 1315869.12500 to 1313987.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1313987.3750\n",
      "Epoch 84/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1312614.5000\n",
      "Epoch 84: loss improved from 1313987.37500 to 1312880.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1312880.7500\n",
      "Epoch 85/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1308478.3750\n",
      "Epoch 85: loss improved from 1312880.75000 to 1308548.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1308548.2500\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1306959.8750\n",
      "Epoch 86: loss improved from 1308548.25000 to 1307884.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1307884.0000\n",
      "Epoch 87/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1300834.8750\n",
      "Epoch 87: loss improved from 1307884.00000 to 1301144.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1301144.6250\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1299237.7500\n",
      "Epoch 88: loss improved from 1301144.62500 to 1299043.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1299043.5000\n",
      "Epoch 89/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1298784.1250\n",
      "Epoch 89: loss improved from 1299043.50000 to 1298118.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1298118.0000\n",
      "Epoch 90/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1295111.3750\n",
      "Epoch 90: loss improved from 1298118.00000 to 1295447.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1295447.0000\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1291826.1250\n",
      "Epoch 91: loss improved from 1295447.00000 to 1291826.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1291826.1250\n",
      "Epoch 92/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1290335.0000\n",
      "Epoch 92: loss improved from 1291826.12500 to 1290847.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1290847.1250\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1288513.1250\n",
      "Epoch 93: loss improved from 1290847.12500 to 1288331.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1288331.8750\n",
      "Epoch 94/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1284592.2500\n",
      "Epoch 94: loss improved from 1288331.87500 to 1284552.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1284552.1250\n",
      "Epoch 95/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1282711.5000\n",
      "Epoch 95: loss improved from 1284552.12500 to 1282550.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1282550.1250\n",
      "Epoch 96/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1279813.5000\n",
      "Epoch 96: loss improved from 1282550.12500 to 1279901.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1279901.1250\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1278593.5000\n",
      "Epoch 97: loss improved from 1279901.12500 to 1278328.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1278328.7500\n",
      "Epoch 98/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1277215.2500\n",
      "Epoch 98: loss improved from 1278328.75000 to 1277410.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1277410.0000\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1271838.8750\n",
      "Epoch 99: loss improved from 1277410.00000 to 1272204.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1272204.1250\n",
      "Epoch 100/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1272651.1250\n",
      "Epoch 100: loss did not improve from 1272204.12500\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1272539.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 12:28:49 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 12:28:49 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpqqbfkcfc\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpqqbfkcfc\\model\\data\\model\\assets\n",
      "2023/05/28 12:30:19 INFO mlflow.tracking.fluent: Experiment with name 'dnn4_stacked2vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_191 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_196 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_76 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_77 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,019,000\n",
      "Trainable params: 42,019,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 2:32 - loss: 3845231.5000  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.0341s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.0341s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3187/3189 [============================>.] - ETA: 0s - loss: 2740108.7500\n",
      "Epoch 1: loss improved from inf to 2739847.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 8ms/step - loss: 2739847.2500\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2732745.5000\n",
      "Epoch 2: loss improved from 2739847.25000 to 2734099.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2734099.2500\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2730564.0000\n",
      "Epoch 3: loss improved from 2734099.25000 to 2729782.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2729782.2500\n",
      "Epoch 4/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2725809.5000\n",
      "Epoch 4: loss improved from 2729782.25000 to 2725458.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2725458.7500\n",
      "Epoch 5/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2720345.7500\n",
      "Epoch 5: loss improved from 2725458.75000 to 2719747.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2719747.2500\n",
      "Epoch 6/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2711039.5000\n",
      "Epoch 6: loss improved from 2719747.25000 to 2711421.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2711421.5000\n",
      "Epoch 7/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2702390.7500\n",
      "Epoch 7: loss improved from 2711421.50000 to 2702090.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2702090.5000\n",
      "Epoch 8/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2691153.7500\n",
      "Epoch 8: loss improved from 2702090.50000 to 2691499.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2691499.0000\n",
      "Epoch 9/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2681938.5000\n",
      "Epoch 9: loss improved from 2691499.00000 to 2681812.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2681812.7500\n",
      "Epoch 10/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2671092.7500\n",
      "Epoch 10: loss improved from 2681812.75000 to 2671830.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2671830.2500\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2663339.5000\n",
      "Epoch 11: loss improved from 2671830.25000 to 2663339.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2663339.5000\n",
      "Epoch 12/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2655455.2500\n",
      "Epoch 12: loss improved from 2663339.50000 to 2655058.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2655058.0000\n",
      "Epoch 13/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2647214.5000\n",
      "Epoch 13: loss improved from 2655058.00000 to 2647118.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2647118.2500\n",
      "Epoch 14/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2641573.7500\n",
      "Epoch 14: loss improved from 2647118.25000 to 2640994.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 45s 14ms/step - loss: 2640994.7500\n",
      "Epoch 15/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2635174.5000\n",
      "Epoch 15: loss improved from 2640994.75000 to 2634544.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2634544.5000\n",
      "Epoch 16/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2626754.5000\n",
      "Epoch 16: loss improved from 2634544.50000 to 2627747.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2627747.5000\n",
      "Epoch 17/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2622881.0000\n",
      "Epoch 17: loss improved from 2627747.50000 to 2622414.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2622414.0000\n",
      "Epoch 18/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2615955.0000\n",
      "Epoch 18: loss improved from 2622414.00000 to 2616162.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2616162.2500\n",
      "Epoch 19/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2610134.0000\n",
      "Epoch 19: loss improved from 2616162.25000 to 2610136.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2610136.7500\n",
      "Epoch 20/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2604345.5000\n",
      "Epoch 20: loss improved from 2610136.75000 to 2605363.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2605363.2500\n",
      "Epoch 21/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2600437.0000\n",
      "Epoch 21: loss improved from 2605363.25000 to 2599816.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2599816.0000\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2596825.7500\n",
      "Epoch 22: loss improved from 2599816.00000 to 2596525.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2596525.7500\n",
      "Epoch 23/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2591750.5000\n",
      "Epoch 23: loss improved from 2596525.75000 to 2592042.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2592042.5000\n",
      "Epoch 24/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2588840.7500\n",
      "Epoch 24: loss improved from 2592042.50000 to 2588450.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2588450.7500\n",
      "Epoch 25/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2582973.0000\n",
      "Epoch 25: loss improved from 2588450.75000 to 2582709.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2582709.0000\n",
      "Epoch 26/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2579927.2500\n",
      "Epoch 26: loss improved from 2582709.00000 to 2580442.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2580442.5000\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2575414.7500\n",
      "Epoch 27: loss improved from 2580442.50000 to 2575414.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2575414.7500\n",
      "Epoch 28/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2570626.2500\n",
      "Epoch 28: loss improved from 2575414.75000 to 2571217.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2571217.2500\n",
      "Epoch 29/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2568798.7500\n",
      "Epoch 29: loss improved from 2571217.25000 to 2567858.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2567858.2500\n",
      "Epoch 30/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2564973.2500\n",
      "Epoch 30: loss improved from 2567858.25000 to 2564296.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2564296.5000\n",
      "Epoch 31/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2562040.7500\n",
      "Epoch 31: loss improved from 2564296.50000 to 2561277.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2561277.5000\n",
      "Epoch 32/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2556984.5000\n",
      "Epoch 32: loss improved from 2561277.50000 to 2557307.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2557307.5000\n",
      "Epoch 33/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2556026.2500\n",
      "Epoch 33: loss improved from 2557307.50000 to 2554093.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 23s 7ms/step - loss: 2554093.0000\n",
      "Epoch 34/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2550230.7500\n",
      "Epoch 34: loss improved from 2554093.00000 to 2551394.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 2551394.0000\n",
      "Epoch 35/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2548987.7500\n",
      "Epoch 35: loss improved from 2551394.00000 to 2549309.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2549309.7500\n",
      "Epoch 36/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2546799.0000\n",
      "Epoch 36: loss improved from 2549309.75000 to 2546865.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2546865.0000\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2543234.7500\n",
      "Epoch 37: loss improved from 2546865.00000 to 2543234.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2543234.7500\n",
      "Epoch 38/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2539226.5000\n",
      "Epoch 38: loss improved from 2543234.75000 to 2539223.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2539223.7500\n",
      "Epoch 39/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2537524.0000\n",
      "Epoch 39: loss improved from 2539223.75000 to 2538454.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2538454.5000\n",
      "Epoch 40/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2535701.5000\n",
      "Epoch 40: loss improved from 2538454.50000 to 2535116.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2535116.7500\n",
      "Epoch 41/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2531977.7500\n",
      "Epoch 41: loss improved from 2535116.75000 to 2532431.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2532431.0000\n",
      "Epoch 42/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2531610.2500\n",
      "Epoch 42: loss improved from 2532431.00000 to 2530815.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2530815.0000\n",
      "Epoch 43/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2529433.2500\n",
      "Epoch 43: loss improved from 2530815.00000 to 2528767.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2528767.7500\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2527993.5000\n",
      "Epoch 44: loss improved from 2528767.75000 to 2527560.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2527560.5000\n",
      "Epoch 45/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2522236.0000\n",
      "Epoch 45: loss improved from 2527560.50000 to 2522814.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2522814.5000\n",
      "Epoch 46/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2520381.2500\n",
      "Epoch 46: loss improved from 2522814.50000 to 2521187.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2521187.5000\n",
      "Epoch 47/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2519392.2500\n",
      "Epoch 47: loss improved from 2521187.50000 to 2518996.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2518996.2500\n",
      "Epoch 48/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2517822.2500\n",
      "Epoch 48: loss improved from 2518996.25000 to 2518125.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2518125.2500\n",
      "Epoch 49/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2514822.0000\n",
      "Epoch 49: loss improved from 2518125.25000 to 2514557.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2514557.0000\n",
      "Epoch 50/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2512700.5000\n",
      "Epoch 50: loss improved from 2514557.00000 to 2512621.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2512621.2500\n",
      "Epoch 51/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2510176.5000\n",
      "Epoch 51: loss improved from 2512621.25000 to 2510328.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2510328.5000\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2509827.0000\n",
      "Epoch 52: loss improved from 2510328.50000 to 2508970.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2508970.5000\n",
      "Epoch 53/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2507499.5000\n",
      "Epoch 53: loss improved from 2508970.50000 to 2507418.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2507418.0000\n",
      "Epoch 54/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2504592.5000\n",
      "Epoch 54: loss improved from 2507418.00000 to 2504938.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2504938.7500\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2503170.5000\n",
      "Epoch 55: loss improved from 2504938.75000 to 2502678.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2502678.5000\n",
      "Epoch 56/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2501868.0000\n",
      "Epoch 56: loss improved from 2502678.50000 to 2502116.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2502116.2500\n",
      "Epoch 57/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2498381.0000\n",
      "Epoch 57: loss improved from 2502116.25000 to 2500958.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2500958.5000\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2499141.0000\n",
      "Epoch 58: loss improved from 2500958.50000 to 2499141.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2499141.0000\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2497076.7500\n",
      "Epoch 59: loss improved from 2499141.00000 to 2496672.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2496672.2500\n",
      "Epoch 60/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2496964.7500\n",
      "Epoch 60: loss improved from 2496672.25000 to 2495890.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2495890.0000\n",
      "Epoch 61/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2495023.2500\n",
      "Epoch 61: loss improved from 2495890.00000 to 2494875.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2494875.0000\n",
      "Epoch 62/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2494751.5000\n",
      "Epoch 62: loss improved from 2494875.00000 to 2494168.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2494168.7500\n",
      "Epoch 63/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2491382.2500\n",
      "Epoch 63: loss improved from 2494168.75000 to 2491435.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2491435.0000\n",
      "Epoch 64/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2490067.7500\n",
      "Epoch 64: loss improved from 2491435.00000 to 2489887.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2489887.5000\n",
      "Epoch 65/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2486752.5000\n",
      "Epoch 65: loss improved from 2489887.50000 to 2486959.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2486959.2500\n",
      "Epoch 66/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2485933.7500\n",
      "Epoch 66: loss did not improve from 2486959.25000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2487091.0000\n",
      "Epoch 67/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2487798.2500\n",
      "Epoch 67: loss did not improve from 2486959.25000\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2487767.0000\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2484027.2500\n",
      "Epoch 68: loss improved from 2486959.25000 to 2484027.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2484027.2500\n",
      "Epoch 69/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2482809.7500\n",
      "Epoch 69: loss improved from 2484027.25000 to 2482141.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2482141.2500\n",
      "Epoch 70/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2483160.2500\n",
      "Epoch 70: loss did not improve from 2482141.25000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2483258.0000\n",
      "Epoch 71/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2478776.5000\n",
      "Epoch 71: loss did not improve from 2482141.25000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2482290.0000\n",
      "Epoch 72/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2480125.2500\n",
      "Epoch 72: loss improved from 2482141.25000 to 2480129.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2480129.2500\n",
      "Epoch 73/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2479939.5000\n",
      "Epoch 73: loss did not improve from 2480129.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2480180.5000\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2476426.2500\n",
      "Epoch 74: loss improved from 2480129.25000 to 2476720.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2476720.7500\n",
      "Epoch 75/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2478022.7500\n",
      "Epoch 75: loss did not improve from 2476720.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2478022.7500\n",
      "Epoch 76/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2473675.5000\n",
      "Epoch 76: loss improved from 2476720.75000 to 2474019.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2474019.2500\n",
      "Epoch 77/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2475704.2500\n",
      "Epoch 77: loss did not improve from 2474019.25000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2476104.5000\n",
      "Epoch 78/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2474977.0000\n",
      "Epoch 78: loss did not improve from 2474019.25000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2475399.0000\n",
      "Epoch 79/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2475624.5000\n",
      "Epoch 79: loss did not improve from 2474019.25000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2475051.2500\n",
      "Epoch 80/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2470848.2500\n",
      "Epoch 80: loss improved from 2474019.25000 to 2471177.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2471177.0000\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2472101.5000\n",
      "Epoch 81: loss did not improve from 2471177.00000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2471764.5000\n",
      "Epoch 82/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2470783.2500\n",
      "Epoch 82: loss improved from 2471177.00000 to 2470808.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2470808.5000\n",
      "Epoch 83/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2470149.2500\n",
      "Epoch 83: loss improved from 2470808.50000 to 2470149.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2470149.2500\n",
      "Epoch 84/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2469143.7500\n",
      "Epoch 84: loss improved from 2470149.25000 to 2469684.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2469684.0000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2469862.0000\n",
      "Epoch 85: loss improved from 2469684.00000 to 2469371.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2469371.5000\n",
      "Epoch 86/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2466969.0000\n",
      "Epoch 86: loss improved from 2469371.50000 to 2466777.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2466777.7500\n",
      "Epoch 87/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2465119.2500\n",
      "Epoch 87: loss improved from 2466777.75000 to 2465465.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2465465.5000\n",
      "Epoch 88/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2466392.2500\n",
      "Epoch 88: loss did not improve from 2465465.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2467010.5000\n",
      "Epoch 89/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2465987.7500\n",
      "Epoch 89: loss did not improve from 2465465.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2466472.0000\n",
      "Epoch 90/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2462072.5000\n",
      "Epoch 90: loss did not improve from 2465465.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2465812.7500\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2467245.2500\n",
      "Epoch 91: loss did not improve from 2465465.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2466448.0000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2464742.0000\n",
      "Epoch 92: loss improved from 2465465.50000 to 2464321.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2464321.5000\n",
      "Epoch 93/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2462999.5000\n",
      "Epoch 93: loss improved from 2464321.50000 to 2463131.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2463131.2500\n",
      "Epoch 94/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2463854.7500\n",
      "Epoch 94: loss did not improve from 2463131.25000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2463355.5000\n",
      "Epoch 95/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2460085.2500\n",
      "Epoch 95: loss improved from 2463131.25000 to 2460358.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2460358.5000\n",
      "Epoch 96/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2460611.7500\n",
      "Epoch 96: loss did not improve from 2460358.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2460767.0000\n",
      "Epoch 97/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2462248.5000\n",
      "Epoch 97: loss did not improve from 2460358.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2462087.2500\n",
      "Epoch 98/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2460932.5000\n",
      "Epoch 98: loss did not improve from 2460358.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2461195.2500\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2457526.2500\n",
      "Epoch 99: loss improved from 2460358.50000 to 2459327.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2459327.5000\n",
      "Epoch 100/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2458638.5000\n",
      "Epoch 100: loss did not improve from 2459327.50000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2459763.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 13:02:48 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 13:02:48 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpjld_czp6\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpjld_czp6\\model\\data\\model\\assets\n",
      "2023/05/28 13:03:47 INFO mlflow.tracking.fluent: Experiment with name 'dnn3_stacked2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_199 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_200 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_201 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_78 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_204 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_205 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_206 (Dense)           (None, 1024)              328704    \n",
      "                                                                 \n",
      " dense_207 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_80 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_81 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_210 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_211 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_212 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,546,500\n",
      "Trainable params: 4,546,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   6/3189 [..............................] - ETA: 1:57 - loss: 2497512.5000 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0213s vs `on_train_batch_end` time: 0.0995s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0213s vs `on_train_batch_end` time: 0.0995s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2726768.2500\n",
      "Epoch 1: loss improved from inf to 2726768.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 37s 9ms/step - loss: 2726768.2500\n",
      "Epoch 2/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2702217.2500\n",
      "Epoch 2: loss improved from 2726768.25000 to 2703834.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2703834.0000\n",
      "Epoch 3/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2683262.0000\n",
      "Epoch 3: loss improved from 2703834.00000 to 2683075.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2683075.0000\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2664222.7500\n",
      "Epoch 4: loss improved from 2683075.00000 to 2664222.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2664222.7500\n",
      "Epoch 5/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2639357.0000\n",
      "Epoch 5: loss improved from 2664222.75000 to 2639203.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2639203.5000\n",
      "Epoch 6/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2614944.2500\n",
      "Epoch 6: loss improved from 2639203.50000 to 2615494.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2615494.5000\n",
      "Epoch 7/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2592769.7500\n",
      "Epoch 7: loss improved from 2615494.50000 to 2595037.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2595037.5000\n",
      "Epoch 8/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2578814.7500\n",
      "Epoch 8: loss improved from 2595037.50000 to 2578556.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2578556.2500\n",
      "Epoch 9/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2564304.2500\n",
      "Epoch 9: loss improved from 2578556.25000 to 2562871.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2562871.0000\n",
      "Epoch 10/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2548049.7500\n",
      "Epoch 10: loss improved from 2562871.00000 to 2547720.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2547720.7500\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2536119.7500\n",
      "Epoch 11: loss improved from 2547720.75000 to 2536119.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2536119.7500\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2523875.5000\n",
      "Epoch 12: loss improved from 2536119.75000 to 2523403.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2523403.2500\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2512114.5000\n",
      "Epoch 13: loss improved from 2523403.25000 to 2512114.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2512114.5000\n",
      "Epoch 14/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2502871.5000\n",
      "Epoch 14: loss improved from 2512114.50000 to 2502079.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2502079.7500\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2491056.0000\n",
      "Epoch 15: loss improved from 2502079.75000 to 2492036.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2492036.0000\n",
      "Epoch 16/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2481741.2500\n",
      "Epoch 16: loss improved from 2492036.00000 to 2481646.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2481646.0000\n",
      "Epoch 17/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2473800.2500\n",
      "Epoch 17: loss improved from 2481646.00000 to 2473655.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2473655.7500\n",
      "Epoch 18/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2465916.0000\n",
      "Epoch 18: loss improved from 2473655.75000 to 2464804.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2464804.7500\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2456578.0000\n",
      "Epoch 19: loss improved from 2464804.75000 to 2456466.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2456466.0000\n",
      "Epoch 20/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2448286.7500\n",
      "Epoch 20: loss improved from 2456466.00000 to 2448257.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2448257.7500\n",
      "Epoch 21/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2441704.5000\n",
      "Epoch 21: loss improved from 2448257.75000 to 2442155.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2442155.7500\n",
      "Epoch 22/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2433620.2500\n",
      "Epoch 22: loss improved from 2442155.75000 to 2433620.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2433620.2500\n",
      "Epoch 23/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2429345.2500\n",
      "Epoch 23: loss improved from 2433620.25000 to 2428737.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2428737.7500\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2420753.5000\n",
      "Epoch 24: loss improved from 2428737.75000 to 2420636.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2420636.5000\n",
      "Epoch 25/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2415318.2500\n",
      "Epoch 25: loss improved from 2420636.50000 to 2415009.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2415009.5000\n",
      "Epoch 26/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2409167.7500\n",
      "Epoch 26: loss improved from 2415009.50000 to 2409070.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2409070.7500\n",
      "Epoch 27/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2402602.7500\n",
      "Epoch 27: loss improved from 2409070.75000 to 2403850.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2403850.5000\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2398523.0000\n",
      "Epoch 28: loss improved from 2403850.50000 to 2397884.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2397884.7500\n",
      "Epoch 29/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2391667.7500\n",
      "Epoch 29: loss improved from 2397884.75000 to 2390605.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2390605.7500\n",
      "Epoch 30/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2387052.2500\n",
      "Epoch 30: loss improved from 2390605.75000 to 2386466.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2386466.5000\n",
      "Epoch 31/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2381543.0000\n",
      "Epoch 31: loss improved from 2386466.50000 to 2381563.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2381563.2500\n",
      "Epoch 32/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2376505.2500\n",
      "Epoch 32: loss improved from 2381563.25000 to 2376051.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2376051.2500\n",
      "Epoch 33/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2372240.7500\n",
      "Epoch 33: loss improved from 2376051.25000 to 2372232.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2372232.2500\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2366169.0000\n",
      "Epoch 34: loss improved from 2372232.25000 to 2366169.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2366169.0000\n",
      "Epoch 35/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2361533.5000\n",
      "Epoch 35: loss improved from 2366169.00000 to 2361723.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2361723.0000\n",
      "Epoch 36/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2357169.2500\n",
      "Epoch 36: loss improved from 2361723.00000 to 2357461.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2357461.7500\n",
      "Epoch 37/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2351412.5000\n",
      "Epoch 37: loss improved from 2357461.75000 to 2351706.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2351706.2500\n",
      "Epoch 38/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2349319.7500\n",
      "Epoch 38: loss improved from 2351706.25000 to 2348989.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2348989.0000\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2344278.7500\n",
      "Epoch 39: loss improved from 2348989.00000 to 2344254.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2344254.0000\n",
      "Epoch 40/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2339688.5000\n",
      "Epoch 40: loss improved from 2344254.00000 to 2339557.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2339557.2500\n",
      "Epoch 41/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2336183.0000\n",
      "Epoch 41: loss improved from 2339557.25000 to 2336271.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2336271.5000\n",
      "Epoch 42/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2332864.2500\n",
      "Epoch 42: loss improved from 2336271.50000 to 2332822.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2332822.0000\n",
      "Epoch 43/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2327281.5000\n",
      "Epoch 43: loss improved from 2332822.00000 to 2329050.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2329050.2500\n",
      "Epoch 44/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2324345.0000\n",
      "Epoch 44: loss improved from 2329050.25000 to 2324345.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2324345.0000\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2321285.0000\n",
      "Epoch 45: loss improved from 2324345.00000 to 2321285.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 24s 8ms/step - loss: 2321285.0000\n",
      "Epoch 46/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2318133.2500\n",
      "Epoch 46: loss improved from 2321285.00000 to 2318008.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 23s 7ms/step - loss: 2318008.0000\n",
      "Epoch 47/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2313717.2500\n",
      "Epoch 47: loss improved from 2318008.00000 to 2313566.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2313566.7500\n",
      "Epoch 48/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2310354.2500\n",
      "Epoch 48: loss improved from 2313566.75000 to 2310676.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2310676.0000\n",
      "Epoch 49/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2307662.7500\n",
      "Epoch 49: loss improved from 2310676.00000 to 2307807.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2307807.0000\n",
      "Epoch 50/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2305405.5000\n",
      "Epoch 50: loss improved from 2307807.00000 to 2305108.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2305108.2500\n",
      "Epoch 51/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2300627.5000\n",
      "Epoch 51: loss improved from 2305108.25000 to 2300599.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2300599.0000\n",
      "Epoch 52/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2298351.7500\n",
      "Epoch 52: loss improved from 2300599.00000 to 2298106.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2298106.7500\n",
      "Epoch 53/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2295339.5000\n",
      "Epoch 53: loss improved from 2298106.75000 to 2295221.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2295221.7500\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2291636.0000\n",
      "Epoch 54: loss improved from 2295221.75000 to 2291857.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2291857.0000\n",
      "Epoch 55/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2289958.2500\n",
      "Epoch 55: loss improved from 2291857.00000 to 2290303.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2290303.0000\n",
      "Epoch 56/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2285532.7500\n",
      "Epoch 56: loss improved from 2290303.00000 to 2284947.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2284947.5000\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2284134.0000\n",
      "Epoch 57: loss improved from 2284947.50000 to 2283848.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2283848.2500\n",
      "Epoch 58/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2281597.0000\n",
      "Epoch 58: loss improved from 2283848.25000 to 2281375.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2281375.7500\n",
      "Epoch 59/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2279009.5000\n",
      "Epoch 59: loss improved from 2281375.75000 to 2278455.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2278455.2500\n",
      "Epoch 60/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2275663.0000\n",
      "Epoch 60: loss improved from 2278455.25000 to 2276284.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2276284.0000\n",
      "Epoch 61/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2272326.0000\n",
      "Epoch 61: loss improved from 2276284.00000 to 2273019.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2273019.0000\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2270677.2500\n",
      "Epoch 62: loss improved from 2273019.00000 to 2270677.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2270677.2500\n",
      "Epoch 63/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2268180.7500\n",
      "Epoch 63: loss improved from 2270677.25000 to 2267761.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2267761.7500\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2266648.5000\n",
      "Epoch 64: loss improved from 2267761.75000 to 2266345.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2266345.0000\n",
      "Epoch 65/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2265023.7500\n",
      "Epoch 65: loss improved from 2266345.00000 to 2264646.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2264646.7500\n",
      "Epoch 66/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2263350.2500\n",
      "Epoch 66: loss improved from 2264646.75000 to 2262871.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2262871.7500\n",
      "Epoch 67/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2260426.5000\n",
      "Epoch 67: loss improved from 2262871.75000 to 2260264.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2260264.7500\n",
      "Epoch 68/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2257196.5000\n",
      "Epoch 68: loss improved from 2260264.75000 to 2257982.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2257982.0000\n",
      "Epoch 69/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2255441.7500\n",
      "Epoch 69: loss improved from 2257982.00000 to 2255441.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2255441.7500\n",
      "Epoch 70/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2253569.0000\n",
      "Epoch 70: loss improved from 2255441.75000 to 2253031.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2253031.5000\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2249803.0000\n",
      "Epoch 71: loss improved from 2253031.50000 to 2249803.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2249803.0000\n",
      "Epoch 72/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2248317.7500\n",
      "Epoch 72: loss improved from 2249803.00000 to 2248345.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2248345.2500\n",
      "Epoch 73/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2247564.7500\n",
      "Epoch 73: loss improved from 2248345.25000 to 2247000.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2247000.7500\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2245844.5000\n",
      "Epoch 74: loss improved from 2247000.75000 to 2245350.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 2245350.7500\n",
      "Epoch 75/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2242656.0000\n",
      "Epoch 75: loss improved from 2245350.75000 to 2242861.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2242861.0000\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2239906.2500\n",
      "Epoch 76: loss improved from 2242861.00000 to 2239906.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2239906.2500\n",
      "Epoch 77/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2239260.5000\n",
      "Epoch 77: loss improved from 2239906.25000 to 2239144.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2239144.0000\n",
      "Epoch 78/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2236422.5000\n",
      "Epoch 78: loss improved from 2239144.00000 to 2236422.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2236422.5000\n",
      "Epoch 79/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2235634.2500\n",
      "Epoch 79: loss improved from 2236422.50000 to 2235345.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2235345.7500\n",
      "Epoch 80/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2233176.0000\n",
      "Epoch 80: loss improved from 2235345.75000 to 2233434.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2233434.0000\n",
      "Epoch 81/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2232775.0000\n",
      "Epoch 81: loss improved from 2233434.00000 to 2232461.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2232461.2500\n",
      "Epoch 82/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2229825.0000\n",
      "Epoch 82: loss improved from 2232461.25000 to 2229659.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2229659.2500\n",
      "Epoch 83/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2228287.5000\n",
      "Epoch 83: loss improved from 2229659.25000 to 2228191.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2228191.5000\n",
      "Epoch 84/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2225041.7500\n",
      "Epoch 84: loss improved from 2228191.50000 to 2225640.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2225640.0000\n",
      "Epoch 85/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2224442.2500\n",
      "Epoch 85: loss improved from 2225640.00000 to 2224175.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2224175.5000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2221706.2500\n",
      "Epoch 86: loss improved from 2224175.50000 to 2221706.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2221706.2500\n",
      "Epoch 87/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2220895.0000\n",
      "Epoch 87: loss improved from 2221706.25000 to 2220895.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2220895.0000\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2217283.5000\n",
      "Epoch 88: loss improved from 2220895.00000 to 2218416.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2218416.7500\n",
      "Epoch 89/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2215872.2500\n",
      "Epoch 89: loss improved from 2218416.75000 to 2216390.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2216390.2500\n",
      "Epoch 90/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2216928.2500\n",
      "Epoch 90: loss did not improve from 2216390.25000\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2216581.0000\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2213497.7500\n",
      "Epoch 91: loss improved from 2216390.25000 to 2213008.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2213008.7500\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2211694.2500\n",
      "Epoch 92: loss improved from 2213008.75000 to 2211632.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2211632.7500\n",
      "Epoch 93/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2209974.5000\n",
      "Epoch 93: loss improved from 2211632.75000 to 2211076.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2211076.2500\n",
      "Epoch 94/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2207509.7500\n",
      "Epoch 94: loss improved from 2211076.25000 to 2208328.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2208328.0000\n",
      "Epoch 95/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2207795.5000\n",
      "Epoch 95: loss improved from 2208328.00000 to 2207027.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2207027.7500\n",
      "Epoch 96/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2204284.5000\n",
      "Epoch 96: loss improved from 2207027.75000 to 2205523.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2205523.2500\n",
      "Epoch 97/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2204423.7500\n",
      "Epoch 97: loss improved from 2205523.25000 to 2204343.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2204343.0000\n",
      "Epoch 98/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2201413.7500\n",
      "Epoch 98: loss improved from 2204343.00000 to 2202716.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 2202716.7500\n",
      "Epoch 99/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2198649.5000\n",
      "Epoch 99: loss improved from 2202716.75000 to 2201029.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2201029.5000\n",
      "Epoch 100/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2200241.0000\n",
      "Epoch 100: loss improved from 2201029.50000 to 2200215.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2200215.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 13:37:44 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 13:37:44 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp4mj32ar3\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp4mj32ar3\\model\\data\\model\\assets\n",
      "2023/05/28 13:39:32 INFO mlflow.tracking.fluent: Experiment with name 'dnn3_stacked2vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_214 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_215 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_216 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_82 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_217 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_83 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_218 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_219 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_220 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_221 (Dense)           (None, 1024)              328704    \n",
      "                                                                 \n",
      " dense_222 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_223 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_84 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_224 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_85 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_225 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_226 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_227 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_228 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,546,500\n",
      "Trainable params: 4,546,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   6/3189 [..............................] - ETA: 1:30 - loss: 3684288.7500   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.0365s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.0365s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186/3189 [============================>.] - ETA: 0s - loss: 2732595.7500\n",
      "Epoch 1: loss improved from inf to 2732290.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 54s 12ms/step - loss: 2732290.7500\n",
      "Epoch 2/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2727654.5000\n",
      "Epoch 2: loss improved from 2732290.75000 to 2727802.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2727802.5000\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2726322.2500\n",
      "Epoch 3: loss improved from 2727802.50000 to 2726745.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2726745.0000\n",
      "Epoch 4/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2724548.2500\n",
      "Epoch 4: loss improved from 2726745.00000 to 2725481.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2725481.7500\n",
      "Epoch 5/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2724278.2500\n",
      "Epoch 5: loss improved from 2725481.75000 to 2724526.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2724526.7500\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2723564.0000\n",
      "Epoch 6: loss improved from 2724526.75000 to 2723564.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2723564.0000\n",
      "Epoch 7/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2723069.0000\n",
      "Epoch 7: loss improved from 2723564.00000 to 2722938.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2722938.5000\n",
      "Epoch 8/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2722825.7500\n",
      "Epoch 8: loss improved from 2722938.50000 to 2722343.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2722343.0000\n",
      "Epoch 9/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2721715.5000\n",
      "Epoch 9: loss did not improve from 2722343.00000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2722452.2500\n",
      "Epoch 10/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2720490.7500\n",
      "Epoch 10: loss improved from 2722343.00000 to 2721527.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2721527.0000\n",
      "Epoch 11/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2722056.7500\n",
      "Epoch 11: loss did not improve from 2721527.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2721684.0000\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2721926.5000\n",
      "Epoch 12: loss improved from 2721527.00000 to 2721349.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2721349.5000\n",
      "Epoch 13/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2721549.2500\n",
      "Epoch 13: loss improved from 2721349.50000 to 2721217.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2721217.0000\n",
      "Epoch 14/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2721869.7500\n",
      "Epoch 14: loss improved from 2721217.00000 to 2720743.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2720743.0000\n",
      "Epoch 15/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2719828.5000\n",
      "Epoch 15: loss improved from 2720743.00000 to 2720161.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2720161.7500\n",
      "Epoch 16/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2719076.0000\n",
      "Epoch 16: loss did not improve from 2720161.75000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2720523.0000\n",
      "Epoch 17/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2720691.5000\n",
      "Epoch 17: loss did not improve from 2720161.75000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2720957.2500\n",
      "Epoch 18/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2720857.2500\n",
      "Epoch 18: loss did not improve from 2720161.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2720915.5000\n",
      "Epoch 19/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2721112.5000\n",
      "Epoch 19: loss did not improve from 2720161.75000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2720431.2500\n",
      "Epoch 20/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2721088.2500\n",
      "Epoch 20: loss did not improve from 2720161.75000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2720340.7500\n",
      "Epoch 21/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2721401.0000\n",
      "Epoch 21: loss did not improve from 2720161.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2720925.5000\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2721749.7500\n",
      "Epoch 22: loss improved from 2720161.75000 to 2720085.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2720085.2500\n",
      "Epoch 23/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2720358.0000\n",
      "Epoch 23: loss did not improve from 2720085.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2720358.0000\n",
      "Epoch 24/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2721577.0000\n",
      "Epoch 24: loss did not improve from 2720085.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2720123.5000\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2720544.2500\n",
      "Epoch 25: loss improved from 2720085.25000 to 2719945.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2719945.7500\n",
      "Epoch 26/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2719780.5000\n",
      "Epoch 26: loss improved from 2719945.75000 to 2719839.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2719839.7500\n",
      "Epoch 27/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2719063.7500\n",
      "Epoch 27: loss did not improve from 2719839.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2720052.0000\n",
      "Epoch 28/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2719308.5000\n",
      "Epoch 28: loss improved from 2719839.75000 to 2719587.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2719587.5000\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2719120.2500\n",
      "Epoch 29: loss improved from 2719587.50000 to 2719120.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2719120.2500\n",
      "Epoch 30/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2718752.5000\n",
      "Epoch 30: loss improved from 2719120.25000 to 2718942.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2718942.0000\n",
      "Epoch 31/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2719035.0000\n",
      "Epoch 31: loss improved from 2718942.00000 to 2718297.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2718297.5000\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2717399.2500\n",
      "Epoch 32: loss improved from 2718297.50000 to 2717399.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2717399.2500\n",
      "Epoch 33/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2716285.2500\n",
      "Epoch 33: loss improved from 2717399.25000 to 2716489.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2716489.2500\n",
      "Epoch 34/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2714120.5000\n",
      "Epoch 34: loss improved from 2716489.25000 to 2714701.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2714701.0000\n",
      "Epoch 35/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2713385.5000\n",
      "Epoch 35: loss improved from 2714701.00000 to 2712453.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2712453.0000\n",
      "Epoch 36/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2710825.2500\n",
      "Epoch 36: loss improved from 2712453.00000 to 2710825.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2710825.2500\n",
      "Epoch 37/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2708615.0000\n",
      "Epoch 37: loss improved from 2710825.25000 to 2709459.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2709459.0000\n",
      "Epoch 38/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2709384.2500\n",
      "Epoch 38: loss improved from 2709459.00000 to 2709372.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2709372.7500\n",
      "Epoch 39/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2708482.0000\n",
      "Epoch 39: loss improved from 2709372.75000 to 2708026.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2708026.2500\n",
      "Epoch 40/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2710092.2500\n",
      "Epoch 40: loss improved from 2708026.25000 to 2707902.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2707902.7500\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2707322.5000\n",
      "Epoch 41: loss improved from 2707902.75000 to 2707322.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2707322.5000\n",
      "Epoch 42/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2708393.0000\n",
      "Epoch 42: loss improved from 2707322.50000 to 2707247.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2707247.0000\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2707095.7500\n",
      "Epoch 43: loss improved from 2707247.00000 to 2707095.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2707095.7500\n",
      "Epoch 44/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2708071.0000\n",
      "Epoch 44: loss improved from 2707095.75000 to 2706867.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2706867.7500\n",
      "Epoch 45/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2706700.7500\n",
      "Epoch 45: loss improved from 2706867.75000 to 2706494.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2706494.2500\n",
      "Epoch 46/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2706915.5000\n",
      "Epoch 46: loss improved from 2706494.25000 to 2706397.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2706397.7500\n",
      "Epoch 47/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2704317.5000\n",
      "Epoch 47: loss improved from 2706397.75000 to 2705915.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2705915.7500\n",
      "Epoch 48/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2706666.7500\n",
      "Epoch 48: loss did not improve from 2705915.75000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2705977.5000\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2705681.2500\n",
      "Epoch 49: loss improved from 2705915.75000 to 2705681.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2705681.2500\n",
      "Epoch 50/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2704542.2500\n",
      "Epoch 50: loss improved from 2705681.25000 to 2704802.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2704802.5000\n",
      "Epoch 51/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2703414.0000\n",
      "Epoch 51: loss improved from 2704802.50000 to 2703590.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2703590.2500\n",
      "Epoch 52/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2701683.7500\n",
      "Epoch 52: loss improved from 2703590.25000 to 2702610.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2702610.7500\n",
      "Epoch 53/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2701823.5000\n",
      "Epoch 53: loss improved from 2702610.75000 to 2701598.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2701598.0000\n",
      "Epoch 54/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2701897.0000\n",
      "Epoch 54: loss did not improve from 2701598.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2701927.7500\n",
      "Epoch 55/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2702665.7500\n",
      "Epoch 55: loss improved from 2701598.00000 to 2701515.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2701515.7500\n",
      "Epoch 56/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2701405.0000\n",
      "Epoch 56: loss improved from 2701515.75000 to 2701405.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2701405.0000\n",
      "Epoch 57/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2701339.2500\n",
      "Epoch 57: loss improved from 2701405.00000 to 2700964.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2700964.0000\n",
      "Epoch 58/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2699403.7500\n",
      "Epoch 58: loss improved from 2700964.00000 to 2699162.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2699162.7500\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2700211.5000\n",
      "Epoch 59: loss did not improve from 2699162.75000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2699665.0000\n",
      "Epoch 60/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2699028.7500\n",
      "Epoch 60: loss improved from 2699162.75000 to 2698909.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2698909.5000\n",
      "Epoch 61/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2697522.5000\n",
      "Epoch 61: loss improved from 2698909.50000 to 2697522.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2697522.5000\n",
      "Epoch 62/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2699037.5000\n",
      "Epoch 62: loss did not improve from 2697522.50000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2698162.0000\n",
      "Epoch 63/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2701552.0000\n",
      "Epoch 63: loss did not improve from 2697522.50000\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2700656.7500\n",
      "Epoch 64/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2697847.7500\n",
      "Epoch 64: loss improved from 2697522.50000 to 2696821.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2696821.0000\n",
      "Epoch 65/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2697050.0000\n",
      "Epoch 65: loss improved from 2696821.00000 to 2696667.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2696667.7500\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2694589.0000\n",
      "Epoch 66: loss improved from 2696667.75000 to 2696461.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2696461.2500\n",
      "Epoch 67/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2695318.0000\n",
      "Epoch 67: loss improved from 2696461.25000 to 2695258.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2695258.2500\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2694087.2500\n",
      "Epoch 68: loss improved from 2695258.25000 to 2694087.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2694087.2500\n",
      "Epoch 69/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2692436.7500\n",
      "Epoch 69: loss improved from 2694087.25000 to 2693455.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2693455.7500\n",
      "Epoch 70/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2692378.2500\n",
      "Epoch 70: loss improved from 2693455.75000 to 2693177.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2693177.0000\n",
      "Epoch 71/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2690040.2500\n",
      "Epoch 71: loss improved from 2693177.00000 to 2691820.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2691820.0000\n",
      "Epoch 72/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2691741.7500\n",
      "Epoch 72: loss did not improve from 2691820.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2691869.0000\n",
      "Epoch 73/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2692162.7500\n",
      "Epoch 73: loss did not improve from 2691820.00000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2691955.2500\n",
      "Epoch 74/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2691758.7500\n",
      "Epoch 74: loss improved from 2691820.00000 to 2690477.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2690477.0000\n",
      "Epoch 75/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2690707.2500\n",
      "Epoch 75: loss did not improve from 2690477.00000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2690498.7500\n",
      "Epoch 76/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2689711.7500\n",
      "Epoch 76: loss improved from 2690477.00000 to 2689394.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2689394.7500\n",
      "Epoch 77/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2688620.7500\n",
      "Epoch 77: loss improved from 2689394.75000 to 2688713.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2688713.5000\n",
      "Epoch 78/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2687974.7500\n",
      "Epoch 78: loss improved from 2688713.50000 to 2688132.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2688132.7500\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2686836.7500\n",
      "Epoch 79: loss improved from 2688132.75000 to 2688060.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2688060.0000\n",
      "Epoch 80/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2686920.7500\n",
      "Epoch 80: loss improved from 2688060.00000 to 2686785.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2686785.0000\n",
      "Epoch 81/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2686910.7500\n",
      "Epoch 81: loss did not improve from 2686785.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2687383.7500\n",
      "Epoch 82/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2686250.7500\n",
      "Epoch 82: loss improved from 2686785.00000 to 2686631.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2686631.5000\n",
      "Epoch 83/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2687309.7500\n",
      "Epoch 83: loss did not improve from 2686631.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2686803.5000\n",
      "Epoch 84/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2687216.5000\n",
      "Epoch 84: loss improved from 2686631.50000 to 2686316.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2686316.2500\n",
      "Epoch 85/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2684552.7500\n",
      "Epoch 85: loss improved from 2686316.25000 to 2685498.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2685498.0000\n",
      "Epoch 86/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2684717.2500\n",
      "Epoch 86: loss improved from 2685498.00000 to 2684546.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2684546.0000\n",
      "Epoch 87/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2684757.7500\n",
      "Epoch 87: loss did not improve from 2684546.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2684757.7500\n",
      "Epoch 88/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2685207.7500\n",
      "Epoch 88: loss did not improve from 2684546.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2685066.7500\n",
      "Epoch 89/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2685410.2500\n",
      "Epoch 89: loss improved from 2684546.00000 to 2683980.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 2683980.0000\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2684275.2500\n",
      "Epoch 90: loss did not improve from 2683980.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2684275.2500\n",
      "Epoch 91/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2681483.2500\n",
      "Epoch 91: loss improved from 2683980.00000 to 2683188.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2683188.0000\n",
      "Epoch 92/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2682739.7500\n",
      "Epoch 92: loss improved from 2683188.00000 to 2682739.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2682739.7500\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2682069.2500\n",
      "Epoch 93: loss improved from 2682739.75000 to 2682069.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2682069.2500\n",
      "Epoch 94/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2681233.2500\n",
      "Epoch 94: loss improved from 2682069.25000 to 2680808.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2680808.5000\n",
      "Epoch 95/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2681645.2500\n",
      "Epoch 95: loss did not improve from 2680808.50000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2681240.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2681213.0000\n",
      "Epoch 96: loss did not improve from 2680808.50000\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 2681213.0000\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2680867.2500\n",
      "Epoch 97: loss did not improve from 2680808.50000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2680867.2500\n",
      "Epoch 98/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2681073.5000\n",
      "Epoch 98: loss improved from 2680808.50000 to 2680307.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2680307.5000\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2680286.2500\n",
      "Epoch 99: loss improved from 2680307.50000 to 2679125.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2679125.0000\n",
      "Epoch 100/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2678468.0000\n",
      "Epoch 100: loss improved from 2679125.00000 to 2678950.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2678950.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 14:09:12 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 14:09:12 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp6pl96rjn\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp6pl96rjn\\model\\data\\model\\assets\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "MODELS=[\"dnn4_stacked2\",\"dnn3_stacked2\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=1\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\"vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_73 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_31 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_45 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_52 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_24  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,804,690\n",
      "Trainable params: 3,090,002\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736509.5000\n",
      "Epoch 1: loss improved from inf to 2736509.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 253s 79ms/step - loss: 2736509.5000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 2: loss improved from 2736509.50000 to 2736506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736506.0000\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 3: loss improved from 2736506.00000 to 2736505.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.7500\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 4: loss did not improve from 2736505.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736506.7500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 5: loss improved from 2736505.75000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736503.7500\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 6: loss improved from 2736503.75000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736503.0000\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 7: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.0000\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 8: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.0000\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 9: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.0000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.5000\n",
      "Epoch 10: loss improved from 2736503.00000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736501.5000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 11: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.0000\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 12: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 13: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 14: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736506.2500\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 15: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.0000\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 16: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736502.5000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 17: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 18: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736502.7500\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 19: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.7500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 20: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 03:02:46 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 03:02:46 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0mm09674\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0mm09674\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_76 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_32 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_46 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_25  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,804,690\n",
      "Trainable params: 3,090,002\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 1: loss improved from inf to 2736506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 251s 79ms/step - loss: 2736506.0000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 2: loss improved from 2736506.00000 to 2736503.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736503.5000\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 3: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736505.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 4: loss improved from 2736503.50000 to 2736502.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736502.5000\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.0000\n",
      "Epoch 5: loss improved from 2736502.50000 to 2736500.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736500.0000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 6: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736503.7500\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 7: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736505.2500\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 8: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736504.7500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 9: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736504.7500\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.5000\n",
      "Epoch 10: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736500.5000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 11: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.7500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 12: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736503.0000\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 13: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736503.5000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 14: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.2500\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 15: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736504.5000\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 16: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.0000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 17: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736503.0000\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 18: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.7500\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 19: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736502.7500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 20: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 04:26:04 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 04:26:04 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_pgn4e53\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_pgn4e53\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_79 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_33 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_47 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_26  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,804,690\n",
      "Trainable params: 3,090,002\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 1: loss improved from inf to 2736504.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 252s 79ms/step - loss: 2736504.7500\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 2: loss improved from 2736504.75000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736503.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 3: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 4: loss improved from 2736503.75000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736502.7500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 5: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 6: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.7500\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.2500\n",
      "Epoch 7: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736507.2500\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 8: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.2500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 9: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 10: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736506.2500\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.2500\n",
      "Epoch 11: loss improved from 2736502.75000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736502.2500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 12: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 13: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.0000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 14: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 250s 79ms/step - loss: 2736505.2500\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 15: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 250s 79ms/step - loss: 2736503.5000\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 16: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 253s 79ms/step - loss: 2736505.0000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 17: loss improved from 2736502.25000 to 2736501.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 79ms/step - loss: 2736501.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.5000\n",
      "Epoch 18: loss improved from 2736501.75000 to 2736500.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 255s 80ms/step - loss: 2736500.5000\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 19: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 255s 80ms/step - loss: 2736505.2500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 20: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 255s 80ms/step - loss: 2736504.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 05:50:02 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 05:50:02 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp792373wm\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp792373wm\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_82 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_34 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_48 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_55 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_27  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,804,690\n",
      "Trainable params: 3,090,002\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   6/3189 [..............................] - ETA: 4:47 - loss: 2911041.2500WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0489s vs `on_train_batch_end` time: 0.0504s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0489s vs `on_train_batch_end` time: 0.0504s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 1: loss improved from inf to 2736506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 260s 81ms/step - loss: 2736506.0000\n",
      "Epoch 2/20\n",
      "1968/3189 [=================>............] - ETA: 1:37 - loss: 2699776.2500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m mlflow\u001b[39m.\u001b[39mtensorflow\u001b[39m.\u001b[39mautolog()\n\u001b[0;32m     13\u001b[0m model\u001b[39m=\u001b[39mselect_model(m,INPUTS,drop_out\u001b[39m=\u001b[39mdrop_out)\n\u001b[1;32m---> 14\u001b[0m model,history\u001b[39m=\u001b[39mtrain_model(model,EPOCHS)\n\u001b[0;32m     16\u001b[0m directory\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(directory):\n\u001b[0;32m     18\u001b[0m     \u001b[39m#print ('callbacks/model.'+str(file)+'.h5')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model,epochs):\n\u001b[0;32m      2\u001b[0m     my_callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         \u001b[39m#tf.keras.callbacks.ModelCheckpoint(filepath='callbacks/model.{epoch:02d}.h5')\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         ,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     ]\n\u001b[1;32m---> 14\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     15\u001b[0m                     train_gen\n\u001b[0;32m     16\u001b[0m                     ,epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m     17\u001b[0m                     ,callbacks\u001b[39m=\u001b[39;49mmy_callbacks\n\u001b[0;32m     18\u001b[0m                     )\n\u001b[0;32m     20\u001b[0m     plt\u001b[39m.\u001b[39mclf()\n\u001b[0;32m     21\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:553\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m try_log_autologging_event(\n\u001b[0;32m    544\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_start,\n\u001b[0;32m    545\u001b[0m     session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     kwargs,\n\u001b[0;32m    550\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[39mif\u001b[39;00m patch_is_class:\n\u001b[1;32m--> 553\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:170\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[1;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mcls\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:181\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_exception(e)\n\u001b[0;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[39m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[39m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:174\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    173\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_patch_implementation(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    175\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    176\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:232\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mlflow\u001b[39m.\u001b[39mactive_run():\n\u001b[0;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m--> 232\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_patch_implementation(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run:\n\u001b[0;32m    235\u001b[0m     mlflow\u001b[39m.\u001b[39mend_run(RunStatus\u001b[39m.\u001b[39mto_string(RunStatus\u001b[39m.\u001b[39mFINISHED))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\tensorflow\\__init__.py:1226\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[1;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1223\u001b[0m early_stop_callback \u001b[39m=\u001b[39m _get_early_stop_callback(callbacks)\n\u001b[0;32m   1224\u001b[0m _log_early_stop_callback_params(early_stop_callback)\n\u001b[1;32m-> 1226\u001b[0m history \u001b[39m=\u001b[39m original(inst, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1228\u001b[0m \u001b[39mif\u001b[39;00m log_models:\n\u001b[0;32m   1229\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:536\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[1;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[0;32m    534\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[1;32m--> 536\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:471\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[1;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    464\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[0;32m    465\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         og_kwargs,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 471\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39mog_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mog_kwargs)\n\u001b[0;32m    473\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    474\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[0;32m    475\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m         og_kwargs,\n\u001b[0;32m    480\u001b[0m     )\n\u001b[0;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:533\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[1;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[0;32m    530\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    531\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    532\u001b[0m ):\n\u001b[1;32m--> 533\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[0;32m    534\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "#MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "MODELS=[\"VGG16\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=2\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\" vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_61 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_27 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_41 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_48 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_20  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,843,154\n",
      "Trainable params: 3,614,290\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   6/3189 [..............................] - ETA: 2:22 - loss: 3688507.0000 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0235s vs `on_train_batch_end` time: 0.0238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0235s vs `on_train_batch_end` time: 0.0238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736951.2500\n",
      "Epoch 1: loss improved from inf to 2736504.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 104s 32ms/step - loss: 2736504.5000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 2: loss did not improve from 2736504.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.2500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.2500\n",
      "Epoch 3: loss did not improve from 2736504.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736507.2500\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 4: loss did not improve from 2736504.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 5: loss improved from 2736504.50000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 6: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.2500\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 7: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.5000\n",
      "Epoch 8: loss improved from 2736502.75000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736501.5000\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 9: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 10: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 11: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 12: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 13: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.5000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 14: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 15: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 16: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 17: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 18: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.0000\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 19: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 102s 32ms/step - loss: 2736502.5000\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 20: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 23:56:42 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 23:56:42 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0ccs32ey\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0ccs32ey\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_64 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_28 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_42 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_21  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,843,154\n",
      "Trainable params: 3,614,290\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   5/3189 [..............................] - ETA: 2:02 - loss: 2066955.3750  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0171s vs `on_train_batch_end` time: 0.0243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0171s vs `on_train_batch_end` time: 0.0243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 1: loss improved from inf to 2736504.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 104s 32ms/step - loss: 2736504.0000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 2: loss did not improve from 2736504.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 3: loss did not improve from 2736504.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 4: loss improved from 2736504.00000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.0000\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 5: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.5000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.2500\n",
      "Epoch 6: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736507.2500\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 7: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 8: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 9: loss improved from 2736503.00000 to 2736501.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736501.7500\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 10: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.2500\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 11: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 12: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 13: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 14: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 15: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.7500\n",
      "Epoch 16: loss improved from 2736501.75000 to 2736500.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736500.7500\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 17: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 18: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 19: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 20: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 00:30:52 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 00:30:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpzx5b06d6\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpzx5b06d6\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_67 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_29 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_43 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_22  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,843,154\n",
      "Trainable params: 3,614,290\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   5/3189 [..............................] - ETA: 2:04 - loss: 2427656.7500  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0193s vs `on_train_batch_end` time: 0.0223s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0193s vs `on_train_batch_end` time: 0.0223s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 1: loss improved from inf to 2736506.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 105s 32ms/step - loss: 2736506.2500\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 2: loss improved from 2736506.25000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 3: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 4: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 5: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.0000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 6: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.0000\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 7: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.0000\n",
      "Epoch 8/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736736.7500\n",
      "Epoch 8: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 9: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.0000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 10: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.0000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 11: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.7500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 12: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 13: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.2500\n",
      "Epoch 14: loss improved from 2736502.75000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.2500\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 15: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.5000\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 16: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 17: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 18: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.5000\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 19: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.7500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 20: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 01:05:03 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 01:05:03 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7ci9n146\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7ci9n146\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_70 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_30 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_44 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_23  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,843,154\n",
      "Trainable params: 3,614,290\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   5/3189 [..............................] - ETA: 2:02 - loss: 2640719.5000  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0199s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0199s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.0000\n",
      "Epoch 1: loss improved from inf to 2736507.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 103s 32ms/step - loss: 2736507.0000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 2: loss improved from 2736507.00000 to 2736501.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736501.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 3: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 4: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.5000\n",
      "Epoch 5: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736507.5000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736508.5000\n",
      "Epoch 6: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736508.5000\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 7: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 8: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 9: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 10: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 11: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.5000\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 12: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 13: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.5000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 14: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 15: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 16: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.5000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 17: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.2500\n",
      "Epoch 18: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.2500\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 19: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 20: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 01:39:08 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 01:39:08 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpu75eevox\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpu75eevox\\model\\data\\model\\assets\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "#MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "MODELS=[\"MobileNet\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=2\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\" vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet ajusted decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_49 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_23 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_37 (UpSamplin  (None, 150, 150, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 150, 150, 3)       30        \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_16  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,582,054\n",
      "Trainable params: 6,353,190\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 1: loss improved from inf to 2736504.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 87s 26ms/step - loss: 2736504.7500\n",
      "Epoch 2/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736403.5000\n",
      "Epoch 2: loss did not improve from 2736504.75000\n",
      "3189/3189 [==============================] - 62s 19ms/step - loss: 2736506.0000\n",
      "Epoch 3/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736846.5000\n",
      "Epoch 3: loss improved from 2736504.75000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 61s 19ms/step - loss: 2736503.7500\n",
      "Epoch 4/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736735.0000\n",
      "Epoch 4: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736506.0000\n",
      "Epoch 5/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736095.2500\n",
      "Epoch 5: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 61s 19ms/step - loss: 2736504.5000\n",
      "Epoch 6/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737142.2500\n",
      "Epoch 6: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 60s 19ms/step - loss: 2736510.2500\n",
      "Epoch 7/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736783.7500\n",
      "Epoch 7: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736506.7500\n",
      "Epoch 8/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736857.5000\n",
      "Epoch 8: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 60s 19ms/step - loss: 2736506.0000\n",
      "Epoch 9/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737106.7500\n",
      "Epoch 9: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736504.7500\n",
      "Epoch 10/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736363.5000\n",
      "Epoch 10: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736505.2500\n",
      "Epoch 11/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737205.7500\n",
      "Epoch 11: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736506.2500\n",
      "Epoch 12/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2734771.2500\n",
      "Epoch 12: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736505.2500\n",
      "Epoch 13/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735813.7500\n",
      "Epoch 13: loss improved from 2736503.75000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736502.7500\n",
      "Epoch 14/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736631.2500\n",
      "Epoch 14: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736503.5000\n",
      "Epoch 15/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736583.2500\n",
      "Epoch 15: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736508.2500\n",
      "Epoch 16/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736253.0000\n",
      "Epoch 16: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736506.7500\n",
      "Epoch 17/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736360.5000\n",
      "Epoch 17: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.2500\n",
      "Epoch 18/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736832.2500\n",
      "Epoch 18: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736508.0000\n",
      "Epoch 19/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736094.5000\n",
      "Epoch 19: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736507.0000\n",
      "Epoch 20/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736482.0000\n",
      "Epoch 20: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 22:22:58 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 22:22:58 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpp45wf6qs\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpp45wf6qs\\model\\data\\model\\assets\n",
      "2023/05/27 22:24:09 INFO mlflow.tracking.fluent: Experiment with name 'MobileNet_2 vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 20' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_52 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_24 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_38 (UpSamplin  (None, 150, 150, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 150, 150, 3)       30        \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_17  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,582,054\n",
      "Trainable params: 6,353,190\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   5/3189 [..............................] - ETA: 2:53 - loss: 2107624.2500  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0262s vs `on_train_batch_end` time: 0.0289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0262s vs `on_train_batch_end` time: 0.0289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735987.2500\n",
      "Epoch 1: loss improved from inf to 2736505.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 67s 21ms/step - loss: 2736505.7500\n",
      "Epoch 2/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736913.7500\n",
      "Epoch 2: loss improved from 2736505.75000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 3: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 4: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736504.0000\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 5: loss improved from 2736503.75000 to 2736501.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736501.7500\n",
      "Epoch 6/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736274.2500\n",
      "Epoch 6: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736503.0000\n",
      "Epoch 7/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736843.7500\n",
      "Epoch 7: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.0000\n",
      "Epoch 8/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736836.7500\n",
      "Epoch 8: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736507.2500\n",
      "Epoch 9/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2737105.5000\n",
      "Epoch 9: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 58s 18ms/step - loss: 2736507.0000\n",
      "Epoch 10/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736874.2500\n",
      "Epoch 10: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 11/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736455.5000\n",
      "Epoch 11: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 58s 18ms/step - loss: 2736506.2500\n",
      "Epoch 12/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735156.0000\n",
      "Epoch 12: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 58s 18ms/step - loss: 2736505.7500\n",
      "Epoch 13/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736617.5000\n",
      "Epoch 13: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.7500\n",
      "Epoch 14/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736192.2500\n",
      "Epoch 14: loss improved from 2736501.75000 to 2736501.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 58s 18ms/step - loss: 2736501.2500\n",
      "Epoch 15/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736930.5000\n",
      "Epoch 15: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736502.2500\n",
      "Epoch 16/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736718.5000\n",
      "Epoch 16: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 17/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736981.0000\n",
      "Epoch 17: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 18/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735648.0000\n",
      "Epoch 18: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.7500\n",
      "Epoch 19/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736965.2500\n",
      "Epoch 19: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.7500\n",
      "Epoch 20/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2737059.5000\n",
      "Epoch 20: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 22:43:39 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 22:43:39 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpj_uf3guk\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpj_uf3guk\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_55 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_25 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_39 (UpSamplin  (None, 150, 150, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 150, 150, 3)       30        \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_18  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,582,054\n",
      "Trainable params: 6,353,190\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736410.7500\n",
      "Epoch 1: loss improved from inf to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736502.7500\n",
      "Epoch 2/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736360.5000\n",
      "Epoch 2: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 3/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736249.5000\n",
      "Epoch 3: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736508.2500\n",
      "Epoch 4/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736824.7500\n",
      "Epoch 4: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.2500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 5: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.7500\n",
      "Epoch 6/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736958.7500\n",
      "Epoch 6: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 7/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736813.7500\n",
      "Epoch 7: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.5000\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 8: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.0000\n",
      "Epoch 9/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737018.7500\n",
      "Epoch 9: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.5000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 10: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n",
      "Epoch 11/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736041.2500\n",
      "Epoch 11: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.7500\n",
      "Epoch 12/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737322.5000\n",
      "Epoch 12: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736507.5000\n",
      "Epoch 13/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736946.5000\n",
      "Epoch 13: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 14/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736937.7500\n",
      "Epoch 14: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 15/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735879.7500\n",
      "Epoch 15: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.5000\n",
      "Epoch 16/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736520.5000\n",
      "Epoch 16: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.7500\n",
      "Epoch 17/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736618.0000\n",
      "Epoch 17: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n",
      "Epoch 18/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736152.7500\n",
      "Epoch 18: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 19/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736789.5000\n",
      "Epoch 19: loss improved from 2736502.75000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736502.2500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 20: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 23:03:09 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 23:03:09 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn5u4ewh9\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn5u4ewh9\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_58 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_26 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_40 (UpSamplin  (None, 150, 150, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 150, 150, 3)       30        \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_19  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,582,054\n",
      "Trainable params: 6,353,190\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 1: loss improved from inf to 2736506.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736506.7500\n",
      "Epoch 2/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736714.5000\n",
      "Epoch 2: loss improved from 2736506.75000 to 2736505.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.2500\n",
      "Epoch 3/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736042.7500\n",
      "Epoch 3: loss did not improve from 2736505.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736507.2500\n",
      "Epoch 4/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736010.7500\n",
      "Epoch 4: loss did not improve from 2736505.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736510.2500\n",
      "Epoch 5/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735576.5000\n",
      "Epoch 5: loss improved from 2736505.25000 to 2736504.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 6/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736393.0000\n",
      "Epoch 6: loss improved from 2736504.50000 to 2736503.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.5000\n",
      "Epoch 7/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736698.5000\n",
      "Epoch 7: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736508.5000\n",
      "Epoch 8/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736787.7500\n",
      "Epoch 8: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.7500\n",
      "Epoch 9/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735867.0000\n",
      "Epoch 9: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 10/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736091.7500\n",
      "Epoch 10: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 11/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735885.7500\n",
      "Epoch 11: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 12/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735742.7500\n",
      "Epoch 12: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n",
      "Epoch 13/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736645.7500\n",
      "Epoch 13: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 14/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736792.7500\n",
      "Epoch 14: loss improved from 2736503.50000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736502.2500\n",
      "Epoch 15/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736041.2500\n",
      "Epoch 15: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n",
      "Epoch 16/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735013.0000\n",
      "Epoch 16: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 17/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736426.0000\n",
      "Epoch 17: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.7500\n",
      "Epoch 18/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735979.2500\n",
      "Epoch 18: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.7500\n",
      "Epoch 19/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736333.0000\n",
      "Epoch 19: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736508.2500\n",
      "Epoch 20: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736508.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 23:22:33 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 23:22:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpeibxb_st\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpeibxb_st\\model\\data\\model\\assets\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "#MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "MODELS=[\"MobileNet_2\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=2\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\" vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "#MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "MODELS=[\"Transformer1\"]\n",
    "DROP_OUT=[0.05,0.1]\n",
    "RUNS=1\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\" vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting information from MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.entities import ViewType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = mlflow.search_experiments(view_type=ViewType.ALL)\n",
    "runs=[]\n",
    "for exp in experiments:\n",
    "    e_id=exp.experiment_id\n",
    "    run=mlflow.search_runs(experiment_ids=[e_id])\n",
    "    run[\"experiment_name\"]=exp.name\n",
    "    runs.append(run)\n",
    "runs=pd.concat(runs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs estructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.loss</th>\n",
       "      <th>params.shuffle</th>\n",
       "      <th>params.opt_name</th>\n",
       "      <th>params.use_multiprocessing</th>\n",
       "      <th>...</th>\n",
       "      <th>params.steps_per_epoch</th>\n",
       "      <th>params.opt_beta_2</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.log-model.history</th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>params.opt_amsgrad</th>\n",
       "      <th>tags.mlflow.autologging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e88da3699b9b4d59b1fb64aea2b36c83</td>\n",
       "      <td>428376191294209169</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///c:/Users/USUARIO/Desktop/DeepLearning/...</td>\n",
       "      <td>2023-05-28 18:39:33.303000+00:00</td>\n",
       "      <td>2023-05-28 19:11:12.486000+00:00</td>\n",
       "      <td>2678950.000</td>\n",
       "      <td>True</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999</td>\n",
       "      <td>USUARIO</td>\n",
       "      <td>[{\"run_id\": \"e88da3699b9b4d59b1fb64aea2b36c83\"...</td>\n",
       "      <td>melodic-doe-934</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...</td>\n",
       "      <td>dnn3_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54bd0394bcbd468db399a95ee5f7da51</td>\n",
       "      <td>817120816286958842</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///c:/Users/USUARIO/Desktop/DeepLearning/...</td>\n",
       "      <td>2023-05-28 18:03:47.530000+00:00</td>\n",
       "      <td>2023-05-28 18:39:32.660000+00:00</td>\n",
       "      <td>2200215.250</td>\n",
       "      <td>True</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999</td>\n",
       "      <td>USUARIO</td>\n",
       "      <td>[{\"run_id\": \"54bd0394bcbd468db399a95ee5f7da51\"...</td>\n",
       "      <td>luminous-robin-979</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...</td>\n",
       "      <td>dnn3_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e74d2d87fe2d48a494fd1bf82170c273</td>\n",
       "      <td>183422633024821028</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///c:/Users/USUARIO/Desktop/DeepLearning/...</td>\n",
       "      <td>2023-05-28 17:30:20.296000+00:00</td>\n",
       "      <td>2023-05-28 18:03:47.104000+00:00</td>\n",
       "      <td>2459763.250</td>\n",
       "      <td>True</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999</td>\n",
       "      <td>USUARIO</td>\n",
       "      <td>[{\"run_id\": \"e74d2d87fe2d48a494fd1bf82170c273\"...</td>\n",
       "      <td>bold-jay-32</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>377535757391431788</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///c:/Users/USUARIO/Desktop/DeepLearning/...</td>\n",
       "      <td>2023-05-28 16:51:42.075000+00:00</td>\n",
       "      <td>2023-05-28 17:30:19.362000+00:00</td>\n",
       "      <td>1272539.125</td>\n",
       "      <td>True</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999</td>\n",
       "      <td>USUARIO</td>\n",
       "      <td>[{\"run_id\": \"ff8bb6a7956d464ca77cf32be7adf88b\"...</td>\n",
       "      <td>overjoyed-cub-796</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6c5b475716224de99333e82f4935f763</td>\n",
       "      <td>133066846081328314</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///c:/Users/USUARIO/Desktop/DeepLearning/...</td>\n",
       "      <td>2023-05-28 15:42:37.148000+00:00</td>\n",
       "      <td>2023-05-28 16:05:39.887000+00:00</td>\n",
       "      <td>2459603.500</td>\n",
       "      <td>True</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.999</td>\n",
       "      <td>USUARIO</td>\n",
       "      <td>[{\"run_id\": \"6c5b475716224de99333e82f4935f763\"...</td>\n",
       "      <td>powerful-gnu-547</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...</td>\n",
       "      <td>dnn4vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id       experiment_id    status  \\\n",
       "0  e88da3699b9b4d59b1fb64aea2b36c83  428376191294209169  FINISHED   \n",
       "0  54bd0394bcbd468db399a95ee5f7da51  817120816286958842  FINISHED   \n",
       "0  e74d2d87fe2d48a494fd1bf82170c273  183422633024821028  FINISHED   \n",
       "0  ff8bb6a7956d464ca77cf32be7adf88b  377535757391431788  FINISHED   \n",
       "0  6c5b475716224de99333e82f4935f763  133066846081328314  FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  file:///c:/Users/USUARIO/Desktop/DeepLearning/...   \n",
       "0  file:///c:/Users/USUARIO/Desktop/DeepLearning/...   \n",
       "0  file:///c:/Users/USUARIO/Desktop/DeepLearning/...   \n",
       "0  file:///c:/Users/USUARIO/Desktop/DeepLearning/...   \n",
       "0  file:///c:/Users/USUARIO/Desktop/DeepLearning/...   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2023-05-28 18:39:33.303000+00:00 2023-05-28 19:11:12.486000+00:00   \n",
       "0 2023-05-28 18:03:47.530000+00:00 2023-05-28 18:39:32.660000+00:00   \n",
       "0 2023-05-28 17:30:20.296000+00:00 2023-05-28 18:03:47.104000+00:00   \n",
       "0 2023-05-28 16:51:42.075000+00:00 2023-05-28 17:30:19.362000+00:00   \n",
       "0 2023-05-28 15:42:37.148000+00:00 2023-05-28 16:05:39.887000+00:00   \n",
       "\n",
       "   metrics.loss params.shuffle params.opt_name params.use_multiprocessing  \\\n",
       "0   2678950.000           True          Adamax                      False   \n",
       "0   2200215.250           True          Adamax                      False   \n",
       "0   2459763.250           True          Adamax                      False   \n",
       "0   1272539.125           True          Adamax                      False   \n",
       "0   2459603.500           True          Adamax                      False   \n",
       "\n",
       "   ... params.steps_per_epoch params.opt_beta_2 tags.mlflow.user  \\\n",
       "0  ...                   None             0.999          USUARIO   \n",
       "0  ...                   None             0.999          USUARIO   \n",
       "0  ...                   None             0.999          USUARIO   \n",
       "0  ...                   None             0.999          USUARIO   \n",
       "0  ...                   None             0.999          USUARIO   \n",
       "\n",
       "                       tags.mlflow.log-model.history tags.mlflow.runName  \\\n",
       "0  [{\"run_id\": \"e88da3699b9b4d59b1fb64aea2b36c83\"...     melodic-doe-934   \n",
       "0  [{\"run_id\": \"54bd0394bcbd468db399a95ee5f7da51\"...  luminous-robin-979   \n",
       "0  [{\"run_id\": \"e74d2d87fe2d48a494fd1bf82170c273\"...         bold-jay-32   \n",
       "0  [{\"run_id\": \"ff8bb6a7956d464ca77cf32be7adf88b\"...   overjoyed-cub-796   \n",
       "0  [{\"run_id\": \"6c5b475716224de99333e82f4935f763\"...    powerful-gnu-547   \n",
       "\n",
       "  tags.mlflow.source.type                            tags.mlflow.source.name  \\\n",
       "0                   LOCAL  c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...   \n",
       "0                   LOCAL  c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...   \n",
       "0                   LOCAL  c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...   \n",
       "0                   LOCAL  c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...   \n",
       "0                   LOCAL  c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\si...   \n",
       "\n",
       "                                     experiment_name params.opt_amsgrad  \\\n",
       "0  dnn3_stacked2vector_min_size 2500 n_samples 20...                NaN   \n",
       "0  dnn3_stacked2vector_min_size 2500 n_samples 20...                NaN   \n",
       "0  dnn4_stacked2vector_min_size 2500 n_samples 20...                NaN   \n",
       "0  dnn4_stacked2vector_min_size 2500 n_samples 20...                NaN   \n",
       "0  dnn4vector_min_size 2500 n_samples 2000 dropou...                NaN   \n",
       "\n",
       "  tags.mlflow.autologging  \n",
       "0                     NaN  \n",
       "0                     NaN  \n",
       "0                     NaN  \n",
       "0                     NaN  \n",
       "0                     NaN  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time',\n",
       "       'end_time', 'metrics.loss', 'params.shuffle', 'params.opt_name',\n",
       "       'params.use_multiprocessing', 'params.batch_size', 'params.opt_decay',\n",
       "       'params.validation_steps', 'params.class_weight', 'params.opt_beta_1',\n",
       "       'params.sample_weight', 'params.initial_epoch', 'params.max_queue_size',\n",
       "       'params.validation_split', 'params.opt_learning_rate',\n",
       "       'params.opt_epsilon', 'params.epochs', 'params.workers',\n",
       "       'params.validation_batch_size', 'params.validation_freq',\n",
       "       'params.steps_per_epoch', 'params.opt_beta_2', 'tags.mlflow.user',\n",
       "       'tags.mlflow.log-model.history', 'tags.mlflow.runName',\n",
       "       'tags.mlflow.source.type', 'tags.mlflow.source.name', 'experiment_name',\n",
       "       'params.opt_amsgrad', 'tags.mlflow.autologging'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference table for making easier to track experiments by name instead by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MobileNet vector_min_size 2500 n_samples 2000 ...</td>\n",
       "      <td>4472f60e3cb54d76aefb67bcdefa0ee0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MobileNet vector_min_size 2500 n_samples 2000 ...</td>\n",
       "      <td>5945ba3415fd4fea9ca7955ecfbfc108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MobileNet vector_min_size 2500 n_samples 2000 ...</td>\n",
       "      <td>b436ef42b5004d33b2c6a611c31a8908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MobileNet vector_min_size 2500 n_samples 2000 ...</td>\n",
       "      <td>df6f0ffd04274475b1e7dfa36f5c02eb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MobileNet vector_min_size 2500 n_samples 2000 ...</td>\n",
       "      <td>52021dabac544d62a3cf256cd3823bdc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     experiment_name  \\\n",
       "0  MobileNet vector_min_size 2500 n_samples 2000 ...   \n",
       "1  MobileNet vector_min_size 2500 n_samples 2000 ...   \n",
       "2  MobileNet vector_min_size 2500 n_samples 2000 ...   \n",
       "3  MobileNet vector_min_size 2500 n_samples 2000 ...   \n",
       "4  MobileNet vector_min_size 2500 n_samples 2000 ...   \n",
       "\n",
       "                             run_id  experiment_id  \n",
       "0  4472f60e3cb54d76aefb67bcdefa0ee0              1  \n",
       "1  5945ba3415fd4fea9ca7955ecfbfc108              1  \n",
       "2  b436ef42b5004d33b2c6a611c31a8908              1  \n",
       "3  df6f0ffd04274475b1e7dfa36f5c02eb              1  \n",
       "4  52021dabac544d62a3cf256cd3823bdc              1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_experiments=runs.groupby([\"experiment_name\",\"run_id\"]).agg({\"experiment_id\":\"count\"}).reset_index()\n",
    "ref_experiments.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting best experiments by loss\n",
    "As we can see the dnn4 configuration has the best performance, followed by our custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>run_id</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>dnn4vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>f58987e01c624d48ba905c8aa4397406</td>\n",
       "      <td>1.033604e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>dnn4vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>5b4215c79e9b4c049c37477bc6affad7</td>\n",
       "      <td>1.046480e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>dnn4vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>aca9a94dc968481598e70225118bbfd9</td>\n",
       "      <td>1.088232e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>1.272539e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>1.435283e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>ba1cfe393b3f42d1b361c3a226669416</td>\n",
       "      <td>1.447092e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>dnn3vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>d2d8ae3f33364a4bba6f36f537c523b0</td>\n",
       "      <td>1.898246e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>dnn3vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>09a558c7b4c04754a74b49e54793bcee</td>\n",
       "      <td>1.901299e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>dnn3vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>68d38fd565774adb88a0eabd3fb947fe</td>\n",
       "      <td>1.906601e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>VGG16 vector_min_size 2500 n_samples 2000 drop...</td>\n",
       "      <td>933e0af5cbef455fa447f2b0af049c17</td>\n",
       "      <td>2.053982e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      experiment_name  \\\n",
       "78  dnn4vector_min_size 2500 n_samples 2000 dropou...   \n",
       "76  dnn4vector_min_size 2500 n_samples 2000 dropou...   \n",
       "77  dnn4vector_min_size 2500 n_samples 2000 dropou...   \n",
       "74  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "32  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "33  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "70  dnn3vector_min_size 2500 n_samples 2000 dropou...   \n",
       "68  dnn3vector_min_size 2500 n_samples 2000 dropou...   \n",
       "69  dnn3vector_min_size 2500 n_samples 2000 dropou...   \n",
       "21  VGG16 vector_min_size 2500 n_samples 2000 drop...   \n",
       "\n",
       "                              run_id           min  \n",
       "78  f58987e01c624d48ba905c8aa4397406  1.033604e+06  \n",
       "76  5b4215c79e9b4c049c37477bc6affad7  1.046480e+06  \n",
       "77  aca9a94dc968481598e70225118bbfd9  1.088232e+06  \n",
       "74  ff8bb6a7956d464ca77cf32be7adf88b  1.272539e+06  \n",
       "32  7de0e722f0b94c8fa6eed03d15f84aae  1.435283e+06  \n",
       "33  ba1cfe393b3f42d1b361c3a226669416  1.447092e+06  \n",
       "70  d2d8ae3f33364a4bba6f36f537c523b0  1.898246e+06  \n",
       "68  09a558c7b4c04754a74b49e54793bcee  1.901299e+06  \n",
       "69  68d38fd565774adb88a0eabd3fb947fe  1.906601e+06  \n",
       "21  933e0af5cbef455fa447f2b0af049c17  2.053982e+06  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_experiments=runs.groupby([\"experiment_name\",\"run_id\"])[\"metrics.loss\"].aggregate(['min']).reset_index().sort_values(\"min\").head(10)\n",
    "top10_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_experiments.to_csv(\"top10_experiments.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped results by experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_arquitecture=runs.groupby([\"experiment_name\"])[\"metrics.loss\"].aggregate(['min','max','mean']).reset_index().sort_values(\"min\").head(24)\n",
    "group_by_arquitecture.to_csv(\"group_by_arquitecture.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>experiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e88da3699b9b4d59b1fb64aea2b36c83</td>\n",
       "      <td>loss</td>\n",
       "      <td>2732290.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1685299243878</td>\n",
       "      <td>dnn3_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e88da3699b9b4d59b1fb64aea2b36c83</td>\n",
       "      <td>loss</td>\n",
       "      <td>2727802.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1685299263579</td>\n",
       "      <td>dnn3_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e88da3699b9b4d59b1fb64aea2b36c83</td>\n",
       "      <td>loss</td>\n",
       "      <td>2726745.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1685299279912</td>\n",
       "      <td>dnn3_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e88da3699b9b4d59b1fb64aea2b36c83</td>\n",
       "      <td>loss</td>\n",
       "      <td>2725481.75</td>\n",
       "      <td>3</td>\n",
       "      <td>1685299295843</td>\n",
       "      <td>dnn3_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e88da3699b9b4d59b1fb64aea2b36c83</td>\n",
       "      <td>loss</td>\n",
       "      <td>2724526.75</td>\n",
       "      <td>4</td>\n",
       "      <td>1685299311839</td>\n",
       "      <td>dnn3_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>bb507375edfb42cb8651282ab45e7505</td>\n",
       "      <td>loss</td>\n",
       "      <td>2304748.00</td>\n",
       "      <td>95</td>\n",
       "      <td>1685149186162</td>\n",
       "      <td>naivevector_min_size 2500 n_samples 2000 dropo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>bb507375edfb42cb8651282ab45e7505</td>\n",
       "      <td>loss</td>\n",
       "      <td>2304290.25</td>\n",
       "      <td>96</td>\n",
       "      <td>1685149193353</td>\n",
       "      <td>naivevector_min_size 2500 n_samples 2000 dropo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>bb507375edfb42cb8651282ab45e7505</td>\n",
       "      <td>loss</td>\n",
       "      <td>2304467.75</td>\n",
       "      <td>97</td>\n",
       "      <td>1685149200643</td>\n",
       "      <td>naivevector_min_size 2500 n_samples 2000 dropo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>bb507375edfb42cb8651282ab45e7505</td>\n",
       "      <td>loss</td>\n",
       "      <td>2303848.75</td>\n",
       "      <td>98</td>\n",
       "      <td>1685149207974</td>\n",
       "      <td>naivevector_min_size 2500 n_samples 2000 dropo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>bb507375edfb42cb8651282ab45e7505</td>\n",
       "      <td>loss</td>\n",
       "      <td>2304272.50</td>\n",
       "      <td>99</td>\n",
       "      <td>1685149215112</td>\n",
       "      <td>naivevector_min_size 2500 n_samples 2000 dropo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3800 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                run_id   key       value  step      timestamp  \\\n",
       "0     e88da3699b9b4d59b1fb64aea2b36c83  loss  2732290.75     0  1685299243878   \n",
       "1     e88da3699b9b4d59b1fb64aea2b36c83  loss  2727802.50     1  1685299263579   \n",
       "2     e88da3699b9b4d59b1fb64aea2b36c83  loss  2726745.00     2  1685299279912   \n",
       "3     e88da3699b9b4d59b1fb64aea2b36c83  loss  2725481.75     3  1685299295843   \n",
       "4     e88da3699b9b4d59b1fb64aea2b36c83  loss  2724526.75     4  1685299311839   \n",
       "...                                ...   ...         ...   ...            ...   \n",
       "3795  bb507375edfb42cb8651282ab45e7505  loss  2304748.00    95  1685149186162   \n",
       "3796  bb507375edfb42cb8651282ab45e7505  loss  2304290.25    96  1685149193353   \n",
       "3797  bb507375edfb42cb8651282ab45e7505  loss  2304467.75    97  1685149200643   \n",
       "3798  bb507375edfb42cb8651282ab45e7505  loss  2303848.75    98  1685149207974   \n",
       "3799  bb507375edfb42cb8651282ab45e7505  loss  2304272.50    99  1685149215112   \n",
       "\n",
       "                                        experiment_name  experiment_id  \n",
       "0     dnn3_stacked2vector_min_size 2500 n_samples 20...              1  \n",
       "1     dnn3_stacked2vector_min_size 2500 n_samples 20...              1  \n",
       "2     dnn3_stacked2vector_min_size 2500 n_samples 20...              1  \n",
       "3     dnn3_stacked2vector_min_size 2500 n_samples 20...              1  \n",
       "4     dnn3_stacked2vector_min_size 2500 n_samples 20...              1  \n",
       "...                                                 ...            ...  \n",
       "3795  naivevector_min_size 2500 n_samples 2000 dropo...              1  \n",
       "3796  naivevector_min_size 2500 n_samples 2000 dropo...              1  \n",
       "3797  naivevector_min_size 2500 n_samples 2000 dropo...              1  \n",
       "3798  naivevector_min_size 2500 n_samples 2000 dropo...              1  \n",
       "3799  naivevector_min_size 2500 n_samples 2000 dropo...              1  \n",
       "\n",
       "[3800 rows x 7 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_run=pd.read_csv(\"loss_per_run.csv\")\n",
    "loss_per_run=loss_per_run.merge(ref_experiments,how=\"inner\", on=\"run_id\")\n",
    "loss_per_run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only plotting best experiments vs baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>experiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2695908.250</td>\n",
       "      <td>0</td>\n",
       "      <td>1685293079742</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2563457.750</td>\n",
       "      <td>1</td>\n",
       "      <td>1685293107413</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2441821.250</td>\n",
       "      <td>2</td>\n",
       "      <td>1685293128122</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2352661.500</td>\n",
       "      <td>3</td>\n",
       "      <td>1685293147019</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2276490.500</td>\n",
       "      <td>4</td>\n",
       "      <td>1685293166024</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1441792.750</td>\n",
       "      <td>95</td>\n",
       "      <td>1685159091526</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1439988.500</td>\n",
       "      <td>96</td>\n",
       "      <td>1685159109885</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1439147.750</td>\n",
       "      <td>97</td>\n",
       "      <td>1685159128067</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1436770.250</td>\n",
       "      <td>98</td>\n",
       "      <td>1685159147817</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1435283.125</td>\n",
       "      <td>99</td>\n",
       "      <td>1685159168138</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                run_id   key        value  step  \\\n",
       "300   ff8bb6a7956d464ca77cf32be7adf88b  loss  2695908.250     0   \n",
       "301   ff8bb6a7956d464ca77cf32be7adf88b  loss  2563457.750     1   \n",
       "302   ff8bb6a7956d464ca77cf32be7adf88b  loss  2441821.250     2   \n",
       "303   ff8bb6a7956d464ca77cf32be7adf88b  loss  2352661.500     3   \n",
       "304   ff8bb6a7956d464ca77cf32be7adf88b  loss  2276490.500     4   \n",
       "...                                ...   ...          ...   ...   \n",
       "3095  7de0e722f0b94c8fa6eed03d15f84aae  loss  1441792.750    95   \n",
       "3096  7de0e722f0b94c8fa6eed03d15f84aae  loss  1439988.500    96   \n",
       "3097  7de0e722f0b94c8fa6eed03d15f84aae  loss  1439147.750    97   \n",
       "3098  7de0e722f0b94c8fa6eed03d15f84aae  loss  1436770.250    98   \n",
       "3099  7de0e722f0b94c8fa6eed03d15f84aae  loss  1435283.125    99   \n",
       "\n",
       "          timestamp                                    experiment_name  \\\n",
       "300   1685293079742  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "301   1685293107413  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "302   1685293128122  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "303   1685293147019  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "304   1685293166024  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "...             ...                                                ...   \n",
       "3095  1685159091526  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "3096  1685159109885  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "3097  1685159128067  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "3098  1685159147817  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "3099  1685159168138  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "\n",
       "      experiment_id  \n",
       "300               1  \n",
       "301               1  \n",
       "302               1  \n",
       "303               1  \n",
       "304               1  \n",
       "...             ...  \n",
       "3095              1  \n",
       "3096              1  \n",
       "3097              1  \n",
       "3098              1  \n",
       "3099              1  \n",
       "\n",
       "[1010 rows x 7 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_runs=top10_experiments.run_id.values.tolist()\n",
    "selected_runs.append('8a09ee1a20f14361babf17bb7900decd') # naive model\n",
    "runs_to_plot=loss_per_run[loss_per_run[\"run_id\"].isin(selected_runs)]\n",
    "runs_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot=runs_to_plot.groupby([\"experiment_name\",\"run_id\"]).agg({\"value\":\"min\"}).reset_index().sort_values([\"experiment_name\",\"value\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually choosing best per experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot=loss_per_run[loss_per_run[\"run_id\"].isin(['7de0e722f0b94c8fa6eed03d15f84aae'\n",
    ",'68d38fd565774adb88a0eabd3fb947fe'\n",
    ",'ff8bb6a7956d464ca77cf32be7adf88b'\n",
    ",'f58987e01c624d48ba905c8aa4397406'\n",
    ",'8a09ee1a20f14361babf17bb7900decd'\n",
    ",'933e0af5cbef455fa447f2b0af049c17'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>experiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2695908.250</td>\n",
       "      <td>0</td>\n",
       "      <td>1685293079742</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2563457.750</td>\n",
       "      <td>1</td>\n",
       "      <td>1685293107413</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2441821.250</td>\n",
       "      <td>2</td>\n",
       "      <td>1685293128122</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2352661.500</td>\n",
       "      <td>3</td>\n",
       "      <td>1685293147019</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>loss</td>\n",
       "      <td>2276490.500</td>\n",
       "      <td>4</td>\n",
       "      <td>1685293166024</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1441792.750</td>\n",
       "      <td>95</td>\n",
       "      <td>1685159091526</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1439988.500</td>\n",
       "      <td>96</td>\n",
       "      <td>1685159109885</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1439147.750</td>\n",
       "      <td>97</td>\n",
       "      <td>1685159128067</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1436770.250</td>\n",
       "      <td>98</td>\n",
       "      <td>1685159147817</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>loss</td>\n",
       "      <td>1435283.125</td>\n",
       "      <td>99</td>\n",
       "      <td>1685159168138</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                run_id   key        value  step  \\\n",
       "300   ff8bb6a7956d464ca77cf32be7adf88b  loss  2695908.250     0   \n",
       "301   ff8bb6a7956d464ca77cf32be7adf88b  loss  2563457.750     1   \n",
       "302   ff8bb6a7956d464ca77cf32be7adf88b  loss  2441821.250     2   \n",
       "303   ff8bb6a7956d464ca77cf32be7adf88b  loss  2352661.500     3   \n",
       "304   ff8bb6a7956d464ca77cf32be7adf88b  loss  2276490.500     4   \n",
       "...                                ...   ...          ...   ...   \n",
       "3095  7de0e722f0b94c8fa6eed03d15f84aae  loss  1441792.750    95   \n",
       "3096  7de0e722f0b94c8fa6eed03d15f84aae  loss  1439988.500    96   \n",
       "3097  7de0e722f0b94c8fa6eed03d15f84aae  loss  1439147.750    97   \n",
       "3098  7de0e722f0b94c8fa6eed03d15f84aae  loss  1436770.250    98   \n",
       "3099  7de0e722f0b94c8fa6eed03d15f84aae  loss  1435283.125    99   \n",
       "\n",
       "          timestamp                                    experiment_name  \\\n",
       "300   1685293079742  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "301   1685293107413  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "302   1685293128122  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "303   1685293147019  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "304   1685293166024  dnn4_stacked2vector_min_size 2500 n_samples 20...   \n",
       "...             ...                                                ...   \n",
       "3095  1685159091526  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "3096  1685159109885  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "3097  1685159128067  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "3098  1685159147817  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "3099  1685159168138  cnn1vector_min_size 2500 n_samples 2000 dropou...   \n",
       "\n",
       "      experiment_id  \n",
       "300               1  \n",
       "301               1  \n",
       "302               1  \n",
       "303               1  \n",
       "304               1  \n",
       "...             ...  \n",
       "3095              1  \n",
       "3096              1  \n",
       "3097              1  \n",
       "3098              1  \n",
       "3099              1  \n",
       "\n",
       "[510 rows x 7 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68d38fd565774adb88a0eabd3fb947fe</td>\n",
       "      <td>dnn3vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7de0e722f0b94c8fa6eed03d15f84aae</td>\n",
       "      <td>cnn1vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8a09ee1a20f14361babf17bb7900decd</td>\n",
       "      <td>naivevector_min_size 2500 n_samples 2000 dropo...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>933e0af5cbef455fa447f2b0af049c17</td>\n",
       "      <td>VGG16 vector_min_size 2500 n_samples 2000 drop...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f58987e01c624d48ba905c8aa4397406</td>\n",
       "      <td>dnn4vector_min_size 2500 n_samples 2000 dropou...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ff8bb6a7956d464ca77cf32be7adf88b</td>\n",
       "      <td>dnn4_stacked2vector_min_size 2500 n_samples 20...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id  \\\n",
       "0  68d38fd565774adb88a0eabd3fb947fe   \n",
       "1  7de0e722f0b94c8fa6eed03d15f84aae   \n",
       "2  8a09ee1a20f14361babf17bb7900decd   \n",
       "3  933e0af5cbef455fa447f2b0af049c17   \n",
       "4  f58987e01c624d48ba905c8aa4397406   \n",
       "5  ff8bb6a7956d464ca77cf32be7adf88b   \n",
       "\n",
       "                                     experiment_name  key  \n",
       "0  dnn3vector_min_size 2500 n_samples 2000 dropou...  100  \n",
       "1  cnn1vector_min_size 2500 n_samples 2000 dropou...  100  \n",
       "2  naivevector_min_size 2500 n_samples 2000 dropo...  100  \n",
       "3  VGG16 vector_min_size 2500 n_samples 2000 drop...   10  \n",
       "4  dnn4vector_min_size 2500 n_samples 2000 dropou...  100  \n",
       "5  dnn4_stacked2vector_min_size 2500 n_samples 20...  100  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_to_plot.groupby([\"run_id\",\"experiment_name\"])[\"key\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>experiment_name</th>\n",
       "      <th>step</th>\n",
       "      <th>VGG16 vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 20</th>\n",
       "      <th>cnn1vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100</th>\n",
       "      <th>dnn3vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100</th>\n",
       "      <th>dnn4_stacked2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100</th>\n",
       "      <th>dnn4vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100</th>\n",
       "      <th>naivevector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2653176.00</td>\n",
       "      <td>2736508.500</td>\n",
       "      <td>2688449.750</td>\n",
       "      <td>2695908.250</td>\n",
       "      <td>2.490334e+06</td>\n",
       "      <td>2602878.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2497893.25</td>\n",
       "      <td>2736507.000</td>\n",
       "      <td>2598079.000</td>\n",
       "      <td>2563457.750</td>\n",
       "      <td>2.172394e+06</td>\n",
       "      <td>2449336.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2390098.00</td>\n",
       "      <td>2736509.250</td>\n",
       "      <td>2535251.500</td>\n",
       "      <td>2441821.250</td>\n",
       "      <td>2.013391e+06</td>\n",
       "      <td>2414382.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2312510.25</td>\n",
       "      <td>2736503.000</td>\n",
       "      <td>2489790.000</td>\n",
       "      <td>2352661.500</td>\n",
       "      <td>1.901825e+06</td>\n",
       "      <td>2396892.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2249751.25</td>\n",
       "      <td>2736502.500</td>\n",
       "      <td>2454005.500</td>\n",
       "      <td>2276490.500</td>\n",
       "      <td>1.815171e+06</td>\n",
       "      <td>2383863.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1441792.750</td>\n",
       "      <td>1911151.000</td>\n",
       "      <td>1279901.125</td>\n",
       "      <td>1.040519e+06</td>\n",
       "      <td>2304547.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1439988.500</td>\n",
       "      <td>1909335.375</td>\n",
       "      <td>1278328.750</td>\n",
       "      <td>1.037752e+06</td>\n",
       "      <td>2304476.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1439147.750</td>\n",
       "      <td>1908730.000</td>\n",
       "      <td>1277410.000</td>\n",
       "      <td>1.038658e+06</td>\n",
       "      <td>2303230.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1436770.250</td>\n",
       "      <td>1907623.875</td>\n",
       "      <td>1272204.125</td>\n",
       "      <td>1.035480e+06</td>\n",
       "      <td>2303352.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1435283.125</td>\n",
       "      <td>1906601.375</td>\n",
       "      <td>1272539.125</td>\n",
       "      <td>1.033604e+06</td>\n",
       "      <td>2303524.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "experiment_name  step  \\\n",
       "0                   0   \n",
       "1                   1   \n",
       "2                   2   \n",
       "3                   3   \n",
       "4                   4   \n",
       "..                ...   \n",
       "95                 95   \n",
       "96                 96   \n",
       "97                 97   \n",
       "98                 98   \n",
       "99                 99   \n",
       "\n",
       "experiment_name  VGG16 vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 20  \\\n",
       "0                                                       2653176.00                 \n",
       "1                                                       2497893.25                 \n",
       "2                                                       2390098.00                 \n",
       "3                                                       2312510.25                 \n",
       "4                                                       2249751.25                 \n",
       "..                                                             ...                 \n",
       "95                                                             NaN                 \n",
       "96                                                             NaN                 \n",
       "97                                                             NaN                 \n",
       "98                                                             NaN                 \n",
       "99                                                             NaN                 \n",
       "\n",
       "experiment_name  cnn1vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100  \\\n",
       "0                                                      2736508.500                \n",
       "1                                                      2736507.000                \n",
       "2                                                      2736509.250                \n",
       "3                                                      2736503.000                \n",
       "4                                                      2736502.500                \n",
       "..                                                             ...                \n",
       "95                                                     1441792.750                \n",
       "96                                                     1439988.500                \n",
       "97                                                     1439147.750                \n",
       "98                                                     1436770.250                \n",
       "99                                                     1435283.125                \n",
       "\n",
       "experiment_name  dnn3vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100  \\\n",
       "0                                                      2688449.750                \n",
       "1                                                      2598079.000                \n",
       "2                                                      2535251.500                \n",
       "3                                                      2489790.000                \n",
       "4                                                      2454005.500                \n",
       "..                                                             ...                \n",
       "95                                                     1911151.000                \n",
       "96                                                     1909335.375                \n",
       "97                                                     1908730.000                \n",
       "98                                                     1907623.875                \n",
       "99                                                     1906601.375                \n",
       "\n",
       "experiment_name  dnn4_stacked2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100  \\\n",
       "0                                                      2695908.250                         \n",
       "1                                                      2563457.750                         \n",
       "2                                                      2441821.250                         \n",
       "3                                                      2352661.500                         \n",
       "4                                                      2276490.500                         \n",
       "..                                                             ...                         \n",
       "95                                                     1279901.125                         \n",
       "96                                                     1278328.750                         \n",
       "97                                                     1277410.000                         \n",
       "98                                                     1272204.125                         \n",
       "99                                                     1272539.125                         \n",
       "\n",
       "experiment_name  dnn4vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100  \\\n",
       "0                                                     2.490334e+06                \n",
       "1                                                     2.172394e+06                \n",
       "2                                                     2.013391e+06                \n",
       "3                                                     1.901825e+06                \n",
       "4                                                     1.815171e+06                \n",
       "..                                                             ...                \n",
       "95                                                    1.040519e+06                \n",
       "96                                                    1.037752e+06                \n",
       "97                                                    1.038658e+06                \n",
       "98                                                    1.035480e+06                \n",
       "99                                                    1.033604e+06                \n",
       "\n",
       "experiment_name  naivevector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100  \n",
       "0                                                       2602878.50                \n",
       "1                                                       2449336.00                \n",
       "2                                                       2414382.25                \n",
       "3                                                       2396892.25                \n",
       "4                                                       2383863.75                \n",
       "..                                                             ...                \n",
       "95                                                      2304547.75                \n",
       "96                                                      2304476.75                \n",
       "97                                                      2303230.75                \n",
       "98                                                      2303352.50                \n",
       "99                                                      2303524.75                \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_Results=runs_to_plot.pivot(index=\"step\",columns='experiment_name', values='value').reset_index()\n",
    "pivot_Results\n",
    "# pivot_Results.rename({\"02d80ee38ff247779470c4a509a7d480\":\"dnn3_DropOut 0.5\"\n",
    "#                       \"09a558c7b4c04754a74b49e54793bcee\"\n",
    "                      \n",
    "                      \n",
    "#                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VGG16 vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 20',\n",
       " 'cnn1vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100',\n",
       " 'dnn3vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100',\n",
       " 'dnn4_stacked2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100',\n",
       " 'dnn4vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100',\n",
       " 'naivevector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_Results.columns.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.clf()\n",
    "pivot_Results.plot(x=\"step\", y=pivot_Results.columns.tolist()[1:])\n",
    "plt.title('Model Reconstruction Loss')\n",
    "labels=[\"VGG16 Dout=0.5\",\"CNN1  Dout=0.5\",\"DNN3 Dout=0.1\",\"DNN4_Stacked Dout=0.1\",\"DNN4 Dout=0.1\",\"Naive/BaseLine\"]\n",
    "plt.legend(labels=labels, bbox_to_anchor=(1,1.025), loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
