{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.io.wavfile import write\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=r'wavs\\clean\\clnsp0.wav'\n",
    "ms=read(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176320,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampFreq, snd =read(r'wavs\\clean\\clnsp0.wav')\n",
    "write('test-2.wav',sampFreq,snd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, numpy.ndarray)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampFreq),type(snd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def create_data_from_dir(dir_path,top_x=200,vector_min_size=5000):\n",
    "    #dir_path directorio o folder donde estan los wavs\n",
    "    # top_x opcional cuantos archivos maximo desea usar, dejar vacio para usarlos todos\n",
    "    dir = os.listdir(dir_path)\n",
    "    snd_list=[]\n",
    "    s_rates=[]\n",
    "    for i, file in enumerate(dir):\n",
    "        if i<=top_x:\n",
    "            input_file_path = os.path.join(dir_path, file)\n",
    "            sampFreq, snd =read(input_file_path)\n",
    "            num_batches=ceil(snd.shape[0]/vector_min_size) if snd.shape[0]>vector_min_size else 1\n",
    "            ms=np.resize(snd,(vector_min_size*num_batches))\n",
    "            batches=np.hsplit(ms,num_batches)\n",
    "            for batch in batches:\n",
    "                snd_list.append(batch)\n",
    "                s_rates.append(sampFreq)\n",
    "    return snd_list,s_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_min_size=2500\n",
    "n_samples=2000\n",
    "clean_specs,clean_s_rates=create_data_from_dir(r'wavs\\clean',top_x=n_samples,vector_min_size=vector_min_size)\n",
    "noisy_specs,noisy_s_rates=create_data_from_dir(r'wavs\\noisy',top_x=n_samples,vector_min_size=vector_min_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=min(len(clean_specs),len(noisy_specs))\n",
    "clean_specs=clean_specs[:cap]\n",
    "noisy_specs=noisy_specs[:cap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102036, 102036)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_specs),len(noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_specs=np.array(clean_specs)\n",
    "noisy_specs=np.array(noisy_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np   \n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(noisy_specs, clean_specs, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Dense,Flatten,Reshape,InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from PIL import Image\n",
    "import re\n",
    "from math import sqrt\n",
    "from tensorflow.keras.applications import VGG16,ResNet50,MobileNet\n",
    "from tensorflow.keras import layers, Model, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs):\n",
    "    my_callbacks = [\n",
    "        #tf.keras.callbacks.ModelCheckpoint(filepath='callbacks/model.{epoch:02d}.h5')\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "        #filepath='callbacks/model-{epoch:02d}-{loss:2f}.h5'\n",
    "        filepath='callbacks/model.h5'\n",
    "        ,monitor='loss'\n",
    "        ,verbose=1\n",
    "        ,save_weights_only=True\n",
    "        ,save_best_only=True\n",
    "        ,mode='min')\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "                    train_gen\n",
    "                    ,epochs=epochs\n",
    "                    ,callbacks=my_callbacks\n",
    "                    )\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Loss'], loc = 'upper left')\n",
    "\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function based on Audio sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_audio(audio_path, model,file_name):\n",
    "    vector_min_size=2500\n",
    "    preds=[]\n",
    "    sampFreq, snd=read(audio_path)\n",
    "    num_batches=ceil(snd.shape[0]/vector_min_size) if snd.shape[0]>vector_min_size else 1\n",
    "    ms=np.resize(snd,(vector_min_size*num_batches))\n",
    "    batches=np.hsplit(ms,num_batches)\n",
    "    batches=np.array(batches)\n",
    "    preds=model.predict(batches)\n",
    "    preds=np.concatenate([pred for pred in preds])\n",
    "    write(f'{file_name}.wav',sampFreq,preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102036, 2500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_specs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pool of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name,inputs,drop_out=0.1):\n",
    "\n",
    "    image_side=int(sqrt(clean_specs.shape[-1]))\n",
    "    image_dims=(image_side,image_side,1)\n",
    "\n",
    "\n",
    "    if model_name==\"naive\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(64))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(64))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn1\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(3000))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn2\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(1024))\n",
    "        model.add(layers.Dense(256))\n",
    "        model.add(layers.Dense(128))\n",
    "        model.add(layers.Dense(64))\n",
    "        model.add(layers.Dense(128))\n",
    "        model.add(layers.Dense(256))\n",
    "        model.add(layers.Dense(320))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn3\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(1024,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(64,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(320,activation=\"relu\"))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn4\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn4_stacked2\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(1500,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(3000,activation=\"relu\"))\n",
    "        \n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"dnn3_stacked2\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "\n",
    "        model.add(layers.Dense(1024,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(64,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(320,activation=\"relu\"))\n",
    "        model.add(layers.Dense(1024,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(64,activation=\"relu\"))\n",
    "        model.add(layers.Dropout(drop_out))\n",
    "        model.add(layers.Dense(128,activation=\"relu\"))\n",
    "        model.add(layers.Dense(256,activation=\"relu\"))\n",
    "        model.add(layers.Dense(320,activation=\"relu\"))\n",
    "        \n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "\n",
    "    elif model_name==\"cnn1\":\n",
    "\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Reshape(image_dims))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (1,1),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 1, kernel_size = (3,3), padding = 'Same',\n",
    "                        activation ='relu'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"cnn2\":\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Input(shape=(inputs,)))\n",
    "        model.add(layers.Reshape(image_dims))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (1,1),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.MaxPooling2D(2,strides=2))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                        activation ='relu'))\n",
    "        model.add(layers.UpSampling2D(2))\n",
    "        model.add(layers.Conv2D(filters = 1, kernel_size = (3,3), padding = 'Same',\n",
    "                        activation ='relu'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(inputs))\n",
    "        model.compile(optimizer='adamax', loss='mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"VGG16\":\n",
    "        # model_VGG16=VGG16(weights='imagenet',include_top= False)\n",
    "        # model_VGG16.trainable=False\n",
    "        # model=models.Sequential()\n",
    "        # model.add(layers.Input(shape=(inputs,)))\n",
    "        # model.add(layers.Reshape(image_dims))\n",
    "        # model.add(layers.UpSampling2D(5))\n",
    "        # model.add(layers.Conv2D(filters = 3, kernel_size = (27,27),padding = 'Same', \n",
    "        #                 activation ='relu'))\n",
    "        # model.add(model_VGG16)\n",
    "        # model.add(layers.Flatten())\n",
    "        # model.add(layers.Dense(1024))\n",
    "        # model.add(layers.Dense(inputs))\n",
    "        # model.compile(optimizer='adamax', loss='mse')\n",
    "        # model.summary()\n",
    "\n",
    "        Input_vector=keras.Input(shape=(inputs,))\n",
    "        Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "        upscale=keras.layers.UpSampling2D(5)(Input_img)\n",
    "        x1 = keras.layers.Conv2D(3, (27, 27), activation='relu', padding='valid')(upscale)\n",
    "\n",
    "\n",
    "        model_transfer_test = keras.applications.VGG16(\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False\n",
    "        )\n",
    "        model_transfer_test.trainable = False\n",
    "        inputs_transfer = keras.Input(shape=(224, 224, 3))\n",
    "        x = model_transfer_test(x1, training=False)\n",
    "        #pooling layer \n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dense(1024, activation = 'softmax')(x)\n",
    "        x = keras.layers.Dropout(drop_out, input_shape=(1024,))(x)\n",
    "        #final dense layer\n",
    "        outputs = keras.layers.Dense(inputs, activation = 'relu')(x)\n",
    "        model = keras.Model(Input_vector,outputs)\n",
    "        model.compile(optimizer='adam', loss = 'mse')\n",
    "        model.summary()\n",
    "\n",
    "    elif model_name==\"MobileNet\":\n",
    "        Input_vector=keras.Input(shape=(inputs,))\n",
    "        Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "        upscale=keras.layers.UpSampling2D(5)(Input_img)\n",
    "        x1 = keras.layers.Conv2D(3, (27, 27), activation='relu', padding='valid')(upscale)\n",
    "\n",
    "\n",
    "        model_transfer_test = keras.applications.MobileNet(\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False\n",
    "        )\n",
    "        model_transfer_test.trainable = False\n",
    "        inputs_transfer = keras.Input(shape=(224, 224, 3))\n",
    "        x = model_transfer_test(x1, training=False)\n",
    "        #pooling layer \n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dense(1024, activation = 'softmax')(x)\n",
    "        x = keras.layers.Dropout(drop_out, input_shape=(1024,))(x)\n",
    "        #final dense layer\n",
    "        outputs = keras.layers.Dense(inputs, activation = 'relu')(x)\n",
    "        model = keras.Model(Input_vector,outputs)\n",
    "        model.compile(optimizer='adam', loss = 'mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"MobileNet_2\":\n",
    "        Input_vector=keras.Input(shape=(inputs,))\n",
    "        Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "        upscale=keras.layers.UpSampling2D(3)(Input_img)\n",
    "        x1 = keras.layers.Conv2D(3, (3, 3), activation='relu', padding='same')(upscale)\n",
    "\n",
    "\n",
    "        model_transfer_test = keras.applications.MobileNet(\n",
    "        weights='imagenet',\n",
    "        input_shape=(150, 150, 3),\n",
    "        include_top=False\n",
    "        )\n",
    "        model_transfer_test.trainable = False\n",
    "        inputs_transfer = keras.Input(shape=(150, 150, 3))\n",
    "        x = model_transfer_test(x1, training=False)\n",
    "        #pooling layer \n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dense(2500, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop_out, input_shape=(2500,))(x)\n",
    "\n",
    "        #decoder\n",
    "        x = keras.layers.Dense(1024, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(64, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(320, activation = 'relu')(x)\n",
    "\n",
    "\n",
    "        # Input_img2=keras.layers.Reshape(image_dims)(x)\n",
    "        # x1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(Input_img2)\n",
    "        # x1 = keras.layers.UpSampling2D(2)(x1)\n",
    "        # x2 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x1)\n",
    "        # x2 = keras.layers.UpSampling2D(2)(x2)\n",
    "        # x3 = keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x2)\n",
    "        # x3 = keras.layers.UpSampling2D(2)(x3)\n",
    "        # x4 = keras.layers.Conv2D(1, (3, 3), activation='relu', padding='same')(x3)\n",
    "        #flat = keras.layers.Flatten()(x4)\n",
    "        #final dense layer\n",
    "        outputs = keras.layers.Dense(inputs, activation = 'softmax')(x)\n",
    "        model = keras.Model(Input_vector,outputs)\n",
    "        model.compile(optimizer='adam', loss = 'mse')\n",
    "        model.summary()\n",
    "    elif model_name==\"MobileNet_3\":\n",
    "        Input_vector=keras.Input(shape=(inputs,))\n",
    "        Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "        x = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(Input_img)\n",
    "        x = keras.layers.MaxPooling2D(2,strides=2)(x)\n",
    "        x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.MaxPooling2D(2,strides=2)(x)\n",
    "        x = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.MaxPooling2D(2,strides=2)(x)\n",
    "        x = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.UpSampling2D(2)(x)\n",
    "        x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.UpSampling2D(2)(x)\n",
    "        x = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = keras.layers.UpSampling2D(2)(x)\n",
    "        x = keras.layers.Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "        upscale=keras.layers.UpSampling2D(4)(x)\n",
    "        # x1 = keras.layers.Conv2D(3, (3, 3), activation='relu', padding='same')(upscale)\n",
    "\n",
    "\n",
    "        model_transfer_test = keras.applications.MobileNet(\n",
    "        weights='imagenet',\n",
    "        input_shape=(192, 192, 3),\n",
    "        include_top=False\n",
    "        )\n",
    "        model_transfer_test.trainable = False\n",
    "        inputs_transfer = keras.Input(shape=(192, 192, 3))\n",
    "        x = model_transfer_test(upscale, training=False)\n",
    "        #pooling layer \n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dense(300, activation = 'relu')(x)\n",
    "        x = keras.layers.Dropout(drop_out, input_shape=(300,))(x)\n",
    "\n",
    "        #decoder\n",
    "        # x = keras.layers.Dense(1024, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(64, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        # x = keras.layers.Dense(320, activation = 'relu')(x)\n",
    "\n",
    "\n",
    "        # Input_img2=keras.layers.Reshape(image_dims)(x)\n",
    "        # x1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(Input_img2)\n",
    "        # x1 = keras.layers.UpSampling2D(2)(x1)\n",
    "        # x2 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x1)\n",
    "        # x2 = keras.layers.UpSampling2D(2)(x2)\n",
    "        # x3 = keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x2)\n",
    "        # x3 = keras.layers.UpSampling2D(2)(x3)\n",
    "        # x4 = keras.layers.Conv2D(1, (3, 3), activation='relu', padding='same')(x3)\n",
    "        #flat = keras.layers.Flatten()(x4)\n",
    "        #final dense layer\n",
    "        outputs = keras.layers.Dense(inputs, activation = 'softmax')(x)\n",
    "        model = keras.Model(Input_vector,outputs)\n",
    "        model.compile(optimizer='adam', loss = 'mse')\n",
    "        model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_103 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 06:48:47 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '933e0af5cbef455fa447f2b0af049c17', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/3189 [..............................] - ETA: 1:05:06 - loss: 2981179.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.0298s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.0298s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184/3189 [============================>.] - ETA: 0s - loss: 2654074.2500\n",
      "Epoch 1: loss improved from inf to 2653176.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2653176.0000\n",
      "Epoch 2/10\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2498208.5000\n",
      "Epoch 2: loss improved from 2653176.00000 to 2497893.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2497893.2500\n",
      "Epoch 3/10\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2390646.0000\n",
      "Epoch 3: loss improved from 2497893.25000 to 2390098.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2390098.0000\n",
      "Epoch 4/10\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2310903.7500\n",
      "Epoch 4: loss improved from 2390098.00000 to 2312510.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2312510.2500\n",
      "Epoch 5/10\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2250674.0000\n",
      "Epoch 5: loss improved from 2312510.25000 to 2249751.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2249751.2500\n",
      "Epoch 6/10\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2195201.2500\n",
      "Epoch 6: loss improved from 2249751.25000 to 2195594.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2195594.0000\n",
      "Epoch 7/10\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2152554.0000\n",
      "Epoch 7: loss improved from 2195594.00000 to 2152895.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2152895.0000\n",
      "Epoch 8/10\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2113382.5000\n",
      "Epoch 8: loss improved from 2152895.00000 to 2114708.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2114708.2500\n",
      "Epoch 9/10\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2083456.0000\n",
      "Epoch 9: loss improved from 2114708.25000 to 2083367.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2083367.5000\n",
      "Epoch 10/10\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2053629.8750\n",
      "Epoch 10: loss improved from 2083367.50000 to 2053981.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2053981.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 06:50:39 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 06:50:39 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp1qrl9b4k\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp1qrl9b4k\\model\\data\\model\\assets\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUTS=clean_specs.shape[-1]\n",
    "custom_mobile_net=select_model(\"dnn3\",INPUTS,drop_out=0.1)\n",
    "custom_mobile_net,history_custom_mobile_net=train_model(custom_mobile_net,10)\n",
    "#2736502.5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFLow Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom crafted networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 19:47:25 INFO mlflow.tracking.fluent: Experiment with name 'naivevector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 1:29:44 - loss: 6463151.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0027s). Check your callbacks.\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2605078.0000\n",
      "Epoch 1: loss improved from inf to 2602324.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 2ms/step - loss: 2602324.2500\n",
      "Epoch 2/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2440344.0000\n",
      "Epoch 2: loss improved from 2602324.25000 to 2442504.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2442504.0000\n",
      "Epoch 3/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2412632.0000\n",
      "Epoch 3: loss improved from 2442504.00000 to 2412264.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2412264.2500\n",
      "Epoch 4/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2393152.0000\n",
      "Epoch 4: loss improved from 2412264.25000 to 2393724.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2393724.2500\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2380439.7500\n",
      "Epoch 5: loss improved from 2393724.25000 to 2380707.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2380707.7500\n",
      "Epoch 6/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2370361.0000\n",
      "Epoch 6: loss improved from 2380707.75000 to 2370161.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2370161.2500\n",
      "Epoch 7/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2365976.0000\n",
      "Epoch 7: loss improved from 2370161.25000 to 2364179.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2364179.2500\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2359368.0000\n",
      "Epoch 8: loss improved from 2364179.25000 to 2359692.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2359692.2500\n",
      "Epoch 9/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2354779.0000\n",
      "Epoch 9: loss improved from 2359692.25000 to 2354810.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2354810.2500\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2349304.7500\n",
      "Epoch 10: loss improved from 2354810.25000 to 2348834.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2348834.7500\n",
      "Epoch 11/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2344967.7500\n",
      "Epoch 11: loss improved from 2348834.75000 to 2345544.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2345544.0000\n",
      "Epoch 12/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2343694.0000\n",
      "Epoch 12: loss improved from 2345544.00000 to 2343307.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2343307.2500\n",
      "Epoch 13/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2341847.5000\n",
      "Epoch 13: loss improved from 2343307.25000 to 2341554.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2341554.0000\n",
      "Epoch 14/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2336245.0000\n",
      "Epoch 14: loss improved from 2341554.00000 to 2338480.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2338480.0000\n",
      "Epoch 15/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2335219.2500\n",
      "Epoch 15: loss improved from 2338480.00000 to 2335778.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2335778.0000\n",
      "Epoch 16/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2335471.5000\n",
      "Epoch 16: loss improved from 2335778.00000 to 2333960.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2333960.2500\n",
      "Epoch 17/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2332032.2500\n",
      "Epoch 17: loss improved from 2333960.25000 to 2332204.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2332204.5000\n",
      "Epoch 18/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2331701.7500\n",
      "Epoch 18: loss improved from 2332204.50000 to 2331564.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2331564.5000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2331105.0000\n",
      "Epoch 19: loss improved from 2331564.50000 to 2330072.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2330072.5000\n",
      "Epoch 20/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2329864.0000\n",
      "Epoch 20: loss improved from 2330072.50000 to 2329194.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2329194.2500\n",
      "Epoch 21/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2328807.7500\n",
      "Epoch 21: loss improved from 2329194.25000 to 2327223.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2327223.2500\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2326712.5000\n",
      "Epoch 22: loss improved from 2327223.25000 to 2326397.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2326397.7500\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2324650.7500\n",
      "Epoch 23: loss improved from 2326397.75000 to 2324469.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2324469.7500\n",
      "Epoch 24/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2321721.0000\n",
      "Epoch 24: loss improved from 2324469.75000 to 2322196.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2322196.0000\n",
      "Epoch 25/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2321256.0000\n",
      "Epoch 25: loss improved from 2322196.00000 to 2320912.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2320912.5000\n",
      "Epoch 26/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2320351.5000\n",
      "Epoch 26: loss improved from 2320912.50000 to 2320818.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2320818.7500\n",
      "Epoch 27/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2322815.0000\n",
      "Epoch 27: loss improved from 2320818.75000 to 2320503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2320503.7500\n",
      "Epoch 28/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2318249.7500\n",
      "Epoch 28: loss improved from 2320503.75000 to 2319082.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2319082.0000\n",
      "Epoch 29/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2317314.0000\n",
      "Epoch 29: loss improved from 2319082.00000 to 2318463.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2318463.0000\n",
      "Epoch 30/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2318792.0000\n",
      "Epoch 30: loss improved from 2318463.00000 to 2318450.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2318450.0000\n",
      "Epoch 31/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2320999.7500\n",
      "Epoch 31: loss improved from 2318450.00000 to 2318286.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2318286.7500\n",
      "Epoch 32/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2317060.2500\n",
      "Epoch 32: loss improved from 2318286.75000 to 2317621.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2317621.2500\n",
      "Epoch 33/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2315735.7500\n",
      "Epoch 33: loss improved from 2317621.25000 to 2315246.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2315246.5000\n",
      "Epoch 34/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2316080.0000\n",
      "Epoch 34: loss did not improve from 2315246.50000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2316097.0000\n",
      "Epoch 35/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2315603.0000\n",
      "Epoch 35: loss did not improve from 2315246.50000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2315805.0000\n",
      "Epoch 36/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2314906.0000\n",
      "Epoch 36: loss improved from 2315246.50000 to 2314353.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2314353.7500\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2314942.0000\n",
      "Epoch 37: loss did not improve from 2314353.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2314634.2500\n",
      "Epoch 38/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2314253.7500\n",
      "Epoch 38: loss improved from 2314353.75000 to 2313710.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2313710.5000\n",
      "Epoch 39/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2315163.7500\n",
      "Epoch 39: loss did not improve from 2313710.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2313771.2500\n",
      "Epoch 40/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2313366.2500\n",
      "Epoch 40: loss improved from 2313710.50000 to 2312444.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2312444.5000\n",
      "Epoch 41/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2312784.0000\n",
      "Epoch 41: loss did not improve from 2312444.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2312730.2500\n",
      "Epoch 42/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2312866.7500\n",
      "Epoch 42: loss improved from 2312444.50000 to 2311900.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2311900.7500\n",
      "Epoch 43/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2311005.2500\n",
      "Epoch 43: loss improved from 2311900.75000 to 2311894.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2311894.7500\n",
      "Epoch 44/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2312308.0000\n",
      "Epoch 44: loss improved from 2311894.75000 to 2311704.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2311704.0000\n",
      "Epoch 45/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2312609.7500\n",
      "Epoch 45: loss did not improve from 2311704.00000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2312509.0000\n",
      "Epoch 46/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2312558.0000\n",
      "Epoch 46: loss did not improve from 2311704.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2312460.5000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2311104.0000\n",
      "Epoch 47: loss improved from 2311704.00000 to 2311104.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311104.0000\n",
      "Epoch 48/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2311238.0000\n",
      "Epoch 48: loss did not improve from 2311104.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311175.5000\n",
      "Epoch 49/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2308779.2500\n",
      "Epoch 49: loss improved from 2311104.00000 to 2310666.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310666.7500\n",
      "Epoch 50/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2310456.5000\n",
      "Epoch 50: loss improved from 2310666.75000 to 2310409.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310409.5000\n",
      "Epoch 51/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2310299.2500\n",
      "Epoch 51: loss improved from 2310409.50000 to 2310124.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310124.2500\n",
      "Epoch 52/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2311929.7500\n",
      "Epoch 52: loss did not improve from 2310124.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310601.5000\n",
      "Epoch 53/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2309463.2500\n",
      "Epoch 53: loss improved from 2310124.25000 to 2309483.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2309483.7500\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2309158.7500\n",
      "Epoch 54: loss did not improve from 2309483.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2310031.5000\n",
      "Epoch 55/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2309401.0000\n",
      "Epoch 55: loss improved from 2309483.75000 to 2308485.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308485.7500\n",
      "Epoch 56/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2309545.7500\n",
      "Epoch 56: loss improved from 2308485.75000 to 2308159.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308159.7500\n",
      "Epoch 57/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2309113.5000\n",
      "Epoch 57: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2309095.7500\n",
      "Epoch 58/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2310232.2500\n",
      "Epoch 58: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2308563.0000\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2308232.5000\n",
      "Epoch 59: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2308693.7500\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2308718.7500\n",
      "Epoch 60: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308534.0000\n",
      "Epoch 61/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2308142.5000\n",
      "Epoch 61: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308611.7500\n",
      "Epoch 62/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2308689.7500\n",
      "Epoch 62: loss did not improve from 2308159.75000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308321.0000\n",
      "Epoch 63/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2306702.5000\n",
      "Epoch 63: loss improved from 2308159.75000 to 2307394.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2307394.5000\n",
      "Epoch 64/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2308689.5000\n",
      "Epoch 64: loss did not improve from 2307394.50000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308586.7500\n",
      "Epoch 65/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2307864.5000\n",
      "Epoch 65: loss did not improve from 2307394.50000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2308105.5000\n",
      "Epoch 66/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2306869.7500\n",
      "Epoch 66: loss did not improve from 2307394.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2308066.0000\n",
      "Epoch 67/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2303739.2500\n",
      "Epoch 67: loss improved from 2307394.50000 to 2306300.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2306300.2500\n",
      "Epoch 68/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2307276.7500\n",
      "Epoch 68: loss did not improve from 2306300.25000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2306576.0000\n",
      "Epoch 69/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2306764.0000\n",
      "Epoch 69: loss did not improve from 2306300.25000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2307060.0000\n",
      "Epoch 70/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2308126.2500\n",
      "Epoch 70: loss improved from 2306300.25000 to 2306075.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2306075.7500\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2306392.5000\n",
      "Epoch 71: loss improved from 2306075.75000 to 2305890.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305890.0000\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2306411.7500\n",
      "Epoch 72: loss did not improve from 2305890.00000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2306411.7500\n",
      "Epoch 73/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2305524.0000\n",
      "Epoch 73: loss improved from 2305890.00000 to 2305856.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2305856.7500\n",
      "Epoch 74/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2306055.5000\n",
      "Epoch 74: loss did not improve from 2305856.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2306550.5000\n",
      "Epoch 75/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2307077.0000\n",
      "Epoch 75: loss did not improve from 2305856.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2306068.7500\n",
      "Epoch 76/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2306438.0000\n",
      "Epoch 76: loss did not improve from 2305856.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306266.7500\n",
      "Epoch 77/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2306273.0000\n",
      "Epoch 77: loss improved from 2305856.75000 to 2305533.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305533.0000\n",
      "Epoch 78/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2305613.5000\n",
      "Epoch 78: loss improved from 2305533.00000 to 2305458.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305458.5000\n",
      "Epoch 79/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2306171.0000\n",
      "Epoch 79: loss did not improve from 2305458.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306550.5000\n",
      "Epoch 80/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2306320.7500\n",
      "Epoch 80: loss improved from 2305458.50000 to 2305438.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305438.7500\n",
      "Epoch 81/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2304703.2500\n",
      "Epoch 81: loss improved from 2305438.75000 to 2305139.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305139.7500\n",
      "Epoch 82/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2303373.2500\n",
      "Epoch 82: loss did not improve from 2305139.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305926.2500\n",
      "Epoch 83/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2303931.7500\n",
      "Epoch 83: loss improved from 2305139.75000 to 2304588.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304588.0000\n",
      "Epoch 84/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2306890.7500\n",
      "Epoch 84: loss did not improve from 2304588.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305110.5000\n",
      "Epoch 85/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2306132.5000\n",
      "Epoch 85: loss did not improve from 2304588.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304901.2500\n",
      "Epoch 86/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2305158.7500\n",
      "Epoch 86: loss improved from 2304588.00000 to 2303979.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303979.0000\n",
      "Epoch 87/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2307104.5000\n",
      "Epoch 87: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304906.7500\n",
      "Epoch 88/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2302791.0000\n",
      "Epoch 88: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304529.2500\n",
      "Epoch 89/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2304245.7500\n",
      "Epoch 89: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304454.7500\n",
      "Epoch 90/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2304997.5000\n",
      "Epoch 90: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304728.5000\n",
      "Epoch 91/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2305020.7500\n",
      "Epoch 91: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305460.7500\n",
      "Epoch 92/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2304250.2500\n",
      "Epoch 92: loss did not improve from 2303979.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304295.5000\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2303844.0000\n",
      "Epoch 93: loss improved from 2303979.00000 to 2303844.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303844.0000\n",
      "Epoch 94/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2304540.0000\n",
      "Epoch 94: loss did not improve from 2303844.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304837.0000\n",
      "Epoch 95/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2304651.5000\n",
      "Epoch 95: loss improved from 2303844.00000 to 2303542.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303542.7500\n",
      "Epoch 96/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2304884.5000\n",
      "Epoch 96: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304748.0000\n",
      "Epoch 97/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2303307.7500\n",
      "Epoch 97: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304290.2500\n",
      "Epoch 98/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2304543.0000\n",
      "Epoch 98: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304467.7500\n",
      "Epoch 99/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2303664.5000\n",
      "Epoch 99: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303848.7500\n",
      "Epoch 100/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2304794.7500\n",
      "Epoch 100: loss did not improve from 2303542.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304272.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:00:15 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 20:00:15 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmps2sh4u7w\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:00:54 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n",
      "2023/05/26 20:01:01 INFO mlflow.tracking.fluent: Experiment with name 'naivevector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2722772.0000\n",
      "Epoch 1: loss improved from inf to 2722473.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 62s 18ms/step - loss: 2722473.5000\n",
      "Epoch 2/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2585965.7500\n",
      "Epoch 2: loss improved from 2722473.50000 to 2586382.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2586382.2500\n",
      "Epoch 3/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2567833.2500\n",
      "Epoch 3: loss improved from 2586382.25000 to 2569288.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2569288.2500\n",
      "Epoch 4/100\n",
      "3165/3189 [============================>.] - ETA: 0s - loss: 2556589.2500\n",
      "Epoch 4: loss improved from 2569288.25000 to 2560930.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2560930.7500\n",
      "Epoch 5/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2555681.5000\n",
      "Epoch 5: loss improved from 2560930.75000 to 2554782.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2554782.0000\n",
      "Epoch 6/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2550899.2500\n",
      "Epoch 6: loss improved from 2554782.00000 to 2550218.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2550218.0000\n",
      "Epoch 7/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2545456.0000\n",
      "Epoch 7: loss improved from 2550218.00000 to 2546862.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2546862.0000\n",
      "Epoch 8/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2544502.2500\n",
      "Epoch 8: loss improved from 2546862.00000 to 2544213.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2544213.7500\n",
      "Epoch 9/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2540942.5000\n",
      "Epoch 9: loss improved from 2544213.75000 to 2541356.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2541356.2500\n",
      "Epoch 10/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2538108.0000\n",
      "Epoch 10: loss improved from 2541356.25000 to 2537610.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2537610.2500\n",
      "Epoch 11/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2535556.5000\n",
      "Epoch 11: loss improved from 2537610.25000 to 2536277.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2536277.7500\n",
      "Epoch 12/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2534399.7500\n",
      "Epoch 12: loss improved from 2536277.75000 to 2534993.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2534993.7500\n",
      "Epoch 13/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2534784.2500\n",
      "Epoch 13: loss improved from 2534993.75000 to 2534055.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2534055.2500\n",
      "Epoch 14/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2534833.2500\n",
      "Epoch 14: loss improved from 2534055.25000 to 2532504.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2532504.0000\n",
      "Epoch 15/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2532274.7500\n",
      "Epoch 15: loss improved from 2532504.00000 to 2532161.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2532161.7500\n",
      "Epoch 16/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2530099.5000\n",
      "Epoch 16: loss improved from 2532161.75000 to 2530412.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2530412.5000\n",
      "Epoch 17/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2529417.2500\n",
      "Epoch 17: loss improved from 2530412.50000 to 2529262.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2529262.5000\n",
      "Epoch 18/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2531274.0000\n",
      "Epoch 18: loss did not improve from 2529262.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2529534.5000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2530008.0000\n",
      "Epoch 19: loss did not improve from 2529262.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2529591.7500\n",
      "Epoch 20/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2526666.2500\n",
      "Epoch 20: loss improved from 2529262.50000 to 2528327.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2528327.7500\n",
      "Epoch 21/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2528293.5000\n",
      "Epoch 21: loss improved from 2528327.75000 to 2527234.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2527234.2500\n",
      "Epoch 22/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2527475.7500\n",
      "Epoch 22: loss improved from 2527234.25000 to 2527155.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2527155.0000\n",
      "Epoch 23/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2526973.5000\n",
      "Epoch 23: loss improved from 2527155.00000 to 2526400.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2526400.5000\n",
      "Epoch 24/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2529772.2500\n",
      "Epoch 24: loss did not improve from 2526400.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2527277.2500\n",
      "Epoch 25/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2523510.0000\n",
      "Epoch 25: loss improved from 2526400.50000 to 2525435.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525435.7500\n",
      "Epoch 26/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2524787.2500\n",
      "Epoch 26: loss improved from 2525435.75000 to 2525428.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525428.2500\n",
      "Epoch 27/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2524784.7500\n",
      "Epoch 27: loss did not improve from 2525428.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525793.5000\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2525657.5000\n",
      "Epoch 28: loss improved from 2525428.25000 to 2525119.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525119.0000\n",
      "Epoch 29/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2524597.2500\n",
      "Epoch 29: loss improved from 2525119.00000 to 2524987.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2524987.5000\n",
      "Epoch 30/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2524711.5000\n",
      "Epoch 30: loss improved from 2524987.50000 to 2524469.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2524469.0000\n",
      "Epoch 31/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2522321.0000\n",
      "Epoch 31: loss improved from 2524469.00000 to 2523055.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2523055.0000\n",
      "Epoch 32/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2527765.0000\n",
      "Epoch 32: loss did not improve from 2523055.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2525414.7500\n",
      "Epoch 33/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2523394.7500\n",
      "Epoch 33: loss did not improve from 2523055.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2523284.0000\n",
      "Epoch 34/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2523745.7500\n",
      "Epoch 34: loss did not improve from 2523055.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2523303.2500\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2522539.0000\n",
      "Epoch 35: loss improved from 2523055.00000 to 2522931.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2522931.5000\n",
      "Epoch 36/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2522564.2500\n",
      "Epoch 36: loss improved from 2522931.50000 to 2522696.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2522696.2500\n",
      "Epoch 37/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2525087.7500\n",
      "Epoch 37: loss did not improve from 2522696.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2523318.5000\n",
      "Epoch 38/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2523180.0000\n",
      "Epoch 38: loss improved from 2522696.25000 to 2522554.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2522554.5000\n",
      "Epoch 39/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2521603.7500\n",
      "Epoch 39: loss improved from 2522554.50000 to 2521954.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2521954.5000\n",
      "Epoch 40/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2521967.2500\n",
      "Epoch 40: loss improved from 2521954.50000 to 2521842.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2521842.5000\n",
      "Epoch 41/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2520817.5000\n",
      "Epoch 41: loss improved from 2521842.50000 to 2521280.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2521280.5000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2520886.7500\n",
      "Epoch 42: loss improved from 2521280.50000 to 2521134.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2521134.7500\n",
      "Epoch 43/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2522616.5000\n",
      "Epoch 43: loss improved from 2521134.75000 to 2520469.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2520469.5000\n",
      "Epoch 44/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2520889.5000\n",
      "Epoch 44: loss did not improve from 2520469.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2520889.5000\n",
      "Epoch 45/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2518918.2500\n",
      "Epoch 45: loss improved from 2520469.50000 to 2519864.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519864.2500\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2518226.0000\n",
      "Epoch 46: loss did not improve from 2519864.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519880.7500\n",
      "Epoch 47/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2519744.5000\n",
      "Epoch 47: loss improved from 2519864.25000 to 2519816.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519816.7500\n",
      "Epoch 48/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2517017.2500\n",
      "Epoch 48: loss improved from 2519816.75000 to 2518696.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518696.0000\n",
      "Epoch 49/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2519213.0000\n",
      "Epoch 49: loss did not improve from 2518696.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518956.7500\n",
      "Epoch 50/100\n",
      "3166/3189 [============================>.] - ETA: 0s - loss: 2519201.7500\n",
      "Epoch 50: loss did not improve from 2518696.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519213.5000\n",
      "Epoch 51/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2518777.0000\n",
      "Epoch 51: loss improved from 2518696.00000 to 2518480.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518480.2500\n",
      "Epoch 52/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2516858.5000\n",
      "Epoch 52: loss improved from 2518480.25000 to 2518330.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518330.2500\n",
      "Epoch 53/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2517421.0000\n",
      "Epoch 53: loss improved from 2518330.25000 to 2517381.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517381.2500\n",
      "Epoch 54/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2518174.0000\n",
      "Epoch 54: loss did not improve from 2517381.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517633.5000\n",
      "Epoch 55/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2518395.5000\n",
      "Epoch 55: loss did not improve from 2517381.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518374.0000\n",
      "Epoch 56/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2515167.0000\n",
      "Epoch 56: loss improved from 2517381.25000 to 2516851.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516851.5000\n",
      "Epoch 57/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2516794.2500\n",
      "Epoch 57: loss did not improve from 2516851.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517461.5000\n",
      "Epoch 58/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2516170.2500\n",
      "Epoch 58: loss did not improve from 2516851.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517239.2500\n",
      "Epoch 59/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2516882.2500\n",
      "Epoch 59: loss improved from 2516851.50000 to 2516640.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516640.5000\n",
      "Epoch 60/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2516490.2500\n",
      "Epoch 60: loss improved from 2516640.50000 to 2515898.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515898.0000\n",
      "Epoch 61/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2517255.5000\n",
      "Epoch 61: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516445.7500\n",
      "Epoch 62/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2518946.2500\n",
      "Epoch 62: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516879.2500\n",
      "Epoch 63/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2517102.7500\n",
      "Epoch 63: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517511.0000\n",
      "Epoch 64/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2514824.7500\n",
      "Epoch 64: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516190.2500\n",
      "Epoch 65/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2514750.5000\n",
      "Epoch 65: loss did not improve from 2515898.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516200.7500\n",
      "Epoch 66/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2512032.7500\n",
      "Epoch 66: loss improved from 2515898.00000 to 2515044.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515044.7500\n",
      "Epoch 67/100\n",
      "3165/3189 [============================>.] - ETA: 0s - loss: 2514891.0000\n",
      "Epoch 67: loss did not improve from 2515044.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515052.7500\n",
      "Epoch 68/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2515565.7500\n",
      "Epoch 68: loss did not improve from 2515044.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516021.0000\n",
      "Epoch 69/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2515184.7500\n",
      "Epoch 69: loss did not improve from 2515044.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515184.7500\n",
      "Epoch 70/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2515543.2500\n",
      "Epoch 70: loss did not improve from 2515044.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515277.2500\n",
      "Epoch 71/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2514791.5000\n",
      "Epoch 71: loss improved from 2515044.75000 to 2514802.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514802.5000\n",
      "Epoch 72/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2512833.5000\n",
      "Epoch 72: loss improved from 2514802.50000 to 2514074.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514074.2500\n",
      "Epoch 73/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2516472.0000\n",
      "Epoch 73: loss improved from 2514074.25000 to 2513680.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513680.0000\n",
      "Epoch 74/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2512954.0000\n",
      "Epoch 74: loss did not improve from 2513680.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514316.0000\n",
      "Epoch 75/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2513216.2500\n",
      "Epoch 75: loss did not improve from 2513680.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513848.0000\n",
      "Epoch 76/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2515196.0000\n",
      "Epoch 76: loss improved from 2513680.00000 to 2513151.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513151.5000\n",
      "Epoch 77/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2513890.5000\n",
      "Epoch 77: loss did not improve from 2513151.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513809.5000\n",
      "Epoch 78/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2513574.7500\n",
      "Epoch 78: loss did not improve from 2513151.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513957.2500\n",
      "Epoch 79/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2511891.7500\n",
      "Epoch 79: loss improved from 2513151.50000 to 2513022.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513022.2500\n",
      "Epoch 80/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2514103.5000\n",
      "Epoch 80: loss improved from 2513022.25000 to 2511813.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511813.7500\n",
      "Epoch 81/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2513286.2500\n",
      "Epoch 81: loss did not improve from 2511813.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512658.0000\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2512891.0000\n",
      "Epoch 82: loss did not improve from 2511813.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512529.7500\n",
      "Epoch 83/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2511723.7500\n",
      "Epoch 83: loss did not improve from 2511813.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512018.5000\n",
      "Epoch 84/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2513289.0000\n",
      "Epoch 84: loss improved from 2511813.75000 to 2511581.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511581.2500\n",
      "Epoch 85/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2511965.2500\n",
      "Epoch 85: loss improved from 2511581.25000 to 2511339.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511339.0000\n",
      "Epoch 86/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2510545.2500\n",
      "Epoch 86: loss improved from 2511339.00000 to 2510966.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510966.2500\n",
      "Epoch 87/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2508315.0000\n",
      "Epoch 87: loss improved from 2510966.25000 to 2510750.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510750.2500\n",
      "Epoch 88/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2509805.5000\n",
      "Epoch 88: loss improved from 2510750.25000 to 2510658.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510658.0000\n",
      "Epoch 89/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2510441.5000\n",
      "Epoch 89: loss improved from 2510658.00000 to 2510411.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510411.2500\n",
      "Epoch 90/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2506473.7500\n",
      "Epoch 90: loss improved from 2510411.25000 to 2509874.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2509874.7500\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2510291.5000\n",
      "Epoch 91: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510629.7500\n",
      "Epoch 92/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2508956.7500\n",
      "Epoch 92: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510590.0000\n",
      "Epoch 93/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2506109.7500\n",
      "Epoch 93: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510083.0000\n",
      "Epoch 94/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2510412.5000\n",
      "Epoch 94: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2509984.5000\n",
      "Epoch 95/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2510416.5000\n",
      "Epoch 95: loss did not improve from 2509874.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510542.2500\n",
      "Epoch 96/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2510398.0000\n",
      "Epoch 96: loss improved from 2509874.75000 to 2509348.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2509348.0000\n",
      "Epoch 97/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2512928.7500\n",
      "Epoch 97: loss did not improve from 2509348.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510263.2500\n",
      "Epoch 98/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2509760.0000\n",
      "Epoch 98: loss improved from 2509348.00000 to 2508850.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2508850.7500\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2508247.7500\n",
      "Epoch 99: loss improved from 2508850.75000 to 2507456.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2507456.2500\n",
      "Epoch 100/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2506773.5000\n",
      "Epoch 100: loss did not improve from 2507456.25000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2508106.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:13:52 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 20:13:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpse3yw75e\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:14:39 INFO mlflow.tracking.fluent: Experiment with name 'dnn1vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 3:12 - loss: 25237928.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0188s vs `on_train_batch_end` time: 0.0308s). Check your callbacks.\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 3116367.5000\n",
      "Epoch 1: loss improved from inf to 3114861.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 3114861.7500\n",
      "Epoch 2/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2389506.0000\n",
      "Epoch 2: loss improved from 3114861.75000 to 2389677.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2389677.5000\n",
      "Epoch 3/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2343839.7500\n",
      "Epoch 3: loss improved from 2389677.50000 to 2343122.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2343122.7500\n",
      "Epoch 4/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2305812.2500\n",
      "Epoch 4: loss improved from 2343122.75000 to 2306677.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2306677.7500\n",
      "Epoch 5/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2328884.7500\n",
      "Epoch 5: loss did not improve from 2306677.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2328805.7500\n",
      "Epoch 6/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2305745.5000\n",
      "Epoch 6: loss improved from 2306677.75000 to 2305949.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2305949.0000\n",
      "Epoch 7/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2292381.7500\n",
      "Epoch 7: loss improved from 2305949.00000 to 2292362.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2292362.2500\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2277615.2500\n",
      "Epoch 8: loss improved from 2292362.25000 to 2276743.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2276743.7500\n",
      "Epoch 9/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2270412.5000\n",
      "Epoch 9: loss improved from 2276743.75000 to 2270754.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2270754.7500\n",
      "Epoch 10/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2276892.5000\n",
      "Epoch 10: loss did not improve from 2270754.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2275593.0000\n",
      "Epoch 11/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2240151.0000\n",
      "Epoch 11: loss improved from 2270754.75000 to 2240152.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2240152.0000\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2240827.2500\n",
      "Epoch 12: loss did not improve from 2240152.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2240827.2500\n",
      "Epoch 13/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2234795.0000\n",
      "Epoch 13: loss improved from 2240152.00000 to 2236581.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2236581.7500\n",
      "Epoch 14/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2241845.7500\n",
      "Epoch 14: loss did not improve from 2236581.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2242469.2500\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2233020.7500\n",
      "Epoch 15: loss improved from 2236581.75000 to 2233285.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2233285.7500\n",
      "Epoch 16/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2240107.2500\n",
      "Epoch 16: loss did not improve from 2233285.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2238934.5000\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2234025.7500\n",
      "Epoch 17: loss did not improve from 2233285.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2234475.7500\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2220509.7500\n",
      "Epoch 18: loss improved from 2233285.75000 to 2220509.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2220509.5000\n",
      "Epoch 19/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2217812.2500\n",
      "Epoch 19: loss improved from 2220509.50000 to 2217677.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2217677.7500\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2224279.5000\n",
      "Epoch 20: loss did not improve from 2217677.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2223277.7500\n",
      "Epoch 21/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2222628.5000\n",
      "Epoch 21: loss did not improve from 2217677.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2222904.2500\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2208561.0000\n",
      "Epoch 22: loss improved from 2217677.75000 to 2208519.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2208519.0000\n",
      "Epoch 23/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2217894.0000\n",
      "Epoch 23: loss did not improve from 2208519.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2217373.2500\n",
      "Epoch 24/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2208823.7500\n",
      "Epoch 24: loss improved from 2208519.00000 to 2208454.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2208454.5000\n",
      "Epoch 25/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2202746.0000\n",
      "Epoch 25: loss improved from 2208454.50000 to 2204225.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2204225.2500\n",
      "Epoch 26/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2203097.7500\n",
      "Epoch 26: loss improved from 2204225.25000 to 2202841.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2202841.7500\n",
      "Epoch 27/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2214963.5000\n",
      "Epoch 27: loss did not improve from 2202841.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2213591.2500\n",
      "Epoch 28/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2212599.5000\n",
      "Epoch 28: loss did not improve from 2202841.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2211329.5000\n",
      "Epoch 29/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2206546.5000\n",
      "Epoch 29: loss did not improve from 2202841.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2207447.0000\n",
      "Epoch 30/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2190346.2500\n",
      "Epoch 30: loss improved from 2202841.75000 to 2190748.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2190748.0000\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2202135.2500\n",
      "Epoch 31: loss did not improve from 2190748.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2202135.2500\n",
      "Epoch 32/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2202553.7500\n",
      "Epoch 32: loss did not improve from 2190748.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2203054.7500\n",
      "Epoch 33/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2202203.5000\n",
      "Epoch 33: loss did not improve from 2190748.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2202204.7500\n",
      "Epoch 34/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2179440.0000\n",
      "Epoch 34: loss improved from 2190748.00000 to 2179412.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2179412.5000\n",
      "Epoch 35/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2199143.5000\n",
      "Epoch 35: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2198981.5000\n",
      "Epoch 36/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2199900.7500\n",
      "Epoch 36: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2199740.2500\n",
      "Epoch 37/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2183944.5000\n",
      "Epoch 37: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2185332.7500\n",
      "Epoch 38/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2199498.2500\n",
      "Epoch 38: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2200541.0000\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2194765.5000\n",
      "Epoch 39: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2193998.7500\n",
      "Epoch 40/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2195593.0000\n",
      "Epoch 40: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2195193.7500\n",
      "Epoch 41/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2187018.5000\n",
      "Epoch 41: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2186806.0000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2186419.0000\n",
      "Epoch 42: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2184160.2500\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2189085.7500\n",
      "Epoch 43: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2189085.7500\n",
      "Epoch 44/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2187785.0000\n",
      "Epoch 44: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186543.2500\n",
      "Epoch 45/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2193007.5000\n",
      "Epoch 45: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192686.5000\n",
      "Epoch 46/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2184395.5000\n",
      "Epoch 46: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186207.5000\n",
      "Epoch 47/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2187988.5000\n",
      "Epoch 47: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2187415.2500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2192978.7500\n",
      "Epoch 48: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192961.0000\n",
      "Epoch 49/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2188859.0000\n",
      "Epoch 49: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2189294.0000\n",
      "Epoch 50/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2182340.5000\n",
      "Epoch 50: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182631.5000\n",
      "Epoch 51/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2187973.5000\n",
      "Epoch 51: loss did not improve from 2179412.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186580.7500\n",
      "Epoch 52/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2178298.2500\n",
      "Epoch 52: loss improved from 2179412.50000 to 2177704.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2177704.5000\n",
      "Epoch 53/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2185256.2500\n",
      "Epoch 53: loss did not improve from 2177704.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185647.2500\n",
      "Epoch 54/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2176563.5000\n",
      "Epoch 54: loss improved from 2177704.50000 to 2176803.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2176803.5000\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2172674.5000\n",
      "Epoch 55: loss improved from 2176803.50000 to 2172239.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2172239.2500\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2185344.0000\n",
      "Epoch 56: loss did not improve from 2172239.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185158.5000\n",
      "Epoch 57/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2180815.5000\n",
      "Epoch 57: loss did not improve from 2172239.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180913.0000\n",
      "Epoch 58/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2167078.2500\n",
      "Epoch 58: loss improved from 2172239.25000 to 2168698.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2168698.7500\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2181656.2500\n",
      "Epoch 59: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2181427.2500\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2182080.5000\n",
      "Epoch 60: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2182080.5000\n",
      "Epoch 61/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2192352.0000\n",
      "Epoch 61: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2191335.2500\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2179200.0000\n",
      "Epoch 62: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2179200.0000\n",
      "Epoch 63/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2187344.0000\n",
      "Epoch 63: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2188471.0000\n",
      "Epoch 64/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2174149.5000\n",
      "Epoch 64: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2174430.0000\n",
      "Epoch 65/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2188444.5000\n",
      "Epoch 65: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2187467.2500\n",
      "Epoch 66/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2186102.2500\n",
      "Epoch 66: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2187174.0000\n",
      "Epoch 67/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2175864.0000\n",
      "Epoch 67: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2176209.2500\n",
      "Epoch 68/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2178020.2500\n",
      "Epoch 68: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2179060.0000\n",
      "Epoch 69/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2178536.0000\n",
      "Epoch 69: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2178527.2500\n",
      "Epoch 70/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2180602.5000\n",
      "Epoch 70: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2179502.7500\n",
      "Epoch 71/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2183240.5000\n",
      "Epoch 71: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2183418.7500\n",
      "Epoch 72/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2183286.7500\n",
      "Epoch 72: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2182995.7500\n",
      "Epoch 73/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2180976.0000\n",
      "Epoch 73: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2179996.5000\n",
      "Epoch 74/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2180503.5000\n",
      "Epoch 74: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2180806.0000\n",
      "Epoch 75/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2170686.5000\n",
      "Epoch 75: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2170143.7500\n",
      "Epoch 76/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2186110.5000\n",
      "Epoch 76: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2185486.0000\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2184913.2500\n",
      "Epoch 77: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2184921.5000\n",
      "Epoch 78/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2178020.7500\n",
      "Epoch 78: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2178164.0000\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2175746.5000\n",
      "Epoch 79: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2175716.0000\n",
      "Epoch 80/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2172809.7500\n",
      "Epoch 80: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2174257.5000\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2184373.0000\n",
      "Epoch 81: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2184373.0000\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2178543.0000\n",
      "Epoch 82: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2178521.2500\n",
      "Epoch 83/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2173868.0000\n",
      "Epoch 83: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2175026.2500\n",
      "Epoch 84/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2180005.5000\n",
      "Epoch 84: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2180519.0000\n",
      "Epoch 85/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2183952.7500\n",
      "Epoch 85: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2183572.7500\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2170678.2500\n",
      "Epoch 86: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2171654.7500\n",
      "Epoch 87/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2178011.5000\n",
      "Epoch 87: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2178173.0000\n",
      "Epoch 88/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2175192.2500\n",
      "Epoch 88: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2175444.7500\n",
      "Epoch 89/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2181377.0000\n",
      "Epoch 89: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2181018.2500\n",
      "Epoch 90/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2168382.5000\n",
      "Epoch 90: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2169466.5000\n",
      "Epoch 91/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2181213.0000\n",
      "Epoch 91: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2181703.2500\n",
      "Epoch 92/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2170542.0000\n",
      "Epoch 92: loss did not improve from 2168698.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2171385.0000\n",
      "Epoch 93/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2158449.0000\n",
      "Epoch 93: loss improved from 2168698.75000 to 2157095.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2157095.0000\n",
      "Epoch 94/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2173452.5000\n",
      "Epoch 94: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2174830.7500\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2177404.7500\n",
      "Epoch 95: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2178000.7500\n",
      "Epoch 96/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2171843.2500\n",
      "Epoch 96: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2173422.0000\n",
      "Epoch 97/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2177091.5000\n",
      "Epoch 97: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2176639.7500\n",
      "Epoch 98/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2174805.0000\n",
      "Epoch 98: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2174215.2500\n",
      "Epoch 99/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2176623.2500\n",
      "Epoch 99: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2175831.7500\n",
      "Epoch 100/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2173997.2500\n",
      "Epoch 100: loss did not improve from 2157095.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2172982.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:34:59 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 20:34:59 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmps7r9jioq\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:36:14 INFO mlflow.tracking.fluent: Experiment with name 'dnn1vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 1:23 - loss: 124459152.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0092s vs `on_train_batch_end` time: 0.0214s). Check your callbacks.\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 12120683.0000\n",
      "Epoch 1: loss improved from inf to 12116690.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 12116690.0000\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2896840.5000\n",
      "Epoch 2: loss improved from 12116690.00000 to 2896840.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2896840.5000\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2641287.2500\n",
      "Epoch 3: loss improved from 2896840.50000 to 2641247.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2641247.2500\n",
      "Epoch 4/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2602502.2500\n",
      "Epoch 4: loss improved from 2641247.25000 to 2602214.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2602214.5000\n",
      "Epoch 5/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2583407.0000\n",
      "Epoch 5: loss improved from 2602214.50000 to 2583296.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2583296.2500\n",
      "Epoch 6/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2563538.0000\n",
      "Epoch 6: loss improved from 2583296.25000 to 2564046.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2564046.7500\n",
      "Epoch 7/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2554781.7500\n",
      "Epoch 7: loss improved from 2564046.75000 to 2555140.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2555140.2500\n",
      "Epoch 8/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2557326.5000\n",
      "Epoch 8: loss did not improve from 2555140.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2557842.0000\n",
      "Epoch 9/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2554835.0000\n",
      "Epoch 9: loss improved from 2555140.25000 to 2554944.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2554944.7500\n",
      "Epoch 10/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2552513.2500\n",
      "Epoch 10: loss improved from 2554944.75000 to 2552721.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2552721.0000\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2550573.7500\n",
      "Epoch 11: loss improved from 2552721.00000 to 2550294.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550294.5000\n",
      "Epoch 12/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2544873.0000\n",
      "Epoch 12: loss improved from 2550294.50000 to 2544411.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2544411.2500\n",
      "Epoch 13/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2550150.5000\n",
      "Epoch 13: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550163.0000\n",
      "Epoch 14/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2550482.5000\n",
      "Epoch 14: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550192.7500\n",
      "Epoch 15/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2547265.7500\n",
      "Epoch 15: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2547265.7500\n",
      "Epoch 16/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2546884.5000\n",
      "Epoch 16: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2546884.5000\n",
      "Epoch 17/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2548866.0000\n",
      "Epoch 17: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2548858.2500\n",
      "Epoch 18/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2550459.7500\n",
      "Epoch 18: loss did not improve from 2544411.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2549970.0000\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2541015.0000\n",
      "Epoch 19: loss improved from 2544411.25000 to 2542175.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542175.2500\n",
      "Epoch 20/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2543735.0000\n",
      "Epoch 20: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2544508.7500\n",
      "Epoch 21/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2543376.7500\n",
      "Epoch 21: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542993.2500\n",
      "Epoch 22/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2541304.0000\n",
      "Epoch 22: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543724.2500\n",
      "Epoch 23/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2543210.0000\n",
      "Epoch 23: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543637.5000\n",
      "Epoch 24/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2543198.2500\n",
      "Epoch 24: loss did not improve from 2542175.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545746.2500\n",
      "Epoch 25/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2537889.2500\n",
      "Epoch 25: loss improved from 2542175.25000 to 2537889.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537889.2500\n",
      "Epoch 26/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2541069.0000\n",
      "Epoch 26: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542186.0000\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2543958.7500\n",
      "Epoch 27: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543099.0000\n",
      "Epoch 28/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2540179.2500\n",
      "Epoch 28: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2542260.7500\n",
      "Epoch 29/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2539776.7500\n",
      "Epoch 29: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539755.0000\n",
      "Epoch 30/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2546314.5000\n",
      "Epoch 30: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545942.7500\n",
      "Epoch 31/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2545421.7500\n",
      "Epoch 31: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545172.0000\n",
      "Epoch 32/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2539523.5000\n",
      "Epoch 32: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539350.0000\n",
      "Epoch 33/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2539235.5000\n",
      "Epoch 33: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539418.7500\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2540374.5000\n",
      "Epoch 34: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540551.5000\n",
      "Epoch 35/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2543186.2500\n",
      "Epoch 35: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542569.5000\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2542450.2500\n",
      "Epoch 36: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542029.5000\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2540752.2500\n",
      "Epoch 37: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540988.7500\n",
      "Epoch 38/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2536605.5000\n",
      "Epoch 38: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2538654.2500\n",
      "Epoch 39/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2537843.2500\n",
      "Epoch 39: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539115.2500\n",
      "Epoch 40/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2538567.5000\n",
      "Epoch 40: loss did not improve from 2537889.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539366.2500\n",
      "Epoch 41/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2534054.5000\n",
      "Epoch 41: loss improved from 2537889.25000 to 2536399.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2536399.5000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2540317.0000\n",
      "Epoch 42: loss did not improve from 2536399.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539422.2500\n",
      "Epoch 43/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2539298.5000\n",
      "Epoch 43: loss did not improve from 2536399.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540481.2500\n",
      "Epoch 44/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2539064.2500\n",
      "Epoch 44: loss did not improve from 2536399.50000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2540025.5000\n",
      "Epoch 45/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2530805.2500\n",
      "Epoch 45: loss improved from 2536399.50000 to 2530491.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530491.7500\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2541222.0000\n",
      "Epoch 46: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539527.2500\n",
      "Epoch 47/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2533466.7500\n",
      "Epoch 47: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533899.0000\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2541820.5000\n",
      "Epoch 48: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2541665.0000\n",
      "Epoch 49/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2535560.2500\n",
      "Epoch 49: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534752.2500\n",
      "Epoch 50/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2539672.5000\n",
      "Epoch 50: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540001.5000\n",
      "Epoch 51/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2538582.7500\n",
      "Epoch 51: loss did not improve from 2530491.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2538850.5000\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2530526.7500\n",
      "Epoch 52: loss improved from 2530491.75000 to 2529932.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2529932.0000\n",
      "Epoch 53/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2534331.7500\n",
      "Epoch 53: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533585.7500\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2540037.2500\n",
      "Epoch 54: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2539079.0000\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2534888.2500\n",
      "Epoch 55: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535211.5000\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2537379.7500\n",
      "Epoch 56: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537371.5000\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2537716.0000\n",
      "Epoch 57: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2538415.5000\n",
      "Epoch 58/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2535982.7500\n",
      "Epoch 58: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537231.0000\n",
      "Epoch 59/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2532925.7500\n",
      "Epoch 59: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532877.7500\n",
      "Epoch 60/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2536129.2500\n",
      "Epoch 60: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537519.5000\n",
      "Epoch 61/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2538296.2500\n",
      "Epoch 61: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537590.0000\n",
      "Epoch 62/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2536066.2500\n",
      "Epoch 62: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2536390.5000\n",
      "Epoch 63/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2535056.0000\n",
      "Epoch 63: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533075.2500\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2536087.7500\n",
      "Epoch 64: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2536087.7500\n",
      "Epoch 65/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2535831.2500\n",
      "Epoch 65: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535814.7500\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2537379.2500\n",
      "Epoch 66: loss did not improve from 2529932.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537379.2500\n",
      "Epoch 67/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2531078.7500\n",
      "Epoch 67: loss improved from 2529932.00000 to 2529853.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2529853.7500\n",
      "Epoch 68/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2529779.2500\n",
      "Epoch 68: loss improved from 2529853.75000 to 2529666.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2529666.0000\n",
      "Epoch 69/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2531784.2500\n",
      "Epoch 69: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531378.5000\n",
      "Epoch 70/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533859.2500\n",
      "Epoch 70: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533634.5000\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2536495.5000\n",
      "Epoch 71: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2536495.5000\n",
      "Epoch 72/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2532836.2500\n",
      "Epoch 72: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2533415.2500\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2535951.7500\n",
      "Epoch 73: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535794.5000\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2534653.0000\n",
      "Epoch 74: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2534488.2500\n",
      "Epoch 75/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2532250.0000\n",
      "Epoch 75: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2531769.2500\n",
      "Epoch 76/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2535480.7500\n",
      "Epoch 76: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2533113.0000\n",
      "Epoch 77/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2533062.7500\n",
      "Epoch 77: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532381.7500\n",
      "Epoch 78/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2534161.7500\n",
      "Epoch 78: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533730.7500\n",
      "Epoch 79/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2535099.7500\n",
      "Epoch 79: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534117.5000\n",
      "Epoch 80/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2531684.5000\n",
      "Epoch 80: loss did not improve from 2529666.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531555.0000\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2529047.0000\n",
      "Epoch 81: loss improved from 2529666.00000 to 2528561.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2528561.2500\n",
      "Epoch 82/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2528095.2500\n",
      "Epoch 82: loss improved from 2528561.25000 to 2528100.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2528100.7500\n",
      "Epoch 83/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2533397.0000\n",
      "Epoch 83: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533097.2500\n",
      "Epoch 84/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2531031.5000\n",
      "Epoch 84: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531406.5000\n",
      "Epoch 85/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2533089.0000\n",
      "Epoch 85: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2532338.2500\n",
      "Epoch 86/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2530725.5000\n",
      "Epoch 86: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532256.0000\n",
      "Epoch 87/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2533558.2500\n",
      "Epoch 87: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533733.7500\n",
      "Epoch 88/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2532143.7500\n",
      "Epoch 88: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532069.7500\n",
      "Epoch 89/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2530272.5000\n",
      "Epoch 89: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530774.0000\n",
      "Epoch 90/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2533151.0000\n",
      "Epoch 90: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533311.0000\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2532821.7500\n",
      "Epoch 91: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531932.0000\n",
      "Epoch 92/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2535044.0000\n",
      "Epoch 92: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534246.2500\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2531134.5000\n",
      "Epoch 93: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532333.2500\n",
      "Epoch 94/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2534269.2500\n",
      "Epoch 94: loss did not improve from 2528100.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533561.5000\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2526682.7500\n",
      "Epoch 95: loss improved from 2528100.75000 to 2526656.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526656.0000\n",
      "Epoch 96/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2533993.0000\n",
      "Epoch 96: loss did not improve from 2526656.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534799.0000\n",
      "Epoch 97/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2533275.7500\n",
      "Epoch 97: loss did not improve from 2526656.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535122.0000\n",
      "Epoch 98/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2534241.5000\n",
      "Epoch 98: loss did not improve from 2526656.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533524.5000\n",
      "Epoch 99/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2525821.0000\n",
      "Epoch 99: loss improved from 2526656.00000 to 2526289.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526289.2500\n",
      "Epoch 100/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2527671.5000\n",
      "Epoch 100: loss did not improve from 2526289.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2529752.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:56:22 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 20:56:22 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpocstk9yw\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 20:57:07 INFO mlflow.tracking.fluent: Experiment with name 'dnn2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2633882.0000\n",
      "Epoch 1: loss improved from inf to 2635471.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 5ms/step - loss: 2635471.0000\n",
      "Epoch 2/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2398270.7500\n",
      "Epoch 2: loss improved from 2635471.00000 to 2396747.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2396747.0000\n",
      "Epoch 3/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2368647.5000\n",
      "Epoch 3: loss improved from 2396747.00000 to 2367878.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2367878.0000\n",
      "Epoch 4/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2354219.2500\n",
      "Epoch 4: loss improved from 2367878.00000 to 2355703.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2355703.5000\n",
      "Epoch 5/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2346573.2500\n",
      "Epoch 5: loss improved from 2355703.50000 to 2346131.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2346131.0000\n",
      "Epoch 6/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2339683.0000\n",
      "Epoch 6: loss improved from 2346131.00000 to 2338758.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2338758.2500\n",
      "Epoch 7/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2334505.7500\n",
      "Epoch 7: loss improved from 2338758.25000 to 2333098.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2333098.5000\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2330694.7500\n",
      "Epoch 8: loss improved from 2333098.50000 to 2329934.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2329934.0000\n",
      "Epoch 9/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2325806.7500\n",
      "Epoch 9: loss improved from 2329934.00000 to 2325310.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2325310.5000\n",
      "Epoch 10/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2321642.7500\n",
      "Epoch 10: loss improved from 2325310.50000 to 2320962.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2320962.5000\n",
      "Epoch 11/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2319119.0000\n",
      "Epoch 11: loss improved from 2320962.50000 to 2318872.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2318872.7500\n",
      "Epoch 12/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2315351.7500\n",
      "Epoch 12: loss improved from 2318872.75000 to 2315235.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2315235.7500\n",
      "Epoch 13/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2314288.2500\n",
      "Epoch 13: loss improved from 2315235.75000 to 2314233.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2314233.0000\n",
      "Epoch 14/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2312455.5000\n",
      "Epoch 14: loss improved from 2314233.00000 to 2311989.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2311989.0000\n",
      "Epoch 15/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2308825.0000\n",
      "Epoch 15: loss improved from 2311989.00000 to 2308520.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2308520.7500\n",
      "Epoch 16/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2307503.2500\n",
      "Epoch 16: loss improved from 2308520.75000 to 2307393.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2307393.0000\n",
      "Epoch 17/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2306250.5000\n",
      "Epoch 17: loss improved from 2307393.00000 to 2305392.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2305392.5000\n",
      "Epoch 18/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2303366.0000\n",
      "Epoch 18: loss improved from 2305392.50000 to 2304378.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2304378.0000\n",
      "Epoch 19/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2299146.2500\n",
      "Epoch 19: loss improved from 2304378.00000 to 2300795.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2300795.0000\n",
      "Epoch 20/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2302018.7500\n",
      "Epoch 20: loss did not improve from 2300795.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2301410.2500\n",
      "Epoch 21/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2300627.5000\n",
      "Epoch 21: loss improved from 2300795.00000 to 2298646.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2298646.7500\n",
      "Epoch 22/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2296299.2500\n",
      "Epoch 22: loss improved from 2298646.75000 to 2298645.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2298645.0000\n",
      "Epoch 23/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2297722.2500\n",
      "Epoch 23: loss improved from 2298645.00000 to 2296935.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2296935.5000\n",
      "Epoch 24/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2295456.7500\n",
      "Epoch 24: loss improved from 2296935.50000 to 2295484.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2295484.2500\n",
      "Epoch 25/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2292931.0000\n",
      "Epoch 25: loss improved from 2295484.25000 to 2294161.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2294161.5000\n",
      "Epoch 26/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2291080.0000\n",
      "Epoch 26: loss improved from 2294161.50000 to 2292463.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2292463.5000\n",
      "Epoch 27/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2289703.7500\n",
      "Epoch 27: loss improved from 2292463.50000 to 2291402.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2291402.7500\n",
      "Epoch 28/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2292468.2500\n",
      "Epoch 28: loss did not improve from 2291402.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2292068.7500\n",
      "Epoch 29/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2290989.7500\n",
      "Epoch 29: loss improved from 2291402.75000 to 2290971.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2290971.7500\n",
      "Epoch 30/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2288405.0000\n",
      "Epoch 30: loss improved from 2290971.75000 to 2288543.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2288543.2500\n",
      "Epoch 31/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2291767.2500\n",
      "Epoch 31: loss did not improve from 2288543.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2290283.0000\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2288279.5000\n",
      "Epoch 32: loss improved from 2288543.25000 to 2288279.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2288279.5000\n",
      "Epoch 33/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2288796.7500\n",
      "Epoch 33: loss did not improve from 2288279.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2288406.7500\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2285637.2500\n",
      "Epoch 34: loss improved from 2288279.50000 to 2285637.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285637.2500\n",
      "Epoch 35/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2284671.0000\n",
      "Epoch 35: loss improved from 2285637.25000 to 2285373.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285373.0000\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2285428.2500\n",
      "Epoch 36: loss did not improve from 2285373.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285721.5000\n",
      "Epoch 37/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2284894.0000\n",
      "Epoch 37: loss improved from 2285373.00000 to 2283441.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2283441.0000\n",
      "Epoch 38/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2285550.5000\n",
      "Epoch 38: loss did not improve from 2283441.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2284036.2500\n",
      "Epoch 39/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2284774.2500\n",
      "Epoch 39: loss did not improve from 2283441.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2284103.7500\n",
      "Epoch 40/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2285122.7500\n",
      "Epoch 40: loss did not improve from 2283441.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285600.0000\n",
      "Epoch 41/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2283392.7500\n",
      "Epoch 41: loss improved from 2283441.00000 to 2283145.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2283145.2500\n",
      "Epoch 42/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2281241.5000\n",
      "Epoch 42: loss did not improve from 2283145.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2283522.7500\n",
      "Epoch 43/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2283041.2500\n",
      "Epoch 43: loss improved from 2283145.25000 to 2282808.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2282808.0000\n",
      "Epoch 44/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2283772.2500\n",
      "Epoch 44: loss improved from 2282808.00000 to 2282717.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2282717.0000\n",
      "Epoch 45/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2279813.2500\n",
      "Epoch 45: loss improved from 2282717.00000 to 2281592.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2281592.2500\n",
      "Epoch 46/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2279946.5000\n",
      "Epoch 46: loss improved from 2281592.25000 to 2279809.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2279809.2500\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2280282.5000\n",
      "Epoch 47: loss did not improve from 2279809.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2280282.5000\n",
      "Epoch 48/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2280597.7500\n",
      "Epoch 48: loss did not improve from 2279809.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2279999.0000\n",
      "Epoch 49/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2278970.7500\n",
      "Epoch 49: loss improved from 2279809.25000 to 2278450.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2278450.7500\n",
      "Epoch 50/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2277661.2500\n",
      "Epoch 50: loss improved from 2278450.75000 to 2278334.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2278334.7500\n",
      "Epoch 51/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2277792.2500\n",
      "Epoch 51: loss did not improve from 2278334.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2279182.0000\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2278900.0000\n",
      "Epoch 52: loss did not improve from 2278334.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278738.5000\n",
      "Epoch 53/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2277761.5000\n",
      "Epoch 53: loss improved from 2278334.75000 to 2278180.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2278180.0000\n",
      "Epoch 54/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2280972.2500\n",
      "Epoch 54: loss did not improve from 2278180.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2278350.0000\n",
      "Epoch 55/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2279078.7500\n",
      "Epoch 55: loss did not improve from 2278180.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2278211.2500\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2276097.7500\n",
      "Epoch 56: loss improved from 2278180.00000 to 2277133.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2277133.0000\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2274511.2500\n",
      "Epoch 57: loss improved from 2277133.00000 to 2274511.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2274511.2500\n",
      "Epoch 58/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2278216.0000\n",
      "Epoch 58: loss did not improve from 2274511.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2277306.2500\n",
      "Epoch 59/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2275222.2500\n",
      "Epoch 59: loss did not improve from 2274511.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2275531.0000\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2275599.5000\n",
      "Epoch 60: loss did not improve from 2274511.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2275786.2500\n",
      "Epoch 61/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2273569.7500\n",
      "Epoch 61: loss improved from 2274511.25000 to 2273741.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273741.0000\n",
      "Epoch 62/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2276052.0000\n",
      "Epoch 62: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2275554.7500\n",
      "Epoch 63/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2276251.2500\n",
      "Epoch 63: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2274756.0000\n",
      "Epoch 64/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2273025.5000\n",
      "Epoch 64: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273875.0000\n",
      "Epoch 65/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2275852.7500\n",
      "Epoch 65: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2275430.5000\n",
      "Epoch 66/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2273435.0000\n",
      "Epoch 66: loss did not improve from 2273741.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2273843.2500\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2272648.7500\n",
      "Epoch 67: loss improved from 2273741.00000 to 2271974.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271974.2500\n",
      "Epoch 68/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2274732.2500\n",
      "Epoch 68: loss did not improve from 2271974.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2272814.0000\n",
      "Epoch 69/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2271918.7500\n",
      "Epoch 69: loss did not improve from 2271974.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273238.0000\n",
      "Epoch 70/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2272706.2500\n",
      "Epoch 70: loss did not improve from 2271974.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2272616.0000\n",
      "Epoch 71/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2274124.7500\n",
      "Epoch 71: loss did not improve from 2271974.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2273407.7500\n",
      "Epoch 72/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2272333.5000\n",
      "Epoch 72: loss improved from 2271974.25000 to 2271594.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271594.7500\n",
      "Epoch 73/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2273740.2500\n",
      "Epoch 73: loss did not improve from 2271594.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2272184.0000\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2271909.7500\n",
      "Epoch 74: loss did not improve from 2271594.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271909.7500\n",
      "Epoch 75/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2271794.7500\n",
      "Epoch 75: loss did not improve from 2271594.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2271852.5000\n",
      "Epoch 76/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2269669.2500\n",
      "Epoch 76: loss did not improve from 2271594.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271896.7500\n",
      "Epoch 77/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2272002.2500\n",
      "Epoch 77: loss improved from 2271594.75000 to 2270983.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2270983.7500\n",
      "Epoch 78/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2269450.2500\n",
      "Epoch 78: loss improved from 2270983.75000 to 2269539.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2269539.7500\n",
      "Epoch 79/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2271268.5000\n",
      "Epoch 79: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2270258.7500\n",
      "Epoch 80/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2270772.0000\n",
      "Epoch 80: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2270063.2500\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2269832.0000\n",
      "Epoch 81: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2269832.0000\n",
      "Epoch 82/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2270705.5000\n",
      "Epoch 82: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2269790.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2270930.2500\n",
      "Epoch 83: loss did not improve from 2269539.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2270257.0000\n",
      "Epoch 84/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2266878.2500\n",
      "Epoch 84: loss improved from 2269539.75000 to 2268292.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2268292.2500\n",
      "Epoch 85/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2269389.2500\n",
      "Epoch 85: loss did not improve from 2268292.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2269375.0000\n",
      "Epoch 86/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2269458.2500\n",
      "Epoch 86: loss did not improve from 2268292.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2269348.5000\n",
      "Epoch 87/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2269142.0000\n",
      "Epoch 87: loss improved from 2268292.25000 to 2268007.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2268007.2500\n",
      "Epoch 88/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2271281.7500\n",
      "Epoch 88: loss did not improve from 2268007.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2269995.5000\n",
      "Epoch 89/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2268491.7500\n",
      "Epoch 89: loss improved from 2268007.25000 to 2267922.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2267922.0000\n",
      "Epoch 90/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2267615.5000\n",
      "Epoch 90: loss did not improve from 2267922.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2269194.0000\n",
      "Epoch 91/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2269110.7500\n",
      "Epoch 91: loss did not improve from 2267922.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2268487.0000\n",
      "Epoch 92/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2268369.7500\n",
      "Epoch 92: loss did not improve from 2267922.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2268914.0000\n",
      "Epoch 93/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2269632.7500\n",
      "Epoch 93: loss did not improve from 2267922.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268345.0000\n",
      "Epoch 94/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2266565.0000\n",
      "Epoch 94: loss improved from 2267922.00000 to 2267308.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2267308.7500\n",
      "Epoch 95/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2268589.5000\n",
      "Epoch 95: loss did not improve from 2267308.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2268498.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2266532.7500\n",
      "Epoch 96: loss improved from 2267308.75000 to 2266532.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2266532.7500\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2267488.7500\n",
      "Epoch 97: loss did not improve from 2266532.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2267488.7500\n",
      "Epoch 98/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2267113.5000\n",
      "Epoch 98: loss did not improve from 2266532.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2266820.5000\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2267313.0000\n",
      "Epoch 99: loss did not improve from 2266532.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2267337.0000\n",
      "Epoch 100/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2267190.0000\n",
      "Epoch 100: loss did not improve from 2266532.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2266787.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 21:16:20 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 21:16:20 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_okbdfa6\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 21:16:53 INFO mlflow.tracking.fluent: Experiment with name 'dnn2vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2645581.7500\n",
      "Epoch 1: loss improved from inf to 2645760.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2645760.0000\n",
      "Epoch 2/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2400213.0000\n",
      "Epoch 2: loss improved from 2645760.00000 to 2400359.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2400359.5000\n",
      "Epoch 3/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2368283.0000\n",
      "Epoch 3: loss improved from 2400359.50000 to 2368649.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2368649.5000\n",
      "Epoch 4/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2357101.7500\n",
      "Epoch 4: loss improved from 2368649.50000 to 2356814.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2356814.7500\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2346609.2500\n",
      "Epoch 5: loss improved from 2356814.75000 to 2346301.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2346301.5000\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2339584.0000\n",
      "Epoch 6: loss improved from 2346301.50000 to 2339584.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2339584.0000\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2332570.0000\n",
      "Epoch 7: loss improved from 2339584.00000 to 2332570.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2332570.0000\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2329626.5000\n",
      "Epoch 8: loss improved from 2332570.00000 to 2329201.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2329201.7500\n",
      "Epoch 9/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2322918.0000\n",
      "Epoch 9: loss improved from 2329201.75000 to 2324004.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2324004.7500\n",
      "Epoch 10/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2319944.0000\n",
      "Epoch 10: loss improved from 2324004.75000 to 2319830.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2319830.5000\n",
      "Epoch 11/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2319237.0000\n",
      "Epoch 11: loss improved from 2319830.50000 to 2318743.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2318743.7500\n",
      "Epoch 12/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2317261.2500\n",
      "Epoch 12: loss improved from 2318743.75000 to 2315727.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2315727.2500\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2310332.7500\n",
      "Epoch 13: loss improved from 2315727.25000 to 2310332.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2310332.7500\n",
      "Epoch 14/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2313438.7500\n",
      "Epoch 14: loss did not improve from 2310332.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2312906.2500\n",
      "Epoch 15/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2309765.5000\n",
      "Epoch 15: loss improved from 2310332.75000 to 2309442.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2309442.0000\n",
      "Epoch 16/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2306582.2500\n",
      "Epoch 16: loss improved from 2309442.00000 to 2305531.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2305531.5000\n",
      "Epoch 17/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2306386.2500\n",
      "Epoch 17: loss improved from 2305531.50000 to 2305517.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2305517.7500\n",
      "Epoch 18/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2304106.0000\n",
      "Epoch 18: loss improved from 2305517.75000 to 2302980.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2302980.2500\n",
      "Epoch 19/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2303695.0000\n",
      "Epoch 19: loss improved from 2302980.25000 to 2302705.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2302705.2500\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2300536.7500\n",
      "Epoch 20: loss improved from 2302705.25000 to 2299935.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2299935.7500\n",
      "Epoch 21/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2299573.0000\n",
      "Epoch 21: loss improved from 2299935.75000 to 2299547.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2299547.2500\n",
      "Epoch 22/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2298933.7500\n",
      "Epoch 22: loss did not improve from 2299547.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2299797.5000\n",
      "Epoch 23/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2298049.0000\n",
      "Epoch 23: loss improved from 2299547.25000 to 2297364.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2297364.5000\n",
      "Epoch 24/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2295872.0000\n",
      "Epoch 24: loss improved from 2297364.50000 to 2295755.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2295755.2500\n",
      "Epoch 25/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2295252.5000\n",
      "Epoch 25: loss improved from 2295755.25000 to 2294586.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2294586.5000\n",
      "Epoch 26/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2295299.5000\n",
      "Epoch 26: loss improved from 2294586.50000 to 2294082.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2294082.5000\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2293244.0000\n",
      "Epoch 27: loss improved from 2294082.50000 to 2293944.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2293944.7500\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2292270.0000\n",
      "Epoch 28: loss improved from 2293944.75000 to 2291789.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2291789.7500\n",
      "Epoch 29/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2291282.2500\n",
      "Epoch 29: loss improved from 2291789.75000 to 2291558.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2291558.5000\n",
      "Epoch 30/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2292765.2500\n",
      "Epoch 30: loss did not improve from 2291558.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2292115.5000\n",
      "Epoch 31/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2288753.0000\n",
      "Epoch 31: loss improved from 2291558.50000 to 2288095.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2288095.7500\n",
      "Epoch 32/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2287613.5000\n",
      "Epoch 32: loss did not improve from 2288095.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2288992.7500\n",
      "Epoch 33/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2287444.5000\n",
      "Epoch 33: loss improved from 2288095.75000 to 2287347.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2287347.0000\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2286793.0000\n",
      "Epoch 34: loss improved from 2287347.00000 to 2286743.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2286743.7500\n",
      "Epoch 35/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2284347.7500\n",
      "Epoch 35: loss improved from 2286743.75000 to 2285348.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2285348.7500\n",
      "Epoch 36/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2286483.0000\n",
      "Epoch 36: loss did not improve from 2285348.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2286471.0000\n",
      "Epoch 37/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2283362.7500\n",
      "Epoch 37: loss improved from 2285348.75000 to 2283759.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2283759.7500\n",
      "Epoch 38/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2285024.7500\n",
      "Epoch 38: loss did not improve from 2283759.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2284687.5000\n",
      "Epoch 39/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2282314.7500\n",
      "Epoch 39: loss improved from 2283759.75000 to 2282611.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2282611.2500\n",
      "Epoch 40/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2284768.2500\n",
      "Epoch 40: loss did not improve from 2282611.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2284599.7500\n",
      "Epoch 41/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2283117.5000\n",
      "Epoch 41: loss did not improve from 2282611.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2283188.0000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2283790.0000\n",
      "Epoch 42: loss did not improve from 2282611.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2282726.5000\n",
      "Epoch 43/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2280445.2500\n",
      "Epoch 43: loss improved from 2282611.25000 to 2280891.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2280891.2500\n",
      "Epoch 44/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2280227.2500\n",
      "Epoch 44: loss improved from 2280891.25000 to 2280270.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2280270.2500\n",
      "Epoch 45/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2279306.0000\n",
      "Epoch 45: loss improved from 2280270.25000 to 2279893.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2279893.0000\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2279894.2500\n",
      "Epoch 46: loss did not improve from 2279893.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2280298.0000\n",
      "Epoch 47/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2278948.5000\n",
      "Epoch 47: loss improved from 2279893.00000 to 2278727.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2278727.0000\n",
      "Epoch 48/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2279877.5000\n",
      "Epoch 48: loss did not improve from 2278727.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2279851.0000\n",
      "Epoch 49/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2279470.5000\n",
      "Epoch 49: loss improved from 2278727.00000 to 2278365.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2278365.0000\n",
      "Epoch 50/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2276905.2500\n",
      "Epoch 50: loss improved from 2278365.00000 to 2278344.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2278344.0000\n",
      "Epoch 51/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2279330.2500\n",
      "Epoch 51: loss did not improve from 2278344.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2278576.7500\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2279425.0000\n",
      "Epoch 52: loss did not improve from 2278344.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2278945.5000\n",
      "Epoch 53/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2278566.0000\n",
      "Epoch 53: loss improved from 2278344.00000 to 2277824.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2277824.0000\n",
      "Epoch 54/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2276741.2500\n",
      "Epoch 54: loss improved from 2277824.00000 to 2276823.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2276823.0000\n",
      "Epoch 55/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2278273.0000\n",
      "Epoch 55: loss did not improve from 2276823.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2277484.5000\n",
      "Epoch 56/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2276941.7500\n",
      "Epoch 56: loss improved from 2276823.00000 to 2276572.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2276572.5000\n",
      "Epoch 57/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2276221.2500\n",
      "Epoch 57: loss improved from 2276572.50000 to 2275814.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2275814.2500\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2276327.5000\n",
      "Epoch 58: loss did not improve from 2275814.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2276327.5000\n",
      "Epoch 59/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2275082.2500\n",
      "Epoch 59: loss improved from 2275814.25000 to 2273982.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2273982.7500\n",
      "Epoch 60/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2276369.2500\n",
      "Epoch 60: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2275938.5000\n",
      "Epoch 61/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2274036.5000\n",
      "Epoch 61: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2274488.0000\n",
      "Epoch 62/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2275049.0000\n",
      "Epoch 62: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2274910.5000\n",
      "Epoch 63/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2274315.7500\n",
      "Epoch 63: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2275171.0000\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2274760.2500\n",
      "Epoch 64: loss did not improve from 2273982.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2274760.2500\n",
      "Epoch 65/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2273830.7500\n",
      "Epoch 65: loss improved from 2273982.75000 to 2273830.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273830.7500\n",
      "Epoch 66/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2273736.5000\n",
      "Epoch 66: loss did not improve from 2273830.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2273956.5000\n",
      "Epoch 67/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2271836.7500\n",
      "Epoch 67: loss improved from 2273830.75000 to 2271836.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2271836.7500\n",
      "Epoch 68/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2273616.7500\n",
      "Epoch 68: loss did not improve from 2271836.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2273862.0000\n",
      "Epoch 69/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2271580.7500\n",
      "Epoch 69: loss did not improve from 2271836.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2272092.0000\n",
      "Epoch 70/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2270828.5000\n",
      "Epoch 70: loss improved from 2271836.75000 to 2270497.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2270497.7500\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2271861.7500\n",
      "Epoch 71: loss did not improve from 2270497.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271810.7500\n",
      "Epoch 72/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2273444.5000\n",
      "Epoch 72: loss did not improve from 2270497.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2272644.7500\n",
      "Epoch 73/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2271182.2500\n",
      "Epoch 73: loss did not improve from 2270497.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271935.5000\n",
      "Epoch 74/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2269609.2500\n",
      "Epoch 74: loss did not improve from 2270497.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2271608.5000\n",
      "Epoch 75/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2265405.7500\n",
      "Epoch 75: loss improved from 2270497.75000 to 2270370.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2270370.5000\n",
      "Epoch 76/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2270036.7500\n",
      "Epoch 76: loss did not improve from 2270370.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2271592.5000\n",
      "Epoch 77/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2270001.7500\n",
      "Epoch 77: loss did not improve from 2270370.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2270530.5000\n",
      "Epoch 78/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2269865.0000\n",
      "Epoch 78: loss improved from 2270370.50000 to 2269874.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2269874.7500\n",
      "Epoch 79/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2271083.0000\n",
      "Epoch 79: loss did not improve from 2269874.75000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2270403.7500\n",
      "Epoch 80/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2270276.5000\n",
      "Epoch 80: loss improved from 2269874.75000 to 2269729.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2269729.5000\n",
      "Epoch 81/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2270292.2500\n",
      "Epoch 81: loss did not improve from 2269729.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2269781.7500\n",
      "Epoch 82/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2267466.2500\n",
      "Epoch 82: loss improved from 2269729.50000 to 2268590.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2268590.2500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2269210.5000\n",
      "Epoch 83: loss did not improve from 2268590.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269589.7500\n",
      "Epoch 84/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2271159.7500\n",
      "Epoch 84: loss did not improve from 2268590.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270608.0000\n",
      "Epoch 85/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2270096.7500\n",
      "Epoch 85: loss did not improve from 2268590.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269009.2500\n",
      "Epoch 86/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2268498.2500\n",
      "Epoch 86: loss did not improve from 2268590.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269357.2500\n",
      "Epoch 87/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2268273.7500\n",
      "Epoch 87: loss improved from 2268590.25000 to 2268481.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268481.5000\n",
      "Epoch 88/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2267695.5000\n",
      "Epoch 88: loss did not improve from 2268481.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269072.5000\n",
      "Epoch 89/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2267895.0000\n",
      "Epoch 89: loss improved from 2268481.50000 to 2267288.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267288.5000\n",
      "Epoch 90/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2268530.7500\n",
      "Epoch 90: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268226.5000\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2268464.0000\n",
      "Epoch 91: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268464.0000\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2267702.5000\n",
      "Epoch 92: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267666.0000\n",
      "Epoch 93/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2268370.0000\n",
      "Epoch 93: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267728.7500\n",
      "Epoch 94/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2268347.5000\n",
      "Epoch 94: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268810.2500\n",
      "Epoch 95/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2269824.0000\n",
      "Epoch 95: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267892.5000\n",
      "Epoch 96/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2267496.2500\n",
      "Epoch 96: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267792.2500\n",
      "Epoch 97/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2267814.5000\n",
      "Epoch 97: loss did not improve from 2267288.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268282.0000\n",
      "Epoch 98/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2268472.2500\n",
      "Epoch 98: loss improved from 2267288.50000 to 2267237.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267237.0000\n",
      "Epoch 99/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2266674.0000\n",
      "Epoch 99: loss improved from 2267237.00000 to 2266091.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2266091.0000\n",
      "Epoch 100/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2265527.0000\n",
      "Epoch 100: loss did not improve from 2266091.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266925.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 21:35:19 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 21:35:19 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp8_xk00mc\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 21:39:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp8_xk00mc\\model, flavor: tensorflow), fall back to return ['tensorflow==2.9.0']. Set logging level to DEBUG to see the full traceback.\n",
      "2023/05/26 21:39:42 INFO mlflow.tracking.fluent: Experiment with name 'cnn1vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 25, 25, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 12, 12, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736766.5000\n",
      "Epoch 1: loss improved from inf to 2736991.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 206s 14ms/step - loss: 2736991.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2737436.7500\n",
      "Epoch 2: loss improved from 2736991.50000 to 2736504.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736504.5000\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2734603.0000\n",
      "Epoch 3: loss did not improve from 2736504.50000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736505.2500\n",
      "Epoch 4/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736733.7500\n",
      "Epoch 4: loss improved from 2736504.50000 to 2736502.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.5000\n",
      "Epoch 5/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736612.7500\n",
      "Epoch 5: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736505.0000\n",
      "Epoch 6/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2735858.5000\n",
      "Epoch 6: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736506.2500\n",
      "Epoch 7/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2738445.0000\n",
      "Epoch 7: loss improved from 2736502.50000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.2500\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737184.2500\n",
      "Epoch 8: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.7500\n",
      "Epoch 9/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 9: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736504.0000\n",
      "Epoch 10/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 10: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.5000\n",
      "Epoch 11/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735303.2500\n",
      "Epoch 11: loss improved from 2736502.25000 to 2736500.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736500.7500\n",
      "Epoch 12/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736135.5000\n",
      "Epoch 12: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736501.2500\n",
      "Epoch 13/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737211.2500\n",
      "Epoch 13: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.2500\n",
      "Epoch 14/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736218.7500\n",
      "Epoch 14: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.7500\n",
      "Epoch 15/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736961.5000\n",
      "Epoch 15: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.5000\n",
      "Epoch 16/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2734565.0000\n",
      "Epoch 16: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736501.2500\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736825.5000\n",
      "Epoch 17: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.7500\n",
      "Epoch 18/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735272.0000\n",
      "Epoch 18: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736500.7500\n",
      "Epoch 19/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736732.7500\n",
      "Epoch 19: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.7500\n",
      "Epoch 20/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737526.0000\n",
      "Epoch 20: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736502.2500\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.5000\n",
      "Epoch 21: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736501.5000\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735358.5000\n",
      "Epoch 22: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.7500\n",
      "Epoch 23/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737107.7500\n",
      "Epoch 23: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.5000\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737957.0000\n",
      "Epoch 24: loss improved from 2736500.75000 to 2736499.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736499.2500\n",
      "Epoch 25/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2733744.5000\n",
      "Epoch 25: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.5000\n",
      "Epoch 26/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736697.2500\n",
      "Epoch 26: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736501.5000\n",
      "Epoch 27/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736757.2500\n",
      "Epoch 27: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.5000\n",
      "Epoch 28/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735465.2500\n",
      "Epoch 28: loss improved from 2736499.25000 to 2736499.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736499.0000\n",
      "Epoch 29/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736033.5000\n",
      "Epoch 29: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736500.2500\n",
      "Epoch 30/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735614.0000\n",
      "Epoch 30: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736503.5000\n",
      "Epoch 31/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736855.2500\n",
      "Epoch 31: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.7500\n",
      "Epoch 32/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736439.0000\n",
      "Epoch 32: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736502.5000\n",
      "Epoch 33/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737418.7500\n",
      "Epoch 33: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736502.5000\n",
      "Epoch 34/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737853.2500\n",
      "Epoch 34: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2736500.0000\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736110.2500\n",
      "Epoch 35: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 2736502.2500\n",
      "Epoch 36/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736844.7500\n",
      "Epoch 36: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736499.2500\n",
      "Epoch 37/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735962.7500\n",
      "Epoch 37: loss improved from 2736499.00000 to 2736498.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736498.2500\n",
      "Epoch 38/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736989.0000\n",
      "Epoch 38: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736503.0000\n",
      "Epoch 39/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735999.5000\n",
      "Epoch 39: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736502.2500\n",
      "Epoch 40/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737375.2500\n",
      "Epoch 40: loss improved from 2736498.25000 to 2736498.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736498.0000\n",
      "Epoch 41/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736828.0000\n",
      "Epoch 41: loss did not improve from 2736498.00000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.7500\n",
      "Epoch 42/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736773.5000\n",
      "Epoch 42: loss did not improve from 2736498.00000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736499.0000\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736954.0000\n",
      "Epoch 43: loss did not improve from 2736498.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736498.0000\n",
      "Epoch 44/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735662.7500\n",
      "Epoch 44: loss improved from 2736498.00000 to 2736497.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.7500\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736329.5000\n",
      "Epoch 45: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736498.2500\n",
      "Epoch 46/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736675.0000\n",
      "Epoch 46: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736500.7500\n",
      "Epoch 47/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2732057.5000\n",
      "Epoch 47: loss improved from 2736497.75000 to 2736496.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736496.7500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736757.7500\n",
      "Epoch 48: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736497.2500\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736497.2500\n",
      "Epoch 49: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.2500\n",
      "Epoch 50/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2738377.0000\n",
      "Epoch 50: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736500.2500\n",
      "Epoch 51/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735647.0000\n",
      "Epoch 51: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736499.2500\n",
      "Epoch 52/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736661.5000\n",
      "Epoch 52: loss improved from 2736496.75000 to 2736496.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736496.0000\n",
      "Epoch 53/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2734570.5000\n",
      "Epoch 53: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736497.7500\n",
      "Epoch 54/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736037.7500\n",
      "Epoch 54: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.7500\n",
      "Epoch 55/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736683.5000\n",
      "Epoch 55: loss improved from 2736496.00000 to 2736495.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736495.0000\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736405.5000\n",
      "Epoch 56: loss improved from 2736495.00000 to 2736492.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736492.7500\n",
      "Epoch 57/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735056.2500\n",
      "Epoch 57: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736496.0000\n",
      "Epoch 58/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736914.0000\n",
      "Epoch 58: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.7500\n",
      "Epoch 59/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2734132.2500\n",
      "Epoch 59: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.2500\n",
      "Epoch 60/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736407.2500\n",
      "Epoch 60: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736497.2500\n",
      "Epoch 61/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737259.5000\n",
      "Epoch 61: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.2500\n",
      "Epoch 62/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737449.7500\n",
      "Epoch 62: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736497.0000\n",
      "Epoch 63/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736497.2500\n",
      "Epoch 63: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 2736497.2500\n",
      "Epoch 64/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2737977.5000\n",
      "Epoch 64: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736494.7500\n",
      "Epoch 65/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735915.2500\n",
      "Epoch 65: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.0000\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737066.5000\n",
      "Epoch 66: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736886.2500\n",
      "Epoch 67: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.0000\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736145.2500\n",
      "Epoch 68: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 69/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737335.7500\n",
      "Epoch 69: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.0000\n",
      "Epoch 70/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737083.2500\n",
      "Epoch 70: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.0000\n",
      "Epoch 71/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737011.0000\n",
      "Epoch 71: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 72/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2735739.5000\n",
      "Epoch 72: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 73/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2738035.5000\n",
      "Epoch 73: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.5000\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737544.0000\n",
      "Epoch 74: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.7500\n",
      "Epoch 75/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2734051.7500\n",
      "Epoch 75: loss improved from 2736492.75000 to 2736492.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735564.0000\n",
      "Epoch 76: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 77/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2735283.7500\n",
      "Epoch 77: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 78/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736612.5000\n",
      "Epoch 78: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.7500\n",
      "Epoch 79/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2735346.2500\n",
      "Epoch 79: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.0000\n",
      "Epoch 80/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736455.2500\n",
      "Epoch 80: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736495.5000\n",
      "Epoch 81: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.5000\n",
      "Epoch 82/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736430.0000\n",
      "Epoch 82: loss improved from 2736492.25000 to 2736490.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.5000\n",
      "Epoch 83/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736491.2500\n",
      "Epoch 83: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.7500\n",
      "Epoch 84/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2735652.5000\n",
      "Epoch 84: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 85/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736491.2500\n",
      "Epoch 85: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.2500\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 86: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 87/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735152.5000\n",
      "Epoch 87: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.0000\n",
      "Epoch 88/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736010.7500\n",
      "Epoch 88: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.5000\n",
      "Epoch 89/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737226.2500\n",
      "Epoch 89: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.0000\n",
      "Epoch 90/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2735185.5000\n",
      "Epoch 90: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736387.5000\n",
      "Epoch 91: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 92/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736652.0000\n",
      "Epoch 92: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737311.2500\n",
      "Epoch 93: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 94/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736653.7500\n",
      "Epoch 94: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 95/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735650.2500\n",
      "Epoch 95: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 96/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737971.5000\n",
      "Epoch 96: loss did not improve from 2736490.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.5000\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736490.0000\n",
      "Epoch 97: loss improved from 2736490.50000 to 2736490.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.0000\n",
      "Epoch 98/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736342.0000\n",
      "Epoch 98: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.5000\n",
      "Epoch 99/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737115.2500\n",
      "Epoch 99: loss improved from 2736490.00000 to 2736489.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736489.5000\n",
      "Epoch 100/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737174.2500\n",
      "Epoch 100: loss improved from 2736489.50000 to 2736489.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736489.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 22:13:11 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 22:13:11 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7f4iasyi\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7f4iasyi\\model\\data\\model\\assets\n",
      "2023/05/26 22:14:01 INFO mlflow.tracking.fluent: Experiment with name 'cnn1vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 25, 25, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSampling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_4 (UpSampling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   3/3189 [..............................] - ETA: 6:00 - loss: 1797291.0000  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736875.5000\n",
      "Epoch 1: loss improved from inf to 2736508.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2736508.5000\n",
      "Epoch 2/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736613.0000\n",
      "Epoch 2: loss improved from 2736508.50000 to 2736507.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736507.0000\n",
      "Epoch 3/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2735947.2500\n",
      "Epoch 3: loss did not improve from 2736507.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736509.2500\n",
      "Epoch 4/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737578.5000\n",
      "Epoch 4: loss improved from 2736507.00000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.0000\n",
      "Epoch 5/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737905.5000\n",
      "Epoch 5: loss improved from 2736503.00000 to 2736502.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.5000\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.5000\n",
      "Epoch 6: loss improved from 2736502.50000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 7/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737234.0000\n",
      "Epoch 7: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736507.2500\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2702161.0000\n",
      "Epoch 8: loss improved from 2736501.50000 to 2703252.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2703252.7500\n",
      "Epoch 9/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2544300.2500\n",
      "Epoch 9: loss improved from 2703252.75000 to 2544025.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2544025.0000\n",
      "Epoch 10/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2391632.7500\n",
      "Epoch 10: loss improved from 2544025.00000 to 2391069.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2391069.7500\n",
      "Epoch 11/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2280872.2500\n",
      "Epoch 11: loss improved from 2391069.75000 to 2280697.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2280697.7500\n",
      "Epoch 12/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2195186.0000\n",
      "Epoch 12: loss improved from 2280697.75000 to 2194244.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2194244.2500\n",
      "Epoch 13/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2123835.7500\n",
      "Epoch 13: loss improved from 2194244.25000 to 2124353.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2124353.2500\n",
      "Epoch 14/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2076272.0000\n",
      "Epoch 14: loss improved from 2124353.25000 to 2075844.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2075844.6250\n",
      "Epoch 15/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2024277.2500\n",
      "Epoch 15: loss improved from 2075844.62500 to 2024038.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2024038.0000\n",
      "Epoch 16/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1984109.6250\n",
      "Epoch 16: loss improved from 2024038.00000 to 1983486.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1983486.3750\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1951336.5000\n",
      "Epoch 17: loss improved from 1983486.37500 to 1951176.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1951176.5000\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1924460.5000\n",
      "Epoch 18: loss improved from 1951176.50000 to 1924433.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1924433.7500\n",
      "Epoch 19/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1896129.0000\n",
      "Epoch 19: loss improved from 1924433.75000 to 1895131.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1895131.7500\n",
      "Epoch 20/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1868498.6250\n",
      "Epoch 20: loss improved from 1895131.75000 to 1868498.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 7ms/step - loss: 1868498.6250\n",
      "Epoch 21/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1848674.8750\n",
      "Epoch 21: loss improved from 1868498.62500 to 1848553.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1848553.3750\n",
      "Epoch 22/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1826879.3750\n",
      "Epoch 22: loss improved from 1848553.37500 to 1827353.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1827353.7500\n",
      "Epoch 23/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1806971.7500\n",
      "Epoch 23: loss improved from 1827353.75000 to 1806971.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1806971.7500\n",
      "Epoch 24/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1794231.6250\n",
      "Epoch 24: loss improved from 1806971.75000 to 1794087.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1794087.0000\n",
      "Epoch 25/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1773664.6250\n",
      "Epoch 25: loss improved from 1794087.00000 to 1773109.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1773109.7500\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1758569.3750\n",
      "Epoch 26: loss improved from 1773109.75000 to 1758569.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1758569.3750\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1743945.2500\n",
      "Epoch 27: loss improved from 1758569.37500 to 1745145.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1745145.5000\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1733679.6250\n",
      "Epoch 28: loss improved from 1745145.50000 to 1733679.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1733679.6250\n",
      "Epoch 29/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1720667.8750\n",
      "Epoch 29: loss improved from 1733679.62500 to 1720661.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1720661.3750\n",
      "Epoch 30/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1711864.7500\n",
      "Epoch 30: loss improved from 1720661.37500 to 1712243.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1712243.5000\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1697659.1250\n",
      "Epoch 31: loss improved from 1712243.50000 to 1697659.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1697659.1250\n",
      "Epoch 32/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1688557.0000\n",
      "Epoch 32: loss improved from 1697659.12500 to 1688670.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1688670.8750\n",
      "Epoch 33/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1686408.7500\n",
      "Epoch 33: loss improved from 1688670.87500 to 1685643.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1685643.6250\n",
      "Epoch 34/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1662932.0000\n",
      "Epoch 34: loss improved from 1685643.62500 to 1662951.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1662951.2500\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1662989.6250\n",
      "Epoch 35: loss did not improve from 1662951.25000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1662989.6250\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1653388.6250\n",
      "Epoch 36: loss improved from 1662951.25000 to 1653995.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1653995.7500\n",
      "Epoch 37/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1647153.0000\n",
      "Epoch 37: loss improved from 1653995.75000 to 1646347.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1646347.7500\n",
      "Epoch 38/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1637545.6250\n",
      "Epoch 38: loss improved from 1646347.75000 to 1637332.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1637332.3750\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1629795.1250\n",
      "Epoch 39: loss improved from 1637332.37500 to 1629795.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1629795.1250\n",
      "Epoch 40/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1624545.3750\n",
      "Epoch 40: loss improved from 1629795.12500 to 1624750.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1624750.0000\n",
      "Epoch 41/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1616648.8750\n",
      "Epoch 41: loss improved from 1624750.00000 to 1616941.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1616941.6250\n",
      "Epoch 42/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1610869.6250\n",
      "Epoch 42: loss improved from 1616941.62500 to 1610644.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1610644.7500\n",
      "Epoch 43/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1600616.0000\n",
      "Epoch 43: loss improved from 1610644.75000 to 1600779.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1600779.8750\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1599234.5000\n",
      "Epoch 44: loss improved from 1600779.87500 to 1598528.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1598528.0000\n",
      "Epoch 45/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1590304.6250\n",
      "Epoch 45: loss improved from 1598528.00000 to 1590419.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1590419.2500\n",
      "Epoch 46/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1587923.7500\n",
      "Epoch 46: loss improved from 1590419.25000 to 1587950.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1587950.8750\n",
      "Epoch 47/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1581815.0000\n",
      "Epoch 47: loss improved from 1587950.87500 to 1581532.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1581532.1250\n",
      "Epoch 48/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1577135.2500\n",
      "Epoch 48: loss improved from 1581532.12500 to 1577199.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1577199.1250\n",
      "Epoch 49/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1571683.2500\n",
      "Epoch 49: loss improved from 1577199.12500 to 1571051.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1571051.1250\n",
      "Epoch 50/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1566474.3750\n",
      "Epoch 50: loss improved from 1571051.12500 to 1566578.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1566578.5000\n",
      "Epoch 51/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1564313.3750\n",
      "Epoch 51: loss improved from 1566578.50000 to 1563697.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1563697.5000\n",
      "Epoch 52/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1558657.3750\n",
      "Epoch 52: loss improved from 1563697.50000 to 1558801.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1558801.7500\n",
      "Epoch 53/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1551732.1250\n",
      "Epoch 53: loss improved from 1558801.75000 to 1551622.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1551622.2500\n",
      "Epoch 54/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1550417.1250\n",
      "Epoch 54: loss improved from 1551622.25000 to 1550417.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1550417.1250\n",
      "Epoch 55/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1544959.0000\n",
      "Epoch 55: loss improved from 1550417.12500 to 1544914.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1544914.6250\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1545230.1250\n",
      "Epoch 56: loss did not improve from 1544914.62500\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1545516.2500\n",
      "Epoch 57/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1540253.7500\n",
      "Epoch 57: loss improved from 1544914.62500 to 1540438.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1540438.8750\n",
      "Epoch 58/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1533923.3750\n",
      "Epoch 58: loss improved from 1540438.87500 to 1533454.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 1533454.0000\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1530465.3750\n",
      "Epoch 59: loss improved from 1533454.00000 to 1530262.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1530262.3750\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1524705.7500\n",
      "Epoch 60: loss improved from 1530262.37500 to 1524705.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1524705.7500\n",
      "Epoch 61/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1525753.1250\n",
      "Epoch 61: loss did not improve from 1524705.75000\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1525780.7500\n",
      "Epoch 62/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1520279.5000\n",
      "Epoch 62: loss improved from 1524705.75000 to 1520303.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1520303.7500\n",
      "Epoch 63/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1519904.0000\n",
      "Epoch 63: loss improved from 1520303.75000 to 1519359.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1519359.8750\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1511633.5000\n",
      "Epoch 64: loss improved from 1519359.87500 to 1511763.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1511763.7500\n",
      "Epoch 65/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1512280.1250\n",
      "Epoch 65: loss did not improve from 1511763.75000\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1512280.1250\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1508215.5000\n",
      "Epoch 66: loss improved from 1511763.75000 to 1508333.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1508333.3750\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1503338.8750\n",
      "Epoch 67: loss improved from 1508333.37500 to 1504521.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1504521.7500\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1500431.1250\n",
      "Epoch 68: loss improved from 1504521.75000 to 1500431.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1500431.1250\n",
      "Epoch 69/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1499534.3750\n",
      "Epoch 69: loss improved from 1500431.12500 to 1498838.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1498838.0000\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1494802.6250\n",
      "Epoch 70: loss improved from 1498838.00000 to 1494802.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1494802.6250\n",
      "Epoch 71/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1494295.3750\n",
      "Epoch 71: loss improved from 1494802.62500 to 1494431.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1494431.8750\n",
      "Epoch 72/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1492360.0000\n",
      "Epoch 72: loss improved from 1494431.87500 to 1492463.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1492463.6250\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1486360.1250\n",
      "Epoch 73: loss improved from 1492463.62500 to 1486360.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1486360.1250\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1485988.2500\n",
      "Epoch 74: loss improved from 1486360.12500 to 1485903.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1485903.5000\n",
      "Epoch 75/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1483580.2500\n",
      "Epoch 75: loss improved from 1485903.50000 to 1483438.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1483438.0000\n",
      "Epoch 76/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1480795.5000\n",
      "Epoch 76: loss improved from 1483438.00000 to 1480865.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1480865.1250\n",
      "Epoch 77/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1478660.3750\n",
      "Epoch 77: loss improved from 1480865.12500 to 1478618.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1478618.2500\n",
      "Epoch 78/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1475365.5000\n",
      "Epoch 78: loss improved from 1478618.25000 to 1475826.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1475826.1250\n",
      "Epoch 79/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1475658.3750\n",
      "Epoch 79: loss did not improve from 1475826.12500\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1475909.7500\n",
      "Epoch 80/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1472964.0000\n",
      "Epoch 80: loss improved from 1475826.12500 to 1472711.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1472711.2500\n",
      "Epoch 81/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1471626.7500\n",
      "Epoch 81: loss improved from 1472711.25000 to 1471454.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1471454.1250\n",
      "Epoch 82/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1468536.0000\n",
      "Epoch 82: loss improved from 1471454.12500 to 1469370.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1469370.3750\n",
      "Epoch 83/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1463263.3750\n",
      "Epoch 83: loss improved from 1469370.37500 to 1463292.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1463292.7500\n",
      "Epoch 84/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1465985.8750\n",
      "Epoch 84: loss did not improve from 1463292.75000\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1465796.7500\n",
      "Epoch 85/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1461092.3750\n",
      "Epoch 85: loss improved from 1463292.75000 to 1461518.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 6ms/step - loss: 1461518.8750\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1461178.5000\n",
      "Epoch 86: loss improved from 1461518.87500 to 1461309.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 23s 7ms/step - loss: 1461309.0000\n",
      "Epoch 87/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1458641.7500\n",
      "Epoch 87: loss improved from 1461309.00000 to 1458743.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 1458743.7500\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1453095.6250\n",
      "Epoch 88: loss improved from 1458743.75000 to 1453188.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1453188.8750\n",
      "Epoch 89/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1454567.3750\n",
      "Epoch 89: loss did not improve from 1453188.87500\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1455397.5000\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1452518.2500\n",
      "Epoch 90: loss improved from 1453188.87500 to 1452518.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1452518.2500\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1452148.1250\n",
      "Epoch 91: loss improved from 1452518.25000 to 1452107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1452107.7500\n",
      "Epoch 92/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1448278.8750\n",
      "Epoch 92: loss improved from 1452107.75000 to 1448053.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1448053.6250\n",
      "Epoch 93/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1445561.2500\n",
      "Epoch 93: loss improved from 1448053.62500 to 1446286.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1446286.3750\n",
      "Epoch 94/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1447753.8750\n",
      "Epoch 94: loss did not improve from 1446286.37500\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1447582.8750\n",
      "Epoch 95/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1444480.7500\n",
      "Epoch 95: loss improved from 1446286.37500 to 1443919.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1443919.3750\n",
      "Epoch 96/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1440702.7500\n",
      "Epoch 96: loss improved from 1443919.37500 to 1441792.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1441792.7500\n",
      "Epoch 97/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1440638.5000\n",
      "Epoch 97: loss improved from 1441792.75000 to 1439988.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1439988.5000\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1439147.7500\n",
      "Epoch 98: loss improved from 1439988.50000 to 1439147.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 1439147.7500\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1436770.2500\n",
      "Epoch 99: loss improved from 1439147.75000 to 1436770.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1436770.2500\n",
      "Epoch 100/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1435171.0000\n",
      "Epoch 100: loss improved from 1436770.25000 to 1435283.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1435283.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 22:46:08 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 22:46:08 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp4ulbad_c\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp4ulbad_c\\model\\data\\model\\assets\n",
      "2023/05/26 22:46:49 INFO mlflow.tracking.fluent: Experiment with name 'cnn2vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 25, 25, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 25, 25, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 6, 6, 32)          18464     \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSampling  (None, 12, 12, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSampling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,132,005\n",
      "Trainable params: 6,132,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 54s - loss: 1721651.5000    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.0100s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.0100s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736242.7500\n",
      "Epoch 1: loss improved from inf to 2736506.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 36s 10ms/step - loss: 2736506.2500\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736702.5000\n",
      "Epoch 2: loss improved from 2736506.25000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.0000\n",
      "Epoch 3/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736609.5000\n",
      "Epoch 3: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736505.7500\n",
      "Epoch 4/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737202.2500\n",
      "Epoch 4: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736506.7500\n",
      "Epoch 5/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736396.7500\n",
      "Epoch 5: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736505.7500\n",
      "Epoch 6/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736732.5000\n",
      "Epoch 6: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736505.0000\n",
      "Epoch 7/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736698.0000\n",
      "Epoch 7: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736505.0000\n",
      "Epoch 8/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736198.7500\n",
      "Epoch 8: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736505.0000\n",
      "Epoch 9/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 9: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736503.5000\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737081.7500\n",
      "Epoch 10: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736503.7500\n",
      "Epoch 11/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736962.0000\n",
      "Epoch 11: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736505.7500\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736274.0000\n",
      "Epoch 12: loss improved from 2736503.00000 to 2736500.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736500.7500\n",
      "Epoch 13/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737069.2500\n",
      "Epoch 13: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736502.7500\n",
      "Epoch 14/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736880.7500\n",
      "Epoch 14: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736500.7500\n",
      "Epoch 15/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736602.7500\n",
      "Epoch 15: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736502.7500\n",
      "Epoch 16/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736470.0000\n",
      "Epoch 16: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736501.5000\n",
      "Epoch 17/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737035.0000\n",
      "Epoch 17: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736503.5000\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736865.0000\n",
      "Epoch 18: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736504.5000\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736727.2500\n",
      "Epoch 19: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736504.5000\n",
      "Epoch 20/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736247.7500\n",
      "Epoch 20: loss improved from 2736500.75000 to 2736500.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736500.0000\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 21: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736502.5000\n",
      "Epoch 22/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.2500\n",
      "Epoch 22: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736501.2500\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736328.5000\n",
      "Epoch 23: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736502.7500\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737051.2500\n",
      "Epoch 24: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736500.7500\n",
      "Epoch 25/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736283.2500\n",
      "Epoch 25: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736502.2500\n",
      "Epoch 26/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736894.7500\n",
      "Epoch 26: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736501.7500\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.2500\n",
      "Epoch 27: loss improved from 2736500.00000 to 2736499.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736499.2500\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736342.7500\n",
      "Epoch 28: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736503.5000\n",
      "Epoch 29/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736377.2500\n",
      "Epoch 29: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736501.5000\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735573.5000\n",
      "Epoch 30: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736501.5000\n",
      "Epoch 31/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737548.7500\n",
      "Epoch 31: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736501.2500\n",
      "Epoch 32/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736826.5000\n",
      "Epoch 32: loss did not improve from 2736499.25000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736503.0000\n",
      "Epoch 33/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736823.5000\n",
      "Epoch 33: loss improved from 2736499.25000 to 2736497.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736497.7500\n",
      "Epoch 34/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735688.7500\n",
      "Epoch 34: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736502.7500\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736810.5000\n",
      "Epoch 35: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736499.5000\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736778.7500\n",
      "Epoch 36: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736500.5000\n",
      "Epoch 37/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736604.5000\n",
      "Epoch 37: loss did not improve from 2736497.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736499.5000\n",
      "Epoch 38/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736312.7500\n",
      "Epoch 38: loss improved from 2736497.75000 to 2736496.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736496.7500\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.0000\n",
      "Epoch 39: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.0000\n",
      "Epoch 40/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736133.5000\n",
      "Epoch 40: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 41/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737555.5000\n",
      "Epoch 41: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736500.7500\n",
      "Epoch 42/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735564.7500\n",
      "Epoch 42: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736551.5000\n",
      "Epoch 43: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.0000\n",
      "Epoch 44/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736792.0000\n",
      "Epoch 44: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.2500\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736949.7500\n",
      "Epoch 45: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 46/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734988.0000\n",
      "Epoch 46: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736501.5000\n",
      "Epoch 47/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737038.0000\n",
      "Epoch 47: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736501.2500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736813.7500\n",
      "Epoch 48: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736502.2500\n",
      "Epoch 49/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737211.2500\n",
      "Epoch 49: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736498.0000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737210.0000\n",
      "Epoch 50: loss improved from 2736496.75000 to 2736495.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736495.5000\n",
      "Epoch 51/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736447.5000\n",
      "Epoch 51: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736500.5000\n",
      "Epoch 52/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736756.5000\n",
      "Epoch 52: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.0000\n",
      "Epoch 53/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736823.5000\n",
      "Epoch 53: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.0000\n",
      "Epoch 54/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.0000\n",
      "Epoch 54: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.0000\n",
      "Epoch 55/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737017.0000\n",
      "Epoch 55: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736495.7500\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737348.7500\n",
      "Epoch 56: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736498.5000\n",
      "Epoch 57/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735988.7500\n",
      "Epoch 57: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.2500\n",
      "Epoch 58/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737899.7500\n",
      "Epoch 58: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736498.5000\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736306.0000\n",
      "Epoch 59: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736499.2500\n",
      "Epoch 60/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736641.7500\n",
      "Epoch 60: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.2500\n",
      "Epoch 61/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736247.5000\n",
      "Epoch 61: loss did not improve from 2736495.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 62/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736838.0000\n",
      "Epoch 62: loss improved from 2736495.50000 to 2736494.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 63/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736795.0000\n",
      "Epoch 63: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736495.0000\n",
      "Epoch 64/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736244.0000\n",
      "Epoch 64: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736495.0000\n",
      "Epoch 65/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737426.0000\n",
      "Epoch 65: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.7500\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736712.0000\n",
      "Epoch 66: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736495.7500\n",
      "Epoch 67/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736633.0000\n",
      "Epoch 67: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736494.7500\n",
      "Epoch 68/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736766.5000\n",
      "Epoch 68: loss did not improve from 2736494.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.0000\n",
      "Epoch 69/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737594.0000\n",
      "Epoch 69: loss improved from 2736494.75000 to 2736493.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.7500\n",
      "Epoch 70/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736432.7500\n",
      "Epoch 70: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736497.7500\n",
      "Epoch 71/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737244.7500\n",
      "Epoch 71: loss improved from 2736493.75000 to 2736493.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736493.2500\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736494.7500\n",
      "Epoch 72: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736498.0000\n",
      "Epoch 73: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736498.0000\n",
      "Epoch 74/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735998.5000\n",
      "Epoch 74: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.2500\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2738334.2500\n",
      "Epoch 75: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736494.7500\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736495.0000\n",
      "Epoch 76: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 33s 10ms/step - loss: 2736495.0000\n",
      "Epoch 77/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737118.0000\n",
      "Epoch 77: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736495.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736691.0000\n",
      "Epoch 78: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736494.5000\n",
      "Epoch 79/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735918.0000\n",
      "Epoch 79: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736495.7500\n",
      "Epoch 80/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735558.2500\n",
      "Epoch 80: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.5000\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736308.2500\n",
      "Epoch 81: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736495.0000\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735944.7500\n",
      "Epoch 82: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 33s 10ms/step - loss: 2736495.7500\n",
      "Epoch 83/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735189.5000\n",
      "Epoch 83: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736493.5000\n",
      "Epoch 84/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735441.5000\n",
      "Epoch 84: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736495.0000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735816.5000\n",
      "Epoch 85: loss improved from 2736493.25000 to 2736492.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 32s 10ms/step - loss: 2736492.5000\n",
      "Epoch 86/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734492.5000\n",
      "Epoch 86: loss did not improve from 2736492.50000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736493.2500\n",
      "Epoch 87/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736493.7500\n",
      "Epoch 87: loss did not improve from 2736492.50000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736493.7500\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737027.0000\n",
      "Epoch 88: loss did not improve from 2736492.50000\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736492.7500\n",
      "Epoch 89/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735406.2500\n",
      "Epoch 89: loss improved from 2736492.50000 to 2736490.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 2736490.2500\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736491.7500\n",
      "Epoch 90: loss did not improve from 2736490.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.7500\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736490.0000\n",
      "Epoch 91: loss improved from 2736490.25000 to 2736490.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736490.0000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735132.2500\n",
      "Epoch 92: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.7500\n",
      "Epoch 93/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736625.5000\n",
      "Epoch 93: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736490.2500\n",
      "Epoch 94/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737863.0000\n",
      "Epoch 94: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.2500\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735666.5000\n",
      "Epoch 95: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736493.5000\n",
      "Epoch 96: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.5000\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736864.7500\n",
      "Epoch 97: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 98/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736625.0000\n",
      "Epoch 98: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.5000\n",
      "Epoch 99: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736492.5000\n",
      "Epoch 100/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736751.0000\n",
      "Epoch 100: loss improved from 2736490.00000 to 2736489.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736489.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 23:36:40 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/26 23:36:40 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp24pbejzk\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp24pbejzk\\model\\data\\model\\assets\n",
      "2023/05/26 23:37:51 INFO mlflow.tracking.fluent: Experiment with name 'cnn2vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_3 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 25, 25, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 25, 25, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 6, 6, 32)          18464     \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 12, 12, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " up_sampling2d_10 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_11 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,132,005\n",
      "Trainable params: 6,132,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.5000\n",
      "Epoch 1: loss improved from inf to 2736507.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 36s 10ms/step - loss: 2736507.5000\n",
      "Epoch 2/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734500.5000\n",
      "Epoch 2: loss improved from 2736507.50000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.0000\n",
      "Epoch 3/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737917.7500\n",
      "Epoch 3: loss improved from 2736503.00000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.5000\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 4: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736506.2500\n",
      "Epoch 5/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736947.2500\n",
      "Epoch 5: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736505.0000\n",
      "Epoch 6/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736896.7500\n",
      "Epoch 6: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736505.2500\n",
      "Epoch 7/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736698.0000\n",
      "Epoch 7: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736502.5000\n",
      "Epoch 8/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736418.2500\n",
      "Epoch 8: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736504.0000\n",
      "Epoch 9/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736929.2500\n",
      "Epoch 9: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736507.0000\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2738533.5000\n",
      "Epoch 10: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.0000\n",
      "Epoch 11/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734593.0000\n",
      "Epoch 11: loss improved from 2736501.50000 to 2736500.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736500.5000\n",
      "Epoch 12/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736177.5000\n",
      "Epoch 12: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736503.7500\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736074.0000\n",
      "Epoch 13: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736507.2500\n",
      "Epoch 14/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737400.5000\n",
      "Epoch 14: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736502.5000\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736109.2500\n",
      "Epoch 15: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736502.7500\n",
      "Epoch 16/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736399.7500\n",
      "Epoch 16: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736502.7500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735577.5000\n",
      "Epoch 17: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736504.7500\n",
      "Epoch 18/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737266.0000\n",
      "Epoch 18: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 19/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.0000\n",
      "Epoch 19: loss improved from 2736500.50000 to 2736499.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.0000\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736482.5000\n",
      "Epoch 20: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.7500\n",
      "Epoch 21/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737329.5000\n",
      "Epoch 21: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.7500\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736424.0000\n",
      "Epoch 22: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.7500\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736557.2500\n",
      "Epoch 23: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736559.0000\n",
      "Epoch 24: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.7500\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737088.2500\n",
      "Epoch 25: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.7500\n",
      "Epoch 26/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735923.0000\n",
      "Epoch 26: loss did not improve from 2736499.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.5000\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736498.5000\n",
      "Epoch 27: loss improved from 2736499.00000 to 2736498.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736498.5000\n",
      "Epoch 28/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735999.2500\n",
      "Epoch 28: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 29/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736886.7500\n",
      "Epoch 29: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.7500\n",
      "Epoch 30/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735404.0000\n",
      "Epoch 30: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 31/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736197.7500\n",
      "Epoch 31: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.5000\n",
      "Epoch 32/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736880.7500\n",
      "Epoch 32: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.2500\n",
      "Epoch 33/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737814.7500\n",
      "Epoch 33: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.0000\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.2500\n",
      "Epoch 34: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.2500\n",
      "Epoch 35/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736151.7500\n",
      "Epoch 35: loss improved from 2736498.50000 to 2736498.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 36/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735990.5000\n",
      "Epoch 36: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.0000\n",
      "Epoch 37/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736582.0000\n",
      "Epoch 37: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.5000\n",
      "Epoch 38/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735959.7500\n",
      "Epoch 38: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.0000\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 39: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736501.7500\n",
      "Epoch 40/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.7500\n",
      "Epoch 40: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.7500\n",
      "Epoch 41/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737785.5000\n",
      "Epoch 41: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.2500\n",
      "Epoch 42/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735460.2500\n",
      "Epoch 42: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 43/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736989.2500\n",
      "Epoch 43: loss did not improve from 2736498.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736500.0000\n",
      "Epoch 44/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736205.5000\n",
      "Epoch 44: loss improved from 2736498.25000 to 2736496.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.0000\n",
      "Epoch 45/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737452.2500\n",
      "Epoch 45: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 46/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736498.2500\n",
      "Epoch 46: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 47/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736489.5000\n",
      "Epoch 47: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.2500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735955.5000\n",
      "Epoch 48: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.0000\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736496.0000\n",
      "Epoch 49: loss did not improve from 2736496.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.0000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735762.5000\n",
      "Epoch 50: loss improved from 2736496.00000 to 2736493.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.7500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736497.7500\n",
      "Epoch 51: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 52/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737029.0000\n",
      "Epoch 52: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736499.5000\n",
      "Epoch 53/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736440.2500\n",
      "Epoch 53: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.7500\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735695.5000\n",
      "Epoch 54: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736497.2500\n",
      "Epoch 55/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737319.0000\n",
      "Epoch 55: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736499.0000\n",
      "Epoch 56/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734964.0000\n",
      "Epoch 56: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.5000\n",
      "Epoch 57/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736331.7500\n",
      "Epoch 57: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.7500\n",
      "Epoch 58/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737488.2500\n",
      "Epoch 58: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736498.5000\n",
      "Epoch 59/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737136.0000\n",
      "Epoch 59: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.0000\n",
      "Epoch 60/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2738013.7500\n",
      "Epoch 60: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 61/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736825.7500\n",
      "Epoch 61: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736495.7500\n",
      "Epoch 62: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.7500\n",
      "Epoch 63/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737305.7500\n",
      "Epoch 63: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736564.7500\n",
      "Epoch 64: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.5000\n",
      "Epoch 65/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736489.0000\n",
      "Epoch 65: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 66/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736664.2500\n",
      "Epoch 66: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.5000\n",
      "Epoch 67/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736926.7500\n",
      "Epoch 67: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.7500\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736369.5000\n",
      "Epoch 68: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.7500\n",
      "Epoch 69/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735998.5000\n",
      "Epoch 69: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.5000\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 70: loss improved from 2736493.75000 to 2736492.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.2500\n",
      "Epoch 71/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736801.5000\n",
      "Epoch 71: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736495.7500\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736108.0000\n",
      "Epoch 72: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736496.2500\n",
      "Epoch 73/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736606.2500\n",
      "Epoch 73: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 74/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736295.5000\n",
      "Epoch 74: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.5000\n",
      "Epoch 75/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735796.7500\n",
      "Epoch 75: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.2500\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736497.7500\n",
      "Epoch 76: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736497.7500\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736494.5000\n",
      "Epoch 77: loss did not improve from 2736492.25000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737232.5000\n",
      "Epoch 78: loss improved from 2736492.25000 to 2736491.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 79/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2738183.7500\n",
      "Epoch 79: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.0000\n",
      "Epoch 80/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736145.7500\n",
      "Epoch 80: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.7500\n",
      "Epoch 81/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736805.5000\n",
      "Epoch 81: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.7500\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736861.5000\n",
      "Epoch 82: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.2500\n",
      "Epoch 83/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735732.5000\n",
      "Epoch 83: loss did not improve from 2736491.50000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.5000\n",
      "Epoch 84/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737746.0000\n",
      "Epoch 84: loss improved from 2736491.50000 to 2736489.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736489.0000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736014.7500\n",
      "Epoch 85: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 86: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.2500\n",
      "Epoch 87/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736179.7500\n",
      "Epoch 87: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736492.5000\n",
      "Epoch 88/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736868.2500\n",
      "Epoch 88: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736494.5000\n",
      "Epoch 89/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735482.2500\n",
      "Epoch 89: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.5000\n",
      "Epoch 90/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736086.5000\n",
      "Epoch 90: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736489.0000\n",
      "Epoch 91/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737188.5000\n",
      "Epoch 91: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736491.2500\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736764.5000\n",
      "Epoch 92: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 30s 10ms/step - loss: 2736492.2500\n",
      "Epoch 93/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735491.0000\n",
      "Epoch 93: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736494.5000\n",
      "Epoch 94/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736544.0000\n",
      "Epoch 94: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736493.5000\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736928.5000\n",
      "Epoch 95: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736489.5000\n",
      "Epoch 96/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736426.7500\n",
      "Epoch 96: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736493.7500\n",
      "Epoch 97/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735040.2500\n",
      "Epoch 97: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.7500\n",
      "Epoch 98/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735479.7500\n",
      "Epoch 98: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736490.0000\n",
      "Epoch 99/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736416.7500\n",
      "Epoch 99: loss did not improve from 2736489.00000\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736491.7500\n",
      "Epoch 100/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737183.0000\n",
      "Epoch 100: loss improved from 2736489.00000 to 2736488.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736488.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 00:26:56 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 00:26:56 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpy4h3nw1e\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpy4h3nw1e\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_34 (Dense)            (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 46s - loss: 3325607.0000    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.0245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.0245s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186/3189 [============================>.] - ETA: 0s - loss: 2603394.0000\n",
      "Epoch 1: loss improved from inf to 2602878.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 4ms/step - loss: 2602878.5000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2449903.2500\n",
      "Epoch 2: loss improved from 2602878.50000 to 2449336.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2449336.0000\n",
      "Epoch 3/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2415777.5000\n",
      "Epoch 3: loss improved from 2449336.00000 to 2414382.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2414382.2500\n",
      "Epoch 4/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2397420.0000\n",
      "Epoch 4: loss improved from 2414382.25000 to 2396892.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2396892.2500\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2384340.0000\n",
      "Epoch 5: loss improved from 2396892.25000 to 2383863.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2383863.7500\n",
      "Epoch 6/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2372725.0000\n",
      "Epoch 6: loss improved from 2383863.75000 to 2372451.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2372451.5000\n",
      "Epoch 7/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2366146.7500\n",
      "Epoch 7: loss improved from 2372451.50000 to 2366517.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2366517.5000\n",
      "Epoch 8/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2359472.7500\n",
      "Epoch 8: loss improved from 2366517.50000 to 2359321.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2359321.0000\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2355448.2500\n",
      "Epoch 9: loss improved from 2359321.00000 to 2354577.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2354577.2500\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2348332.5000\n",
      "Epoch 10: loss improved from 2354577.25000 to 2348100.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2348100.5000\n",
      "Epoch 11/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2344911.5000\n",
      "Epoch 11: loss improved from 2348100.50000 to 2344734.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2344734.7500\n",
      "Epoch 12/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2340517.2500\n",
      "Epoch 12: loss improved from 2344734.75000 to 2341755.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2341755.5000\n",
      "Epoch 13/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2338860.0000\n",
      "Epoch 13: loss improved from 2341755.50000 to 2338762.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2338762.5000\n",
      "Epoch 14/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2336938.0000\n",
      "Epoch 14: loss improved from 2338762.50000 to 2337269.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2337269.2500\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2330934.2500\n",
      "Epoch 15: loss improved from 2337269.25000 to 2334600.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2334600.2500\n",
      "Epoch 16/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2330460.5000\n",
      "Epoch 16: loss improved from 2334600.25000 to 2330903.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2330903.5000\n",
      "Epoch 17/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2328631.7500\n",
      "Epoch 17: loss improved from 2330903.50000 to 2328846.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2328846.0000\n",
      "Epoch 18/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2327124.2500\n",
      "Epoch 18: loss improved from 2328846.00000 to 2327658.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2327658.5000\n",
      "Epoch 19/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2326148.7500\n",
      "Epoch 19: loss improved from 2327658.50000 to 2326051.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2326051.0000\n",
      "Epoch 20/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2325596.0000\n",
      "Epoch 20: loss improved from 2326051.00000 to 2324977.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2324977.7500\n",
      "Epoch 21/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2325600.7500\n",
      "Epoch 21: loss improved from 2324977.75000 to 2323576.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2323576.2500\n",
      "Epoch 22/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2323626.5000\n",
      "Epoch 22: loss improved from 2323576.25000 to 2322742.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2322742.7500\n",
      "Epoch 23/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2321824.5000\n",
      "Epoch 23: loss improved from 2322742.75000 to 2322407.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2322407.5000\n",
      "Epoch 24/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2323352.7500\n",
      "Epoch 24: loss improved from 2322407.50000 to 2321416.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2321416.7500\n",
      "Epoch 25/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2319918.7500\n",
      "Epoch 25: loss improved from 2321416.75000 to 2321005.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2321005.2500\n",
      "Epoch 26/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2319572.5000\n",
      "Epoch 26: loss improved from 2321005.25000 to 2319408.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2319408.2500\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2319682.5000\n",
      "Epoch 27: loss improved from 2319408.25000 to 2318717.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2318717.5000\n",
      "Epoch 28/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2319526.7500\n",
      "Epoch 28: loss did not improve from 2318717.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2319816.2500\n",
      "Epoch 29/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2321163.7500\n",
      "Epoch 29: loss improved from 2318717.50000 to 2317909.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2317909.0000\n",
      "Epoch 30/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2318341.7500\n",
      "Epoch 30: loss did not improve from 2317909.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2318017.2500\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2316938.0000\n",
      "Epoch 31: loss improved from 2317909.00000 to 2316938.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2316938.0000\n",
      "Epoch 32/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2314854.5000\n",
      "Epoch 32: loss improved from 2316938.00000 to 2315076.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2315076.0000\n",
      "Epoch 33/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2315442.2500\n",
      "Epoch 33: loss did not improve from 2315076.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2316176.7500\n",
      "Epoch 34/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2315213.5000\n",
      "Epoch 34: loss improved from 2315076.00000 to 2314931.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2314931.5000\n",
      "Epoch 35/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2314752.5000\n",
      "Epoch 35: loss improved from 2314931.50000 to 2313857.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2313857.0000\n",
      "Epoch 36/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2313802.7500\n",
      "Epoch 36: loss did not improve from 2313857.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2314077.0000\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2314481.2500\n",
      "Epoch 37: loss did not improve from 2313857.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2314481.2500\n",
      "Epoch 38/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2313612.7500\n",
      "Epoch 38: loss improved from 2313857.00000 to 2313809.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2313809.7500\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2313766.7500\n",
      "Epoch 39: loss improved from 2313809.75000 to 2313766.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2313766.7500\n",
      "Epoch 40/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2312805.5000\n",
      "Epoch 40: loss improved from 2313766.75000 to 2312971.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2312971.7500\n",
      "Epoch 41/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2313389.5000\n",
      "Epoch 41: loss improved from 2312971.75000 to 2312818.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2312818.7500\n",
      "Epoch 42/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2312247.2500\n",
      "Epoch 42: loss improved from 2312818.75000 to 2312280.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2312280.2500\n",
      "Epoch 43/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2312641.7500\n",
      "Epoch 43: loss improved from 2312280.25000 to 2311907.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311907.7500\n",
      "Epoch 44/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2314092.0000\n",
      "Epoch 44: loss did not improve from 2311907.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311973.5000\n",
      "Epoch 45/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2314425.5000\n",
      "Epoch 45: loss improved from 2311907.75000 to 2311809.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311809.2500\n",
      "Epoch 46/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2311303.5000\n",
      "Epoch 46: loss improved from 2311809.25000 to 2310980.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310980.7500\n",
      "Epoch 47/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2310282.7500\n",
      "Epoch 47: loss did not improve from 2310980.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311392.5000\n",
      "Epoch 48/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2310350.2500\n",
      "Epoch 48: loss improved from 2310980.75000 to 2310332.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310332.7500\n",
      "Epoch 49/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2311723.5000\n",
      "Epoch 49: loss did not improve from 2310332.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2311152.5000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2311366.5000\n",
      "Epoch 50: loss did not improve from 2310332.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310560.2500\n",
      "Epoch 51/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2309386.2500\n",
      "Epoch 51: loss improved from 2310332.75000 to 2309237.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2309237.0000\n",
      "Epoch 52/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2309923.0000\n",
      "Epoch 52: loss did not improve from 2309237.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310105.0000\n",
      "Epoch 53/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2308935.7500\n",
      "Epoch 53: loss did not improve from 2309237.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2310159.5000\n",
      "Epoch 54/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2310392.0000\n",
      "Epoch 54: loss improved from 2309237.00000 to 2308610.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308610.5000\n",
      "Epoch 55/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2308658.5000\n",
      "Epoch 55: loss did not improve from 2308610.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308895.0000\n",
      "Epoch 56/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2309633.5000\n",
      "Epoch 56: loss did not improve from 2308610.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2309408.7500\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2307786.0000\n",
      "Epoch 57: loss improved from 2308610.50000 to 2308113.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308113.5000\n",
      "Epoch 58/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2308458.5000\n",
      "Epoch 58: loss did not improve from 2308113.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308337.7500\n",
      "Epoch 59/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2309703.7500\n",
      "Epoch 59: loss did not improve from 2308113.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2309272.0000\n",
      "Epoch 60/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2307783.7500\n",
      "Epoch 60: loss improved from 2308113.50000 to 2307715.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307715.2500\n",
      "Epoch 61/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2307694.7500\n",
      "Epoch 61: loss did not improve from 2307715.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307738.0000\n",
      "Epoch 62/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2307235.5000\n",
      "Epoch 62: loss did not improve from 2307715.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308086.7500\n",
      "Epoch 63/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2307043.2500\n",
      "Epoch 63: loss did not improve from 2307715.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307971.2500\n",
      "Epoch 64/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2308277.7500\n",
      "Epoch 64: loss did not improve from 2307715.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2308109.0000\n",
      "Epoch 65/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2309384.2500\n",
      "Epoch 65: loss improved from 2307715.25000 to 2306701.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2306701.0000\n",
      "Epoch 66/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2306494.2500\n",
      "Epoch 66: loss improved from 2306701.00000 to 2306270.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306270.7500\n",
      "Epoch 67/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2306422.5000\n",
      "Epoch 67: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307030.2500\n",
      "Epoch 68/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2308855.7500\n",
      "Epoch 68: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307137.7500\n",
      "Epoch 69/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2306546.0000\n",
      "Epoch 69: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306805.2500\n",
      "Epoch 70/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2306894.0000\n",
      "Epoch 70: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306841.7500\n",
      "Epoch 71/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2306728.0000\n",
      "Epoch 71: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306559.2500\n",
      "Epoch 72/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2308921.7500\n",
      "Epoch 72: loss did not improve from 2306270.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2307111.0000\n",
      "Epoch 73/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2307061.5000\n",
      "Epoch 73: loss improved from 2306270.75000 to 2306047.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306047.0000\n",
      "Epoch 74/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2305553.5000\n",
      "Epoch 74: loss improved from 2306047.00000 to 2305165.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305165.0000\n",
      "Epoch 75/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2305886.7500\n",
      "Epoch 75: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306170.5000\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2305950.7500\n",
      "Epoch 76: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305950.7500\n",
      "Epoch 77/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2306042.5000\n",
      "Epoch 77: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305922.0000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2306174.2500\n",
      "Epoch 78: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2306123.2500\n",
      "Epoch 79/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2306819.0000\n",
      "Epoch 79: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305684.5000\n",
      "Epoch 80/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2305553.0000\n",
      "Epoch 80: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305364.5000\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2305801.0000\n",
      "Epoch 81: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305801.0000\n",
      "Epoch 82/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2307041.7500\n",
      "Epoch 82: loss did not improve from 2305165.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305286.5000\n",
      "Epoch 83/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2306241.7500\n",
      "Epoch 83: loss improved from 2305165.00000 to 2305101.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305101.0000\n",
      "Epoch 84/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2306182.5000\n",
      "Epoch 84: loss improved from 2305101.00000 to 2304841.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304841.5000\n",
      "Epoch 85/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2306232.2500\n",
      "Epoch 85: loss improved from 2304841.50000 to 2304197.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304197.0000\n",
      "Epoch 86/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2304700.5000\n",
      "Epoch 86: loss did not improve from 2304197.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305171.0000\n",
      "Epoch 87/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2305118.5000\n",
      "Epoch 87: loss did not improve from 2304197.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304414.2500\n",
      "Epoch 88/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2307119.7500\n",
      "Epoch 88: loss did not improve from 2304197.00000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2305012.5000\n",
      "Epoch 89/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2304374.7500\n",
      "Epoch 89: loss improved from 2304197.00000 to 2304137.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304137.7500\n",
      "Epoch 90/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2305236.7500\n",
      "Epoch 90: loss did not improve from 2304137.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2305301.0000\n",
      "Epoch 91/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2304434.7500\n",
      "Epoch 91: loss did not improve from 2304137.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304370.7500\n",
      "Epoch 92/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2306352.5000\n",
      "Epoch 92: loss did not improve from 2304137.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304286.7500\n",
      "Epoch 93/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2303907.7500\n",
      "Epoch 93: loss improved from 2304137.75000 to 2304015.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304015.2500\n",
      "Epoch 94/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2304609.0000\n",
      "Epoch 94: loss did not improve from 2304015.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304618.5000\n",
      "Epoch 95/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2303851.0000\n",
      "Epoch 95: loss improved from 2304015.25000 to 2303959.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303959.5000\n",
      "Epoch 96/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2305327.7500\n",
      "Epoch 96: loss did not improve from 2303959.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304547.7500\n",
      "Epoch 97/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2305459.5000\n",
      "Epoch 97: loss did not improve from 2303959.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2304476.7500\n",
      "Epoch 98/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2302896.7500\n",
      "Epoch 98: loss improved from 2303959.50000 to 2303230.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303230.7500\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2303682.5000\n",
      "Epoch 99: loss did not improve from 2303230.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303352.5000\n",
      "Epoch 100/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2303687.7500\n",
      "Epoch 100: loss did not improve from 2303230.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2303524.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 00:40:25 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 00:40:25 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmppmigv8ce\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmppmigv8ce\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_37 (Dense)            (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 54s - loss: 6680369.5000     WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0143s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0143s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735385.7500\n",
      "Epoch 1: loss improved from inf to 2735723.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 36s 3ms/step - loss: 2735723.2500\n",
      "Epoch 2/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2587657.0000\n",
      "Epoch 2: loss improved from 2735723.25000 to 2587661.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2587661.0000\n",
      "Epoch 3/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2566821.5000\n",
      "Epoch 3: loss improved from 2587661.00000 to 2569381.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2569381.7500\n",
      "Epoch 4/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2563698.2500\n",
      "Epoch 4: loss improved from 2569381.75000 to 2561474.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2561474.7500\n",
      "Epoch 5/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2556242.5000\n",
      "Epoch 5: loss improved from 2561474.75000 to 2555460.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2555460.2500\n",
      "Epoch 6/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2552261.5000\n",
      "Epoch 6: loss improved from 2555460.25000 to 2551152.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2551152.2500\n",
      "Epoch 7/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2547143.0000\n",
      "Epoch 7: loss improved from 2551152.25000 to 2547732.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2547732.7500\n",
      "Epoch 8/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2543054.5000\n",
      "Epoch 8: loss improved from 2547732.75000 to 2543858.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2543858.7500\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2541223.0000\n",
      "Epoch 9: loss improved from 2543858.75000 to 2541617.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2541617.7500\n",
      "Epoch 10/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2540386.5000\n",
      "Epoch 10: loss improved from 2541617.75000 to 2538967.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2538967.7500\n",
      "Epoch 11/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2537429.5000\n",
      "Epoch 11: loss improved from 2538967.75000 to 2538045.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2538045.0000\n",
      "Epoch 12/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2534987.5000\n",
      "Epoch 12: loss improved from 2538045.00000 to 2535433.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2535433.0000\n",
      "Epoch 13/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2536372.5000\n",
      "Epoch 13: loss improved from 2535433.00000 to 2534300.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2534300.2500\n",
      "Epoch 14/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2533505.0000\n",
      "Epoch 14: loss improved from 2534300.25000 to 2534024.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2534024.5000\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2532703.0000\n",
      "Epoch 15: loss improved from 2534024.50000 to 2533059.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2533059.7500\n",
      "Epoch 16/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2533063.7500\n",
      "Epoch 16: loss improved from 2533059.75000 to 2531906.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2531906.0000\n",
      "Epoch 17/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2531353.5000\n",
      "Epoch 17: loss improved from 2531906.00000 to 2531289.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2531289.7500\n",
      "Epoch 18/100\n",
      "3168/3189 [============================>.] - ETA: 0s - loss: 2528766.0000\n",
      "Epoch 18: loss improved from 2531289.75000 to 2530286.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2530286.0000\n",
      "Epoch 19/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2528734.5000\n",
      "Epoch 19: loss improved from 2530286.00000 to 2529676.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2529676.7500\n",
      "Epoch 20/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2527638.7500\n",
      "Epoch 20: loss improved from 2529676.75000 to 2528466.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2528466.5000\n",
      "Epoch 21/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2526343.7500\n",
      "Epoch 21: loss improved from 2528466.50000 to 2527711.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2527711.7500\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2526244.2500\n",
      "Epoch 22: loss improved from 2527711.75000 to 2527001.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2527001.2500\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2528197.0000\n",
      "Epoch 23: loss improved from 2527001.25000 to 2526640.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2526640.5000\n",
      "Epoch 24/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2527051.5000\n",
      "Epoch 24: loss did not improve from 2526640.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2526748.7500\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2526528.5000\n",
      "Epoch 25: loss improved from 2526640.50000 to 2525909.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2525909.7500\n",
      "Epoch 26/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2524301.2500\n",
      "Epoch 26: loss improved from 2525909.75000 to 2524961.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2524961.5000\n",
      "Epoch 27/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2524087.0000\n",
      "Epoch 27: loss improved from 2524961.50000 to 2524875.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2524875.2500\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2525861.5000\n",
      "Epoch 28: loss improved from 2524875.25000 to 2524846.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2524846.0000\n",
      "Epoch 29/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2526383.2500\n",
      "Epoch 29: loss improved from 2524846.00000 to 2524475.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2524475.5000\n",
      "Epoch 30/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2524397.7500\n",
      "Epoch 30: loss did not improve from 2524475.50000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2525367.2500\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2524490.5000\n",
      "Epoch 31: loss improved from 2524475.50000 to 2524066.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2524066.7500\n",
      "Epoch 32/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2523795.7500\n",
      "Epoch 32: loss improved from 2524066.75000 to 2522944.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2522944.0000\n",
      "Epoch 33/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2524240.0000\n",
      "Epoch 33: loss did not improve from 2522944.00000\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2523343.2500\n",
      "Epoch 34/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2521337.7500\n",
      "Epoch 34: loss improved from 2522944.00000 to 2521612.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2521612.2500\n",
      "Epoch 35/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2519847.5000\n",
      "Epoch 35: loss did not improve from 2521612.25000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2521979.0000\n",
      "Epoch 36/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2519312.0000\n",
      "Epoch 36: loss did not improve from 2521612.25000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2521845.2500\n",
      "Epoch 37/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2521944.0000\n",
      "Epoch 37: loss improved from 2521612.25000 to 2521107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 3ms/step - loss: 2521107.7500\n",
      "Epoch 38/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2522182.5000\n",
      "Epoch 38: loss improved from 2521107.75000 to 2520690.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2520690.7500\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2519891.7500\n",
      "Epoch 39: loss improved from 2520690.75000 to 2519891.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2519891.7500\n",
      "Epoch 40/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2519358.2500\n",
      "Epoch 40: loss did not improve from 2519891.75000\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2520629.2500\n",
      "Epoch 41/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2518586.7500\n",
      "Epoch 41: loss improved from 2519891.75000 to 2519825.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 8s 2ms/step - loss: 2519825.5000\n",
      "Epoch 42/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2517253.7500\n",
      "Epoch 42: loss did not improve from 2519825.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2520005.5000\n",
      "Epoch 43/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2520015.2500\n",
      "Epoch 43: loss improved from 2519825.50000 to 2519502.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519502.0000\n",
      "Epoch 44/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2519132.2500\n",
      "Epoch 44: loss improved from 2519502.00000 to 2519288.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519288.0000\n",
      "Epoch 45/100\n",
      "3171/3189 [============================>.] - ETA: 0s - loss: 2521453.0000\n",
      "Epoch 45: loss did not improve from 2519288.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519573.2500\n",
      "Epoch 46/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2521434.2500\n",
      "Epoch 46: loss did not improve from 2519288.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519383.0000\n",
      "Epoch 47/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2518722.7500\n",
      "Epoch 47: loss improved from 2519288.00000 to 2518731.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518731.7500\n",
      "Epoch 48/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2519464.2500\n",
      "Epoch 48: loss did not improve from 2518731.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2519403.2500\n",
      "Epoch 49/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2519406.5000\n",
      "Epoch 49: loss did not improve from 2518731.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518806.2500\n",
      "Epoch 50/100\n",
      "3166/3189 [============================>.] - ETA: 0s - loss: 2517872.5000\n",
      "Epoch 50: loss improved from 2518731.75000 to 2518115.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518115.0000\n",
      "Epoch 51/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2520017.0000\n",
      "Epoch 51: loss did not improve from 2518115.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518513.7500\n",
      "Epoch 52/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2514780.2500\n",
      "Epoch 52: loss improved from 2518115.00000 to 2517564.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517564.5000\n",
      "Epoch 53/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2518680.7500\n",
      "Epoch 53: loss did not improve from 2517564.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517687.7500\n",
      "Epoch 54/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2516744.5000\n",
      "Epoch 54: loss improved from 2517564.50000 to 2517223.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517223.7500\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2518270.5000\n",
      "Epoch 55: loss did not improve from 2517223.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518084.0000\n",
      "Epoch 56/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2517346.2500\n",
      "Epoch 56: loss did not improve from 2517223.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517293.7500\n",
      "Epoch 57/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2516787.7500\n",
      "Epoch 57: loss did not improve from 2517223.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2517515.0000\n",
      "Epoch 58/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2517829.5000\n",
      "Epoch 58: loss improved from 2517223.75000 to 2516087.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516087.0000\n",
      "Epoch 59/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2518053.5000\n",
      "Epoch 59: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518065.0000\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2518004.5000\n",
      "Epoch 60: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2518016.0000\n",
      "Epoch 61/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2516238.0000\n",
      "Epoch 61: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516397.2500\n",
      "Epoch 62/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2517395.5000\n",
      "Epoch 62: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516943.2500\n",
      "Epoch 63/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2515999.0000\n",
      "Epoch 63: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516352.2500\n",
      "Epoch 64/100\n",
      "3167/3189 [============================>.] - ETA: 0s - loss: 2514467.7500\n",
      "Epoch 64: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516799.0000\n",
      "Epoch 65/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2516967.7500\n",
      "Epoch 65: loss did not improve from 2516087.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516479.7500\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2515625.7500\n",
      "Epoch 66: loss improved from 2516087.00000 to 2515625.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515625.7500\n",
      "Epoch 67/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2515340.5000\n",
      "Epoch 67: loss did not improve from 2515625.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2516490.5000\n",
      "Epoch 68/100\n",
      "3170/3189 [============================>.] - ETA: 0s - loss: 2513931.5000\n",
      "Epoch 68: loss improved from 2515625.75000 to 2515195.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515195.7500\n",
      "Epoch 69/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2515826.2500\n",
      "Epoch 69: loss did not improve from 2515195.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515734.2500\n",
      "Epoch 70/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2516331.0000\n",
      "Epoch 70: loss improved from 2515195.75000 to 2515181.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515181.0000\n",
      "Epoch 71/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2514769.2500\n",
      "Epoch 71: loss did not improve from 2515181.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515535.7500\n",
      "Epoch 72/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2514982.2500\n",
      "Epoch 72: loss did not improve from 2515181.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515655.2500\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2515221.5000\n",
      "Epoch 73: loss did not improve from 2515181.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515443.7500\n",
      "Epoch 74/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2514254.5000\n",
      "Epoch 74: loss did not improve from 2515181.00000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2515265.7500\n",
      "Epoch 75/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2516589.0000\n",
      "Epoch 75: loss improved from 2515181.00000 to 2514402.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514402.0000\n",
      "Epoch 76/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2513400.0000\n",
      "Epoch 76: loss improved from 2514402.00000 to 2513880.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513880.5000\n",
      "Epoch 77/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2511326.2500\n",
      "Epoch 77: loss did not improve from 2513880.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514030.5000\n",
      "Epoch 78/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2512888.7500\n",
      "Epoch 78: loss did not improve from 2513880.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514033.7500\n",
      "Epoch 79/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2511914.7500\n",
      "Epoch 79: loss improved from 2513880.50000 to 2512349.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512349.5000\n",
      "Epoch 80/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2514276.5000\n",
      "Epoch 80: loss did not improve from 2512349.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2513990.2500\n",
      "Epoch 81/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2515239.7500\n",
      "Epoch 81: loss did not improve from 2512349.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2514389.5000\n",
      "Epoch 82/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2511715.7500\n",
      "Epoch 82: loss improved from 2512349.50000 to 2511927.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511927.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2512345.2500\n",
      "Epoch 83: loss improved from 2511927.75000 to 2511804.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511804.2500\n",
      "Epoch 84/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2512275.5000\n",
      "Epoch 84: loss did not improve from 2511804.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512891.2500\n",
      "Epoch 85/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2512850.2500\n",
      "Epoch 85: loss did not improve from 2511804.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512423.0000\n",
      "Epoch 86/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2512160.7500\n",
      "Epoch 86: loss did not improve from 2511804.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2512328.2500\n",
      "Epoch 87/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2511765.5000\n",
      "Epoch 87: loss did not improve from 2511804.25000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511947.2500\n",
      "Epoch 88/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2510221.2500\n",
      "Epoch 88: loss improved from 2511804.25000 to 2510706.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510706.5000\n",
      "Epoch 89/100\n",
      "3169/3189 [============================>.] - ETA: 0s - loss: 2512463.2500\n",
      "Epoch 89: loss did not improve from 2510706.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511259.2500\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2511512.0000\n",
      "Epoch 90: loss did not improve from 2510706.50000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511398.5000\n",
      "Epoch 91/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2510710.7500\n",
      "Epoch 91: loss improved from 2510706.50000 to 2510591.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510591.0000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2509684.0000\n",
      "Epoch 92: loss improved from 2510591.00000 to 2509567.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2509567.2500\n",
      "Epoch 93/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2509666.7500\n",
      "Epoch 93: loss improved from 2509567.25000 to 2508868.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508868.7500\n",
      "Epoch 94/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2510466.7500\n",
      "Epoch 94: loss did not improve from 2508868.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2511104.7500\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2510169.0000\n",
      "Epoch 95: loss did not improve from 2508868.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510091.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2510075.0000\n",
      "Epoch 96: loss did not improve from 2508868.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2510075.0000\n",
      "Epoch 97/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2509060.7500\n",
      "Epoch 97: loss improved from 2508868.75000 to 2508271.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508271.7500\n",
      "Epoch 98/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2508827.7500\n",
      "Epoch 98: loss did not improve from 2508271.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508894.2500\n",
      "Epoch 99/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2509146.7500\n",
      "Epoch 99: loss did not improve from 2508271.75000\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508903.2500\n",
      "Epoch 100/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2508137.0000\n",
      "Epoch 100: loss improved from 2508271.75000 to 2508159.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 7s 2ms/step - loss: 2508159.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 00:53:52 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 00:53:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpk384aee3\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpk384aee3\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 3104133.0000\n",
      "Epoch 1: loss improved from inf to 3104133.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 3104133.0000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2430314.7500\n",
      "Epoch 2: loss improved from 3104133.00000 to 2430734.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2430734.0000\n",
      "Epoch 3/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2365670.2500\n",
      "Epoch 3: loss improved from 2430734.00000 to 2365595.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2365595.2500\n",
      "Epoch 4/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2331921.0000\n",
      "Epoch 4: loss improved from 2365595.25000 to 2333567.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2333567.0000\n",
      "Epoch 5/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2337124.5000\n",
      "Epoch 5: loss did not improve from 2333567.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2337220.7500\n",
      "Epoch 6/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2303511.5000\n",
      "Epoch 6: loss improved from 2333567.00000 to 2302385.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2302385.7500\n",
      "Epoch 7/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2282293.2500\n",
      "Epoch 7: loss improved from 2302385.75000 to 2281136.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2281136.5000\n",
      "Epoch 8/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2271398.5000\n",
      "Epoch 8: loss improved from 2281136.50000 to 2269490.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2269490.0000\n",
      "Epoch 9/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2273038.5000\n",
      "Epoch 9: loss did not improve from 2269490.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2271121.2500\n",
      "Epoch 10/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2268917.7500\n",
      "Epoch 10: loss improved from 2269490.00000 to 2267572.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2267572.7500\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2264538.0000\n",
      "Epoch 11: loss improved from 2267572.75000 to 2264538.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2264538.0000\n",
      "Epoch 12/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2257869.2500\n",
      "Epoch 12: loss improved from 2264538.00000 to 2255202.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2255202.2500\n",
      "Epoch 13/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2244927.2500\n",
      "Epoch 13: loss improved from 2255202.25000 to 2244900.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2244900.2500\n",
      "Epoch 14/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2246913.7500\n",
      "Epoch 14: loss did not improve from 2244900.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2247235.5000\n",
      "Epoch 15/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2219743.5000\n",
      "Epoch 15: loss improved from 2244900.25000 to 2219607.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2219607.7500\n",
      "Epoch 16/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2240469.5000\n",
      "Epoch 16: loss did not improve from 2219607.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2239204.7500\n",
      "Epoch 17/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2232914.2500\n",
      "Epoch 17: loss did not improve from 2219607.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2232755.7500\n",
      "Epoch 18/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2225842.0000\n",
      "Epoch 18: loss did not improve from 2219607.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2225542.2500\n",
      "Epoch 19/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2218313.2500\n",
      "Epoch 19: loss did not improve from 2219607.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2219798.7500\n",
      "Epoch 20/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2212195.7500\n",
      "Epoch 20: loss improved from 2219607.75000 to 2211116.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2211116.5000\n",
      "Epoch 21/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2216749.5000\n",
      "Epoch 21: loss did not improve from 2211116.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2216356.0000\n",
      "Epoch 22/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2214088.7500\n",
      "Epoch 22: loss did not improve from 2211116.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2213971.2500\n",
      "Epoch 23/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2204323.7500\n",
      "Epoch 23: loss improved from 2211116.50000 to 2205018.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2205018.2500\n",
      "Epoch 24/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2215881.2500\n",
      "Epoch 24: loss did not improve from 2205018.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2216004.5000\n",
      "Epoch 25/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2208355.0000\n",
      "Epoch 25: loss did not improve from 2205018.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2208355.0000\n",
      "Epoch 26/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2209458.2500\n",
      "Epoch 26: loss did not improve from 2205018.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2207687.7500\n",
      "Epoch 27/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2211051.5000\n",
      "Epoch 27: loss did not improve from 2205018.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2212167.5000\n",
      "Epoch 28/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2199003.2500\n",
      "Epoch 28: loss improved from 2205018.25000 to 2199016.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2199016.2500\n",
      "Epoch 29/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2205444.0000\n",
      "Epoch 29: loss did not improve from 2199016.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2205128.5000\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2200649.7500\n",
      "Epoch 30: loss did not improve from 2199016.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2200657.0000\n",
      "Epoch 31/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2190145.5000\n",
      "Epoch 31: loss improved from 2199016.25000 to 2189601.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2189601.0000\n",
      "Epoch 32/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2189377.2500\n",
      "Epoch 32: loss did not improve from 2189601.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2190567.2500\n",
      "Epoch 33/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2194688.7500\n",
      "Epoch 33: loss did not improve from 2189601.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2193768.7500\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2203538.2500\n",
      "Epoch 34: loss did not improve from 2189601.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2204182.5000\n",
      "Epoch 35/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2188830.0000\n",
      "Epoch 35: loss improved from 2189601.00000 to 2188679.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2188679.7500\n",
      "Epoch 36/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2204031.7500\n",
      "Epoch 36: loss did not improve from 2188679.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2203059.2500\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2183249.0000\n",
      "Epoch 37: loss improved from 2188679.75000 to 2183249.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2183249.0000\n",
      "Epoch 38/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2197478.5000\n",
      "Epoch 38: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2197025.5000\n",
      "Epoch 39/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2193149.7500\n",
      "Epoch 39: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192320.7500\n",
      "Epoch 40/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2190489.0000\n",
      "Epoch 40: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2191147.5000\n",
      "Epoch 41/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2197291.7500\n",
      "Epoch 41: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2195737.0000\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2195268.0000\n",
      "Epoch 42: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2193993.7500\n",
      "Epoch 43/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2191021.5000\n",
      "Epoch 43: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2190754.0000\n",
      "Epoch 44/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2189187.0000\n",
      "Epoch 44: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2189864.7500\n",
      "Epoch 45/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2187011.2500\n",
      "Epoch 45: loss did not improve from 2183249.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185281.5000\n",
      "Epoch 46/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2177947.7500\n",
      "Epoch 46: loss improved from 2183249.00000 to 2179675.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2179675.5000\n",
      "Epoch 47/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2196681.7500\n",
      "Epoch 47: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2195891.0000\n",
      "Epoch 48/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2191602.5000\n",
      "Epoch 48: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2190302.2500\n",
      "Epoch 49/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2185199.7500\n",
      "Epoch 49: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186085.2500\n",
      "Epoch 50/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2186965.7500\n",
      "Epoch 50: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186961.7500\n",
      "Epoch 51/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2184673.7500\n",
      "Epoch 51: loss did not improve from 2179675.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186850.7500\n",
      "Epoch 52/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2175852.0000\n",
      "Epoch 52: loss improved from 2179675.50000 to 2175545.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175545.0000\n",
      "Epoch 53/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2167082.2500\n",
      "Epoch 53: loss improved from 2175545.00000 to 2167510.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2167510.0000\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2185689.0000\n",
      "Epoch 54: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185578.7500\n",
      "Epoch 55/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2192459.5000\n",
      "Epoch 55: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2191676.7500\n",
      "Epoch 56/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2178356.7500\n",
      "Epoch 56: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2177989.0000\n",
      "Epoch 57/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2177938.5000\n",
      "Epoch 57: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177048.0000\n",
      "Epoch 58/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2189103.7500\n",
      "Epoch 58: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2188987.7500\n",
      "Epoch 59/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2187311.5000\n",
      "Epoch 59: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2187407.0000\n",
      "Epoch 60/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2187465.0000\n",
      "Epoch 60: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186999.0000\n",
      "Epoch 61/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2184477.2500\n",
      "Epoch 61: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184183.5000\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2179458.5000\n",
      "Epoch 62: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2179458.5000\n",
      "Epoch 63/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2186803.5000\n",
      "Epoch 63: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2187150.7500\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2172982.2500\n",
      "Epoch 64: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2172982.2500\n",
      "Epoch 65/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2186169.7500\n",
      "Epoch 65: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186664.5000\n",
      "Epoch 66/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2175786.0000\n",
      "Epoch 66: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175794.7500\n",
      "Epoch 67/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2177424.2500\n",
      "Epoch 67: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2178372.0000\n",
      "Epoch 68/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2184033.5000\n",
      "Epoch 68: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2183132.5000\n",
      "Epoch 69/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2184778.5000\n",
      "Epoch 69: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184549.0000\n",
      "Epoch 70/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2184429.0000\n",
      "Epoch 70: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2183119.2500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2178368.2500\n",
      "Epoch 71: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2178368.2500\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2181093.7500\n",
      "Epoch 72: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181820.5000\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2177263.7500\n",
      "Epoch 73: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2176343.5000\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2181218.5000\n",
      "Epoch 74: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181296.2500\n",
      "Epoch 75/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2173352.2500\n",
      "Epoch 75: loss did not improve from 2167510.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2174866.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2165694.0000\n",
      "Epoch 76: loss improved from 2167510.00000 to 2164695.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2164695.7500\n",
      "Epoch 77/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2180433.0000\n",
      "Epoch 77: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180988.0000\n",
      "Epoch 78/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2184563.2500\n",
      "Epoch 78: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184283.2500\n",
      "Epoch 79/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2181064.0000\n",
      "Epoch 79: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182586.0000\n",
      "Epoch 80/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2184064.5000\n",
      "Epoch 80: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184850.2500\n",
      "Epoch 81/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2169678.7500\n",
      "Epoch 81: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2170498.7500\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2177562.7500\n",
      "Epoch 82: loss did not improve from 2164695.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177562.7500\n",
      "Epoch 83/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2159750.5000\n",
      "Epoch 83: loss improved from 2164695.75000 to 2162709.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2162709.5000\n",
      "Epoch 84/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2182130.2500\n",
      "Epoch 84: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182139.0000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2183249.2500\n",
      "Epoch 85: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182901.5000\n",
      "Epoch 86/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2182861.7500\n",
      "Epoch 86: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182156.2500\n",
      "Epoch 87/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2180380.2500\n",
      "Epoch 87: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2178745.0000\n",
      "Epoch 88/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2180310.0000\n",
      "Epoch 88: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180397.0000\n",
      "Epoch 89/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2168347.5000\n",
      "Epoch 89: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2168347.5000\n",
      "Epoch 90/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2182041.0000\n",
      "Epoch 90: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181118.2500\n",
      "Epoch 91/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2176942.7500\n",
      "Epoch 91: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2176230.7500\n",
      "Epoch 92/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2172560.7500\n",
      "Epoch 92: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2173342.5000\n",
      "Epoch 93/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2166977.0000\n",
      "Epoch 93: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2167273.0000\n",
      "Epoch 94/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2170673.0000\n",
      "Epoch 94: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2173425.5000\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2175333.5000\n",
      "Epoch 95: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175333.5000\n",
      "Epoch 96/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2181191.2500\n",
      "Epoch 96: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180004.0000\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2175566.0000\n",
      "Epoch 97: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175725.0000\n",
      "Epoch 98/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2163145.0000\n",
      "Epoch 98: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2163310.7500\n",
      "Epoch 99/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2183938.2500\n",
      "Epoch 99: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182802.5000\n",
      "Epoch 100/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2180333.7500\n",
      "Epoch 100: loss did not improve from 2162709.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180775.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 01:13:22 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 01:13:22 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpbm8o6lwx\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpbm8o6lwx\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 1:01:39 - loss: 19191698.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0202s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0202s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3185/3189 [============================>.] - ETA: 0s - loss: 11107802.0000\n",
      "Epoch 1: loss improved from inf to 11097644.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 11097644.0000\n",
      "Epoch 2/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2844826.7500\n",
      "Epoch 2: loss improved from 11097644.00000 to 2844393.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2844393.7500\n",
      "Epoch 3/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2639946.7500\n",
      "Epoch 3: loss improved from 2844393.75000 to 2640636.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2640636.5000\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2600215.0000\n",
      "Epoch 4: loss improved from 2640636.50000 to 2600215.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2600215.0000\n",
      "Epoch 5/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2576911.7500\n",
      "Epoch 5: loss improved from 2600215.00000 to 2577287.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2577287.2500\n",
      "Epoch 6/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2562509.0000\n",
      "Epoch 6: loss improved from 2577287.25000 to 2562476.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2562476.2500\n",
      "Epoch 7/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2564345.7500\n",
      "Epoch 7: loss did not improve from 2562476.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2563597.5000\n",
      "Epoch 8/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2555764.5000\n",
      "Epoch 8: loss improved from 2562476.25000 to 2557104.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2557104.2500\n",
      "Epoch 9/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2548729.2500\n",
      "Epoch 9: loss improved from 2557104.25000 to 2548526.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2548526.2500\n",
      "Epoch 10/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2552097.7500\n",
      "Epoch 10: loss did not improve from 2548526.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2552584.0000\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2553547.2500\n",
      "Epoch 11: loss did not improve from 2548526.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2551398.0000\n",
      "Epoch 12/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2548623.0000\n",
      "Epoch 12: loss improved from 2548526.25000 to 2546973.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2546973.2500\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2553499.7500\n",
      "Epoch 13: loss did not improve from 2546973.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2553499.7500\n",
      "Epoch 14/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2550616.0000\n",
      "Epoch 14: loss did not improve from 2546973.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2550887.5000\n",
      "Epoch 15/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2550711.0000\n",
      "Epoch 15: loss did not improve from 2546973.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550783.7500\n",
      "Epoch 16/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2545325.7500\n",
      "Epoch 16: loss improved from 2546973.25000 to 2545270.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545270.7500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2546368.5000\n",
      "Epoch 17: loss did not improve from 2545270.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545787.0000\n",
      "Epoch 18/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2545737.7500\n",
      "Epoch 18: loss did not improve from 2545270.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2545977.0000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2541031.0000\n",
      "Epoch 19: loss improved from 2545270.75000 to 2540398.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2540398.2500\n",
      "Epoch 20/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2546630.5000\n",
      "Epoch 20: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2546437.5000\n",
      "Epoch 21/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2547382.7500\n",
      "Epoch 21: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2547993.7500\n",
      "Epoch 22/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2542297.5000\n",
      "Epoch 22: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2541932.5000\n",
      "Epoch 23/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2544895.0000\n",
      "Epoch 23: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2543771.5000\n",
      "Epoch 24/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2544404.2500\n",
      "Epoch 24: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2542964.5000\n",
      "Epoch 25/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2540459.0000\n",
      "Epoch 25: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2541112.7500\n",
      "Epoch 26/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2544204.7500\n",
      "Epoch 26: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2543417.5000\n",
      "Epoch 27/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2541292.5000\n",
      "Epoch 27: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2541275.7500\n",
      "Epoch 28/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2545964.2500\n",
      "Epoch 28: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2546048.7500\n",
      "Epoch 29/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2537435.0000\n",
      "Epoch 29: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2541015.0000\n",
      "Epoch 30/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2542940.5000\n",
      "Epoch 30: loss did not improve from 2540398.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2542426.5000\n",
      "Epoch 31/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2538317.5000\n",
      "Epoch 31: loss improved from 2540398.25000 to 2538113.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2538113.2500\n",
      "Epoch 32/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2544001.0000\n",
      "Epoch 32: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2544040.7500\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2542704.0000\n",
      "Epoch 33: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2542704.0000\n",
      "Epoch 34/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2539693.5000\n",
      "Epoch 34: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539136.0000\n",
      "Epoch 35/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2539645.7500\n",
      "Epoch 35: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539018.0000\n",
      "Epoch 36/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2539398.2500\n",
      "Epoch 36: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2540628.5000\n",
      "Epoch 37/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2542484.7500\n",
      "Epoch 37: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2542416.5000\n",
      "Epoch 38/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2538006.7500\n",
      "Epoch 38: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2538763.7500\n",
      "Epoch 39/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537174.5000\n",
      "Epoch 39: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2538563.5000\n",
      "Epoch 40/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2537633.0000\n",
      "Epoch 40: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2540157.2500\n",
      "Epoch 41/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2541933.5000\n",
      "Epoch 41: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2540665.7500\n",
      "Epoch 42/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2539650.2500\n",
      "Epoch 42: loss did not improve from 2538113.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539531.5000\n",
      "Epoch 43/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2537825.2500\n",
      "Epoch 43: loss improved from 2538113.25000 to 2538015.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2538015.5000\n",
      "Epoch 44/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2540363.2500\n",
      "Epoch 44: loss did not improve from 2538015.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2538606.7500\n",
      "Epoch 45/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2534737.5000\n",
      "Epoch 45: loss improved from 2538015.50000 to 2534249.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2534249.5000\n",
      "Epoch 46/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2539513.5000\n",
      "Epoch 46: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539513.5000\n",
      "Epoch 47/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2538571.2500\n",
      "Epoch 47: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2538196.5000\n",
      "Epoch 48/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2537199.2500\n",
      "Epoch 48: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2538098.2500\n",
      "Epoch 49/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2540297.7500\n",
      "Epoch 49: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539173.5000\n",
      "Epoch 50/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2533227.5000\n",
      "Epoch 50: loss did not improve from 2534249.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535477.7500\n",
      "Epoch 51/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2534814.5000\n",
      "Epoch 51: loss improved from 2534249.50000 to 2533886.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533886.7500\n",
      "Epoch 52/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2532891.2500\n",
      "Epoch 52: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535284.7500\n",
      "Epoch 53/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2541116.5000\n",
      "Epoch 53: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2540405.5000\n",
      "Epoch 54/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533559.0000\n",
      "Epoch 54: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2536287.5000\n",
      "Epoch 55/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537666.5000\n",
      "Epoch 55: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2536971.5000\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2539131.5000\n",
      "Epoch 56: loss did not improve from 2533886.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539175.0000\n",
      "Epoch 57/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2533690.0000\n",
      "Epoch 57: loss improved from 2533886.75000 to 2533788.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533788.5000\n",
      "Epoch 58/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2535573.7500\n",
      "Epoch 58: loss did not improve from 2533788.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2534479.7500\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2534270.7500\n",
      "Epoch 59: loss did not improve from 2533788.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2534129.2500\n",
      "Epoch 60/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2532775.2500\n",
      "Epoch 60: loss did not improve from 2533788.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2534876.7500\n",
      "Epoch 61/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2533157.5000\n",
      "Epoch 61: loss improved from 2533788.50000 to 2533345.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533345.5000\n",
      "Epoch 62/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2535486.5000\n",
      "Epoch 62: loss did not improve from 2533345.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535149.5000\n",
      "Epoch 63/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2534347.0000\n",
      "Epoch 63: loss did not improve from 2533345.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535315.0000\n",
      "Epoch 64/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2535513.7500\n",
      "Epoch 64: loss did not improve from 2533345.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537157.7500\n",
      "Epoch 65/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2536504.2500\n",
      "Epoch 65: loss did not improve from 2533345.50000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535837.5000\n",
      "Epoch 66/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2530690.2500\n",
      "Epoch 66: loss improved from 2533345.50000 to 2529133.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2529133.2500\n",
      "Epoch 67/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2536785.7500\n",
      "Epoch 67: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535415.0000\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2529861.2500\n",
      "Epoch 68: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2529769.2500\n",
      "Epoch 69/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2531574.0000\n",
      "Epoch 69: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531582.2500\n",
      "Epoch 70/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2533148.7500\n",
      "Epoch 70: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533039.5000\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537630.0000\n",
      "Epoch 71: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537313.5000\n",
      "Epoch 72/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2530070.7500\n",
      "Epoch 72: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532218.0000\n",
      "Epoch 73/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2533588.2500\n",
      "Epoch 73: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533565.7500\n",
      "Epoch 74/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2532389.0000\n",
      "Epoch 74: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532412.7500\n",
      "Epoch 75/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2534423.0000\n",
      "Epoch 75: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533385.7500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533193.2500\n",
      "Epoch 76: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533891.0000\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533551.0000\n",
      "Epoch 77: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532689.7500\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2533885.7500\n",
      "Epoch 78: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532937.5000\n",
      "Epoch 79/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2533221.5000\n",
      "Epoch 79: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2536790.7500\n",
      "Epoch 80/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2537110.2500\n",
      "Epoch 80: loss did not improve from 2529133.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535827.7500\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2526967.2500\n",
      "Epoch 81: loss improved from 2529133.25000 to 2526753.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2526753.2500\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2533138.7500\n",
      "Epoch 82: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533545.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2537995.5000\n",
      "Epoch 83: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535835.7500\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2531094.5000\n",
      "Epoch 84: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531822.5000\n",
      "Epoch 85/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2533071.2500\n",
      "Epoch 85: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532976.0000\n",
      "Epoch 86/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2532837.2500\n",
      "Epoch 86: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532623.2500\n",
      "Epoch 87/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2528791.5000\n",
      "Epoch 87: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2529432.7500\n",
      "Epoch 88/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2533043.7500\n",
      "Epoch 88: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532796.7500\n",
      "Epoch 89/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2533772.2500\n",
      "Epoch 89: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532908.2500\n",
      "Epoch 90/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2530201.7500\n",
      "Epoch 90: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2530296.0000\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2527925.7500\n",
      "Epoch 91: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2527925.7500\n",
      "Epoch 92/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2531044.5000\n",
      "Epoch 92: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531203.2500\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2531121.0000\n",
      "Epoch 93: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531091.7500\n",
      "Epoch 94/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2536345.2500\n",
      "Epoch 94: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2535917.0000\n",
      "Epoch 95/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2537787.5000\n",
      "Epoch 95: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537265.7500\n",
      "Epoch 96/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2530093.7500\n",
      "Epoch 96: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531078.5000\n",
      "Epoch 97/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2535513.7500\n",
      "Epoch 97: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2533738.2500\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2531969.7500\n",
      "Epoch 98: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2531969.7500\n",
      "Epoch 99/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2527247.5000\n",
      "Epoch 99: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2527489.2500\n",
      "Epoch 100/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2533704.5000\n",
      "Epoch 100: loss did not improve from 2526753.25000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2532724.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 01:32:46 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 01:32:46 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn7lsf9ze\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn7lsf9ze\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 5:28:18 - loss: 2811787.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0283s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3180/3189 [============================>.] - ETA: 0s - loss: 2560252.0000\n",
      "Epoch 1: loss improved from inf to 2560849.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 3ms/step - loss: 2560849.0000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2389209.7500\n",
      "Epoch 2: loss improved from 2560849.00000 to 2390110.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2390110.7500\n",
      "Epoch 3/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2368434.0000\n",
      "Epoch 3: loss improved from 2390110.75000 to 2366883.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2366883.2500\n",
      "Epoch 4/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2355722.2500\n",
      "Epoch 4: loss improved from 2366883.25000 to 2354738.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2354738.2500\n",
      "Epoch 5/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2345760.0000\n",
      "Epoch 5: loss improved from 2354738.25000 to 2345166.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2345166.2500\n",
      "Epoch 6/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2337849.5000\n",
      "Epoch 6: loss improved from 2345166.25000 to 2337576.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2337576.2500\n",
      "Epoch 7/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2332957.0000\n",
      "Epoch 7: loss improved from 2337576.25000 to 2332009.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2332009.5000\n",
      "Epoch 8/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2325761.5000\n",
      "Epoch 8: loss improved from 2332009.50000 to 2325662.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2325662.7500\n",
      "Epoch 9/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2324648.2500\n",
      "Epoch 9: loss improved from 2325662.75000 to 2324146.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2324146.5000\n",
      "Epoch 10/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2322286.7500\n",
      "Epoch 10: loss improved from 2324146.50000 to 2320460.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2320460.2500\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2320577.2500\n",
      "Epoch 11: loss improved from 2320460.25000 to 2320188.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2320188.5000\n",
      "Epoch 12/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2314697.7500\n",
      "Epoch 12: loss improved from 2320188.50000 to 2313240.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2313240.0000\n",
      "Epoch 13/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2316544.0000\n",
      "Epoch 13: loss did not improve from 2313240.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2314451.2500\n",
      "Epoch 14/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2310728.5000\n",
      "Epoch 14: loss improved from 2313240.00000 to 2310467.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2310467.0000\n",
      "Epoch 15/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2309554.2500\n",
      "Epoch 15: loss improved from 2310467.00000 to 2309193.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2309193.0000\n",
      "Epoch 16/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2305291.2500\n",
      "Epoch 16: loss improved from 2309193.00000 to 2306636.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2306636.2500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2306075.0000\n",
      "Epoch 17: loss improved from 2306636.25000 to 2305422.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2305422.5000\n",
      "Epoch 18/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2300909.5000\n",
      "Epoch 18: loss improved from 2305422.50000 to 2302379.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2302379.2500\n",
      "Epoch 19/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2301915.5000\n",
      "Epoch 19: loss improved from 2302379.25000 to 2301512.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2301512.7500\n",
      "Epoch 20/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2300240.0000\n",
      "Epoch 20: loss improved from 2301512.75000 to 2301370.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2301370.2500\n",
      "Epoch 21/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2299396.5000\n",
      "Epoch 21: loss improved from 2301370.25000 to 2300274.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2300274.2500\n",
      "Epoch 22/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2300571.2500\n",
      "Epoch 22: loss improved from 2300274.25000 to 2299389.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2299389.5000\n",
      "Epoch 23/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2296819.2500\n",
      "Epoch 23: loss improved from 2299389.50000 to 2297670.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2297670.0000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2295696.2500\n",
      "Epoch 24: loss improved from 2297670.00000 to 2296084.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2296084.2500\n",
      "Epoch 25/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2294882.0000\n",
      "Epoch 25: loss improved from 2296084.25000 to 2295552.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2295552.5000\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2295506.0000\n",
      "Epoch 26: loss improved from 2295552.50000 to 2295506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2295506.0000\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2291655.2500\n",
      "Epoch 27: loss improved from 2295506.00000 to 2291655.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2291655.2500\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2290957.5000\n",
      "Epoch 28: loss improved from 2291655.25000 to 2290102.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2290102.7500\n",
      "Epoch 29/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2290204.7500\n",
      "Epoch 29: loss did not improve from 2290102.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2290362.2500\n",
      "Epoch 30/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2290282.5000\n",
      "Epoch 30: loss did not improve from 2290102.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2290385.2500\n",
      "Epoch 31/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2286929.7500\n",
      "Epoch 31: loss improved from 2290102.75000 to 2287368.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2287368.7500\n",
      "Epoch 32/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2290484.0000\n",
      "Epoch 32: loss did not improve from 2287368.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2290398.0000\n",
      "Epoch 33/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2290487.5000\n",
      "Epoch 33: loss did not improve from 2287368.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2287906.5000\n",
      "Epoch 34/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2286361.5000\n",
      "Epoch 34: loss improved from 2287368.75000 to 2286092.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2286092.0000\n",
      "Epoch 35/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2286885.2500\n",
      "Epoch 35: loss did not improve from 2286092.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2286364.5000\n",
      "Epoch 36/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2286957.0000\n",
      "Epoch 36: loss did not improve from 2286092.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2286957.0000\n",
      "Epoch 37/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2285167.7500\n",
      "Epoch 37: loss improved from 2286092.00000 to 2284869.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2284869.7500\n",
      "Epoch 38/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2283383.0000\n",
      "Epoch 38: loss improved from 2284869.75000 to 2283710.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2283710.7500\n",
      "Epoch 39/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2281891.5000\n",
      "Epoch 39: loss improved from 2283710.75000 to 2283650.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2283650.5000\n",
      "Epoch 40/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2279010.5000\n",
      "Epoch 40: loss improved from 2283650.50000 to 2282116.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282116.2500\n",
      "Epoch 41/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2282332.5000\n",
      "Epoch 41: loss did not improve from 2282116.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282840.7500\n",
      "Epoch 42/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2282573.0000\n",
      "Epoch 42: loss did not improve from 2282116.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282426.5000\n",
      "Epoch 43/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2279686.2500\n",
      "Epoch 43: loss improved from 2282116.25000 to 2282048.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282048.5000\n",
      "Epoch 44/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2283457.7500\n",
      "Epoch 44: loss improved from 2282048.50000 to 2281460.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2281460.0000\n",
      "Epoch 45/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2281939.5000\n",
      "Epoch 45: loss did not improve from 2281460.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2281583.2500\n",
      "Epoch 46/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2282548.7500\n",
      "Epoch 46: loss did not improve from 2281460.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2282099.2500\n",
      "Epoch 47/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2280712.0000\n",
      "Epoch 47: loss improved from 2281460.00000 to 2280459.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280459.5000\n",
      "Epoch 48/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2280253.0000\n",
      "Epoch 48: loss improved from 2280459.50000 to 2279261.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279261.0000\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2279596.0000\n",
      "Epoch 49: loss did not improve from 2279261.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279596.0000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2278614.0000\n",
      "Epoch 50: loss improved from 2279261.00000 to 2278805.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278805.5000\n",
      "Epoch 51/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2279499.2500\n",
      "Epoch 51: loss improved from 2278805.50000 to 2278660.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2278660.5000\n",
      "Epoch 52/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2278584.5000\n",
      "Epoch 52: loss improved from 2278660.50000 to 2277370.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2277370.5000\n",
      "Epoch 53/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2279734.5000\n",
      "Epoch 53: loss did not improve from 2277370.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278704.0000\n",
      "Epoch 54/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2276906.0000\n",
      "Epoch 54: loss improved from 2277370.50000 to 2276749.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276749.5000\n",
      "Epoch 55/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2278281.5000\n",
      "Epoch 55: loss did not improve from 2276749.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2277201.7500\n",
      "Epoch 56/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2279991.7500\n",
      "Epoch 56: loss improved from 2276749.50000 to 2276327.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2276327.0000\n",
      "Epoch 57/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2277407.0000\n",
      "Epoch 57: loss did not improve from 2276327.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276651.0000\n",
      "Epoch 58/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2276829.0000\n",
      "Epoch 58: loss improved from 2276327.00000 to 2276180.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276180.7500\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2275542.7500\n",
      "Epoch 59: loss improved from 2276180.75000 to 2275689.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275689.5000\n",
      "Epoch 60/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2274834.0000\n",
      "Epoch 60: loss did not improve from 2275689.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275782.7500\n",
      "Epoch 61/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2274675.7500\n",
      "Epoch 61: loss improved from 2275689.50000 to 2275326.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2275326.2500\n",
      "Epoch 62/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2276207.5000\n",
      "Epoch 62: loss improved from 2275326.25000 to 2274646.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274646.0000\n",
      "Epoch 63/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2274401.5000\n",
      "Epoch 63: loss did not improve from 2274646.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275051.0000\n",
      "Epoch 64/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2273392.2500\n",
      "Epoch 64: loss improved from 2274646.00000 to 2274387.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274387.5000\n",
      "Epoch 65/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2273008.5000\n",
      "Epoch 65: loss improved from 2274387.50000 to 2273618.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273618.7500\n",
      "Epoch 66/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2275070.7500\n",
      "Epoch 66: loss did not improve from 2273618.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2274448.5000\n",
      "Epoch 67/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2273746.0000\n",
      "Epoch 67: loss improved from 2273618.75000 to 2273398.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2273398.2500\n",
      "Epoch 68/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2273205.2500\n",
      "Epoch 68: loss improved from 2273398.25000 to 2273206.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273206.5000\n",
      "Epoch 69/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2272849.7500\n",
      "Epoch 69: loss did not improve from 2273206.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273665.2500\n",
      "Epoch 70/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2272241.5000\n",
      "Epoch 70: loss improved from 2273206.50000 to 2272110.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2272110.5000\n",
      "Epoch 71/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2271344.5000\n",
      "Epoch 71: loss did not improve from 2272110.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272385.2500\n",
      "Epoch 72/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2270462.2500\n",
      "Epoch 72: loss improved from 2272110.50000 to 2270982.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270982.2500\n",
      "Epoch 73/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2270735.0000\n",
      "Epoch 73: loss improved from 2270982.25000 to 2270666.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270666.5000\n",
      "Epoch 74/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2272719.0000\n",
      "Epoch 74: loss did not improve from 2270666.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272003.5000\n",
      "Epoch 75/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2270623.0000\n",
      "Epoch 75: loss did not improve from 2270666.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271227.0000\n",
      "Epoch 76/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2269256.2500\n",
      "Epoch 76: loss improved from 2270666.50000 to 2270626.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270626.5000\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2271179.0000\n",
      "Epoch 77: loss did not improve from 2270626.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271179.0000\n",
      "Epoch 78/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2271529.5000\n",
      "Epoch 78: loss did not improve from 2270626.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271506.2500\n",
      "Epoch 79/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2272528.2500\n",
      "Epoch 79: loss did not improve from 2270626.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271421.5000\n",
      "Epoch 80/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2269264.5000\n",
      "Epoch 80: loss improved from 2270626.50000 to 2269954.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269954.5000\n",
      "Epoch 81/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2269067.7500\n",
      "Epoch 81: loss improved from 2269954.50000 to 2269442.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269442.2500\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2270836.2500\n",
      "Epoch 82: loss did not improve from 2269442.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270964.0000\n",
      "Epoch 83/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2269263.5000\n",
      "Epoch 83: loss did not improve from 2269442.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270064.7500\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2267840.7500\n",
      "Epoch 84: loss improved from 2269442.25000 to 2268909.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268909.2500\n",
      "Epoch 85/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2268453.5000\n",
      "Epoch 85: loss did not improve from 2268909.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269930.2500\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2270060.0000\n",
      "Epoch 86: loss improved from 2268909.25000 to 2268447.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268447.0000\n",
      "Epoch 87/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2268942.7500\n",
      "Epoch 87: loss did not improve from 2268447.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268570.5000\n",
      "Epoch 88/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2267589.2500\n",
      "Epoch 88: loss did not improve from 2268447.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268629.0000\n",
      "Epoch 89/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2269065.7500\n",
      "Epoch 89: loss did not improve from 2268447.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268871.2500\n",
      "Epoch 90/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2270560.0000\n",
      "Epoch 90: loss did not improve from 2268447.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269459.0000\n",
      "Epoch 91/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2267473.0000\n",
      "Epoch 91: loss improved from 2268447.00000 to 2267350.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267350.5000\n",
      "Epoch 92/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2267467.7500\n",
      "Epoch 92: loss improved from 2267350.50000 to 2267344.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267344.7500\n",
      "Epoch 93/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2262728.5000\n",
      "Epoch 93: loss did not improve from 2267344.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268457.0000\n",
      "Epoch 94/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2268690.2500\n",
      "Epoch 94: loss did not improve from 2267344.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268153.0000\n",
      "Epoch 95/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2267831.0000\n",
      "Epoch 95: loss did not improve from 2267344.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267899.5000\n",
      "Epoch 96/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2264009.2500\n",
      "Epoch 96: loss improved from 2267344.75000 to 2266331.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266331.5000\n",
      "Epoch 97/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2268362.0000\n",
      "Epoch 97: loss did not improve from 2266331.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268509.0000\n",
      "Epoch 98/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2267877.7500\n",
      "Epoch 98: loss did not improve from 2266331.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267078.5000\n",
      "Epoch 99/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2267267.7500\n",
      "Epoch 99: loss did not improve from 2266331.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267173.7500\n",
      "Epoch 100/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2267546.0000\n",
      "Epoch 100: loss did not improve from 2266331.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267070.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 01:49:33 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 01:49:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpp7e1ckq1\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpp7e1ckq1\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_56 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   6/3189 [..............................] - ETA: 1:25 - loss: 20811474.0000 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0209s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3181/3189 [============================>.] - ETA: 0s - loss: 2759942.0000\n",
      "Epoch 1: loss improved from inf to 2758039.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 24s 3ms/step - loss: 2758039.7500\n",
      "Epoch 2/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2418264.0000\n",
      "Epoch 2: loss improved from 2758039.75000 to 2419434.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2419434.7500\n",
      "Epoch 3/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2375783.2500\n",
      "Epoch 3: loss improved from 2419434.75000 to 2375297.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2375297.2500\n",
      "Epoch 4/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2361154.2500\n",
      "Epoch 4: loss improved from 2375297.25000 to 2359499.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2359499.7500\n",
      "Epoch 5/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2349461.5000\n",
      "Epoch 5: loss improved from 2359499.75000 to 2348873.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2348873.0000\n",
      "Epoch 6/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2341913.2500\n",
      "Epoch 6: loss improved from 2348873.00000 to 2341012.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2341012.0000\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2334512.5000\n",
      "Epoch 7: loss improved from 2341012.00000 to 2334512.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2334512.5000\n",
      "Epoch 8/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2331009.7500\n",
      "Epoch 8: loss improved from 2334512.50000 to 2331009.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2331009.7500\n",
      "Epoch 9/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2321855.2500\n",
      "Epoch 9: loss improved from 2331009.75000 to 2323777.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2323777.5000\n",
      "Epoch 10/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2321313.2500\n",
      "Epoch 10: loss improved from 2323777.50000 to 2322290.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2322290.2500\n",
      "Epoch 11/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2317449.2500\n",
      "Epoch 11: loss improved from 2322290.25000 to 2317652.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2317652.2500\n",
      "Epoch 12/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2314036.2500\n",
      "Epoch 12: loss improved from 2317652.25000 to 2312942.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2312942.5000\n",
      "Epoch 13/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2313133.5000\n",
      "Epoch 13: loss improved from 2312942.50000 to 2312608.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2312608.2500\n",
      "Epoch 14/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2310288.5000\n",
      "Epoch 14: loss improved from 2312608.25000 to 2311047.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2311047.0000\n",
      "Epoch 15/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2308403.2500\n",
      "Epoch 15: loss improved from 2311047.00000 to 2308406.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2308406.2500\n",
      "Epoch 16/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2305285.2500\n",
      "Epoch 16: loss improved from 2308406.25000 to 2306312.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2306312.2500\n",
      "Epoch 17/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2307433.2500\n",
      "Epoch 17: loss improved from 2306312.25000 to 2305961.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2305961.7500\n",
      "Epoch 18/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2300661.7500\n",
      "Epoch 18: loss improved from 2305961.75000 to 2304299.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2304299.5000\n",
      "Epoch 19/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2302123.7500\n",
      "Epoch 19: loss improved from 2304299.50000 to 2303244.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2303244.7500\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2298063.0000\n",
      "Epoch 20: loss improved from 2303244.75000 to 2300004.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2300004.5000\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2299062.7500\n",
      "Epoch 21: loss improved from 2300004.50000 to 2299062.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2299062.7500\n",
      "Epoch 22/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2297088.7500\n",
      "Epoch 22: loss improved from 2299062.75000 to 2296256.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2296256.7500\n",
      "Epoch 23/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2296147.0000\n",
      "Epoch 23: loss did not improve from 2296256.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2296555.5000\n",
      "Epoch 24/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2295291.7500\n",
      "Epoch 24: loss improved from 2296256.75000 to 2294994.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2294994.5000\n",
      "Epoch 25/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2295098.0000\n",
      "Epoch 25: loss did not improve from 2294994.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2296237.5000\n",
      "Epoch 26/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2294448.7500\n",
      "Epoch 26: loss improved from 2294994.50000 to 2294561.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2294561.0000\n",
      "Epoch 27/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2292872.0000\n",
      "Epoch 27: loss improved from 2294561.00000 to 2292580.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2292580.5000\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2289747.7500\n",
      "Epoch 28: loss improved from 2292580.50000 to 2289962.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2289962.7500\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2290249.5000\n",
      "Epoch 29: loss did not improve from 2289962.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2290249.5000\n",
      "Epoch 30/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2290103.5000\n",
      "Epoch 30: loss improved from 2289962.75000 to 2289617.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2289617.2500\n",
      "Epoch 31/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2286504.0000\n",
      "Epoch 31: loss improved from 2289617.25000 to 2287700.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2287700.5000\n",
      "Epoch 32/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2287007.2500\n",
      "Epoch 32: loss improved from 2287700.50000 to 2286866.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2286866.7500\n",
      "Epoch 33/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2288221.5000\n",
      "Epoch 33: loss did not improve from 2286866.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2288381.2500\n",
      "Epoch 34/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2289436.7500\n",
      "Epoch 34: loss did not improve from 2286866.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2288531.5000\n",
      "Epoch 35/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2284622.0000\n",
      "Epoch 35: loss improved from 2286866.75000 to 2286263.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2286263.5000\n",
      "Epoch 36/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2285875.0000\n",
      "Epoch 36: loss improved from 2286263.50000 to 2285628.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2285628.7500\n",
      "Epoch 37/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2282989.5000\n",
      "Epoch 37: loss improved from 2285628.75000 to 2283862.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2283862.0000\n",
      "Epoch 38/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2283189.7500\n",
      "Epoch 38: loss did not improve from 2283862.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2284667.7500\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2284019.7500\n",
      "Epoch 39: loss improved from 2283862.00000 to 2283513.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2283513.5000\n",
      "Epoch 40/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2283844.2500\n",
      "Epoch 40: loss did not improve from 2283513.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2284236.5000\n",
      "Epoch 41/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2284127.0000\n",
      "Epoch 41: loss improved from 2283513.50000 to 2282702.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2282702.7500\n",
      "Epoch 42/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2281655.7500\n",
      "Epoch 42: loss improved from 2282702.75000 to 2281764.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2281764.0000\n",
      "Epoch 43/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2280888.7500\n",
      "Epoch 43: loss improved from 2281764.00000 to 2280860.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280860.7500\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2281563.7500\n",
      "Epoch 44: loss did not improve from 2280860.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2281489.2500\n",
      "Epoch 45/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2278245.5000\n",
      "Epoch 45: loss improved from 2280860.75000 to 2279814.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279814.7500\n",
      "Epoch 46/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2281732.7500\n",
      "Epoch 46: loss did not improve from 2279814.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280442.0000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2280151.2500\n",
      "Epoch 47: loss did not improve from 2279814.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280151.2500\n",
      "Epoch 48/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2279210.2500\n",
      "Epoch 48: loss did not improve from 2279814.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279853.2500\n",
      "Epoch 49/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2278132.2500\n",
      "Epoch 49: loss improved from 2279814.75000 to 2277987.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2277987.7500\n",
      "Epoch 50/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2276575.7500\n",
      "Epoch 50: loss did not improve from 2277987.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278090.5000\n",
      "Epoch 51/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2277433.2500\n",
      "Epoch 51: loss improved from 2277987.75000 to 2277752.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2277752.5000\n",
      "Epoch 52/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2276831.5000\n",
      "Epoch 52: loss improved from 2277752.50000 to 2277294.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2277294.2500\n",
      "Epoch 53/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2277888.2500\n",
      "Epoch 53: loss did not improve from 2277294.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2278275.7500\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2276161.7500\n",
      "Epoch 54: loss improved from 2277294.25000 to 2276022.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276022.7500\n",
      "Epoch 55/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2279017.0000\n",
      "Epoch 55: loss did not improve from 2276022.75000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2278244.7500\n",
      "Epoch 56/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2277417.7500\n",
      "Epoch 56: loss did not improve from 2276022.75000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2276832.2500\n",
      "Epoch 57/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2277462.2500\n",
      "Epoch 57: loss improved from 2276022.75000 to 2274992.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2274992.2500\n",
      "Epoch 58/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2275562.2500\n",
      "Epoch 58: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275589.5000\n",
      "Epoch 59/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2275299.5000\n",
      "Epoch 59: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275181.5000\n",
      "Epoch 60/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2275591.0000\n",
      "Epoch 60: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275127.0000\n",
      "Epoch 61/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2275971.7500\n",
      "Epoch 61: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275112.0000\n",
      "Epoch 62/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2275186.5000\n",
      "Epoch 62: loss did not improve from 2274992.25000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2275378.7500\n",
      "Epoch 63/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2274865.7500\n",
      "Epoch 63: loss improved from 2274992.25000 to 2274603.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2274603.2500\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2274611.7500\n",
      "Epoch 64: loss improved from 2274603.25000 to 2274319.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2274319.5000\n",
      "Epoch 65/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2273264.7500\n",
      "Epoch 65: loss improved from 2274319.50000 to 2273081.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2273081.5000\n",
      "Epoch 66/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2274766.0000\n",
      "Epoch 66: loss did not improve from 2273081.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274415.5000\n",
      "Epoch 67/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2273320.5000\n",
      "Epoch 67: loss improved from 2273081.50000 to 2272530.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272530.5000\n",
      "Epoch 68/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2273210.2500\n",
      "Epoch 68: loss did not improve from 2272530.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272658.5000\n",
      "Epoch 69/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2271259.2500\n",
      "Epoch 69: loss improved from 2272530.50000 to 2271575.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271575.2500\n",
      "Epoch 70/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2271950.0000\n",
      "Epoch 70: loss did not improve from 2271575.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272617.7500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2271419.5000\n",
      "Epoch 71: loss improved from 2271575.25000 to 2271419.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271419.5000\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2272898.7500\n",
      "Epoch 72: loss did not improve from 2271419.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2272383.5000\n",
      "Epoch 73/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2270842.5000\n",
      "Epoch 73: loss improved from 2271419.50000 to 2271241.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271241.0000\n",
      "Epoch 74/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2270976.2500\n",
      "Epoch 74: loss did not improve from 2271241.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271451.5000\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2271179.2500\n",
      "Epoch 75: loss improved from 2271241.00000 to 2269861.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2269861.5000\n",
      "Epoch 76/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2271062.0000\n",
      "Epoch 76: loss did not improve from 2269861.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271385.0000\n",
      "Epoch 77/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2271949.0000\n",
      "Epoch 77: loss did not improve from 2269861.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271856.5000\n",
      "Epoch 78/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2269559.7500\n",
      "Epoch 78: loss did not improve from 2269861.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271255.0000\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2270864.0000\n",
      "Epoch 79: loss did not improve from 2269861.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271264.5000\n",
      "Epoch 80/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2269402.7500\n",
      "Epoch 80: loss improved from 2269861.50000 to 2269396.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269396.0000\n",
      "Epoch 81/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2269891.2500\n",
      "Epoch 81: loss did not improve from 2269396.00000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270655.0000\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2269133.2500\n",
      "Epoch 82: loss improved from 2269396.00000 to 2269133.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2269133.2500\n",
      "Epoch 83/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2271036.5000\n",
      "Epoch 83: loss did not improve from 2269133.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270418.5000\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2270326.0000\n",
      "Epoch 84: loss did not improve from 2269133.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269890.2500\n",
      "Epoch 85/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2270123.7500\n",
      "Epoch 85: loss did not improve from 2269133.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269377.5000\n",
      "Epoch 86/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2269897.0000\n",
      "Epoch 86: loss did not improve from 2269133.25000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269956.7500\n",
      "Epoch 87/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2266414.7500\n",
      "Epoch 87: loss improved from 2269133.25000 to 2269005.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269005.2500\n",
      "Epoch 88/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2268050.2500\n",
      "Epoch 88: loss improved from 2269005.25000 to 2268050.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268050.2500\n",
      "Epoch 89/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2265889.2500\n",
      "Epoch 89: loss improved from 2268050.25000 to 2266858.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266858.5000\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2270061.5000\n",
      "Epoch 90: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268645.5000\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2266799.5000\n",
      "Epoch 91: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267920.0000\n",
      "Epoch 92/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2268547.2500\n",
      "Epoch 92: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267624.0000\n",
      "Epoch 93/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2266680.0000\n",
      "Epoch 93: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268299.0000\n",
      "Epoch 94/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2266261.5000\n",
      "Epoch 94: loss did not improve from 2266858.50000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266977.0000\n",
      "Epoch 95/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2265476.7500\n",
      "Epoch 95: loss improved from 2266858.50000 to 2265919.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2265919.0000\n",
      "Epoch 96/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2269800.2500\n",
      "Epoch 96: loss did not improve from 2265919.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267434.5000\n",
      "Epoch 97/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2267787.0000\n",
      "Epoch 97: loss did not improve from 2265919.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266428.2500\n",
      "Epoch 98/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2267458.2500\n",
      "Epoch 98: loss did not improve from 2265919.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267604.7500\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2267436.5000\n",
      "Epoch 99: loss did not improve from 2265919.00000\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2266886.5000\n",
      "Epoch 100/100\n",
      "3172/3189 [============================>.] - ETA: 0s - loss: 2264371.7500\n",
      "Epoch 100: loss improved from 2265919.00000 to 2264374.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2264374.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 02:05:54 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 02:05:54 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp6_rvcflb\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp6_rvcflb\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 25, 25, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 12, 12, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 6, 6, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSamplin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_14 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737094.2500\n",
      "Epoch 1: loss improved from inf to 2736504.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 5ms/step - loss: 2736504.0000\n",
      "Epoch 2/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2737521.2500\n",
      "Epoch 2: loss did not improve from 2736504.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736506.0000\n",
      "Epoch 3/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737306.7500\n",
      "Epoch 3: loss improved from 2736504.00000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.7500\n",
      "Epoch 4/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2736835.7500\n",
      "Epoch 4: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.0000\n",
      "Epoch 5/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736853.5000\n",
      "Epoch 5: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.2500\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 6: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736504.5000\n",
      "Epoch 7/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735920.0000\n",
      "Epoch 7: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736505.7500\n",
      "Epoch 8/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2734503.7500\n",
      "Epoch 8: loss improved from 2736503.75000 to 2736498.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736498.5000\n",
      "Epoch 9/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736230.7500\n",
      "Epoch 9: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736504.0000\n",
      "Epoch 10/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735528.0000\n",
      "Epoch 10: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.5000\n",
      "Epoch 11/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737574.5000\n",
      "Epoch 11: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.7500\n",
      "Epoch 12/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736100.7500\n",
      "Epoch 12: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736022.2500\n",
      "Epoch 13: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 14/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2735289.2500\n",
      "Epoch 14: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.7500\n",
      "Epoch 15/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737301.7500\n",
      "Epoch 15: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 16/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737081.0000\n",
      "Epoch 16: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.2500\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737010.0000\n",
      "Epoch 17: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.7500\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736552.0000\n",
      "Epoch 18: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.7500\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737216.0000\n",
      "Epoch 19: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.5000\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735598.5000\n",
      "Epoch 20: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 21/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735678.5000\n",
      "Epoch 21: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736502.5000\n",
      "Epoch 22/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2735567.0000\n",
      "Epoch 22: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736504.5000\n",
      "Epoch 23/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736732.0000\n",
      "Epoch 23: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736503.0000\n",
      "Epoch 24/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736589.2500\n",
      "Epoch 24: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.0000\n",
      "Epoch 25/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2737480.7500\n",
      "Epoch 25: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.5000\n",
      "Epoch 26/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736677.2500\n",
      "Epoch 26: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736500.2500\n",
      "Epoch 27/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736914.5000\n",
      "Epoch 27: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736499.0000\n",
      "Epoch 28/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2735519.2500\n",
      "Epoch 28: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.5000\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.2500\n",
      "Epoch 29: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.2500\n",
      "Epoch 30/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736604.7500\n",
      "Epoch 30: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.7500\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2734831.2500\n",
      "Epoch 31: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 32/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736816.0000\n",
      "Epoch 32: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.5000\n",
      "Epoch 33/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736426.5000\n",
      "Epoch 33: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.5000\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2737063.7500\n",
      "Epoch 34: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.0000\n",
      "Epoch 35/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736907.0000\n",
      "Epoch 35: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736499.0000\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735358.5000\n",
      "Epoch 36: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736499.2500\n",
      "Epoch 37/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2734436.5000\n",
      "Epoch 37: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.7500\n",
      "Epoch 38/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736499.5000\n",
      "Epoch 38: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736499.5000\n",
      "Epoch 39/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736084.0000\n",
      "Epoch 39: loss did not improve from 2736498.50000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736499.5000\n",
      "Epoch 40/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2734399.2500\n",
      "Epoch 40: loss improved from 2736498.50000 to 2736498.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.0000\n",
      "Epoch 41/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736296.7500\n",
      "Epoch 41: loss improved from 2736498.00000 to 2736497.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.7500\n",
      "Epoch 42/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2734896.7500\n",
      "Epoch 42: loss improved from 2736497.75000 to 2736497.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.2500\n",
      "Epoch 43/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2737436.2500\n",
      "Epoch 43: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736499.0000\n",
      "Epoch 44/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2736655.0000\n",
      "Epoch 44: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.5000\n",
      "Epoch 45/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736820.2500\n",
      "Epoch 45: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.5000\n",
      "Epoch 46/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2736139.0000\n",
      "Epoch 46: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.2500\n",
      "Epoch 47/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 47: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736500.5000\n",
      "Epoch 48/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736787.2500\n",
      "Epoch 48: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736498.0000\n",
      "Epoch 49/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2737603.0000\n",
      "Epoch 49: loss did not improve from 2736497.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736497.2500\n",
      "Epoch 50/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736243.0000\n",
      "Epoch 50: loss improved from 2736497.25000 to 2736497.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736497.0000\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.2500\n",
      "Epoch 51: loss did not improve from 2736497.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.2500\n",
      "Epoch 52/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736806.7500\n",
      "Epoch 52: loss did not improve from 2736497.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.5000\n",
      "Epoch 53/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735848.5000\n",
      "Epoch 53: loss improved from 2736497.00000 to 2736496.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.7500\n",
      "Epoch 54/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735036.0000\n",
      "Epoch 54: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736497.7500\n",
      "Epoch 55/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736498.2500\n",
      "Epoch 55: loss did not improve from 2736496.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.2500\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736333.0000\n",
      "Epoch 56: loss improved from 2736496.75000 to 2736494.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2734443.0000\n",
      "Epoch 57: loss did not improve from 2736494.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.0000\n",
      "Epoch 58/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2736984.5000\n",
      "Epoch 58: loss did not improve from 2736494.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 59/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736712.0000\n",
      "Epoch 59: loss improved from 2736494.50000 to 2736493.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 60/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736256.7500\n",
      "Epoch 60: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.5000\n",
      "Epoch 61/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736799.7500\n",
      "Epoch 61: loss did not improve from 2736493.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736498.2500\n",
      "Epoch 62/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736080.2500\n",
      "Epoch 62: loss improved from 2736493.75000 to 2736493.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.2500\n",
      "Epoch 63/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736248.0000\n",
      "Epoch 63: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 64/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2735757.0000\n",
      "Epoch 64: loss did not improve from 2736493.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.5000\n",
      "Epoch 65/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736496.5000\n",
      "Epoch 65: loss improved from 2736493.25000 to 2736492.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 66/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2734986.0000\n",
      "Epoch 66: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.2500\n",
      "Epoch 67/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737703.7500\n",
      "Epoch 67: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.2500\n",
      "Epoch 68/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736538.0000\n",
      "Epoch 68: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.5000\n",
      "Epoch 69/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737260.5000\n",
      "Epoch 69: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 70/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736638.5000\n",
      "Epoch 70: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.5000\n",
      "Epoch 71/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736823.5000\n",
      "Epoch 71: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.7500\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737168.0000\n",
      "Epoch 72: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 73/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2734973.5000\n",
      "Epoch 73: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 74/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736562.7500\n",
      "Epoch 74: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.7500\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2735665.7500\n",
      "Epoch 75: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736496.2500\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736494.7500\n",
      "Epoch 76: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2736780.2500\n",
      "Epoch 77: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.5000\n",
      "Epoch 78/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736340.5000\n",
      "Epoch 78: loss did not improve from 2736492.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736476.2500\n",
      "Epoch 79: loss improved from 2736492.75000 to 2736491.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.2500\n",
      "Epoch 80/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2736499.7500\n",
      "Epoch 80: loss did not improve from 2736491.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.7500\n",
      "Epoch 81/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736742.0000\n",
      "Epoch 81: loss improved from 2736491.25000 to 2736490.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.0000\n",
      "Epoch 82/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2736650.7500\n",
      "Epoch 82: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736496.2500\n",
      "Epoch 83/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736361.5000\n",
      "Epoch 83: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736495.0000\n",
      "Epoch 84/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2737532.7500\n",
      "Epoch 84: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.7500\n",
      "Epoch 85/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 85: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 86/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737021.2500\n",
      "Epoch 86: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 87/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736519.5000\n",
      "Epoch 87: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736493.2500\n",
      "Epoch 88/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736493.5000\n",
      "Epoch 88: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.0000\n",
      "Epoch 89/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737232.0000\n",
      "Epoch 89: loss did not improve from 2736490.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2735662.2500\n",
      "Epoch 90: loss improved from 2736490.00000 to 2736489.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736489.2500\n",
      "Epoch 91/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2736492.2500\n",
      "Epoch 91: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736491.7500\n",
      "Epoch 92/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736157.5000\n",
      "Epoch 92: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2736491.7500\n",
      "Epoch 93/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2737485.5000\n",
      "Epoch 93: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736491.7500\n",
      "Epoch 94/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2736568.5000\n",
      "Epoch 94: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 95/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736269.5000\n",
      "Epoch 95: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.2500\n",
      "Epoch 96/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737250.2500\n",
      "Epoch 96: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.5000\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736490.0000\n",
      "Epoch 97: loss did not improve from 2736489.25000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736490.0000\n",
      "Epoch 98/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2736710.2500\n",
      "Epoch 98: loss improved from 2736489.25000 to 2736488.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736488.0000\n",
      "Epoch 99/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2737688.0000\n",
      "Epoch 99: loss did not improve from 2736488.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736489.5000\n",
      "Epoch 100/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737287.0000\n",
      "Epoch 100: loss did not improve from 2736488.00000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 02:32:29 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 02:32:29 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmptl308g4u\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmptl308g4u\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_5 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 25, 25, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 12, 12, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 6, 6, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d_15 (UpSamplin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_16 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_17 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 31:14 - loss: 1190771.8750WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0078s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0078s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737485.7500\n",
      "Epoch 1: loss improved from inf to 2736505.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736505.7500\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 2: loss improved from 2736505.75000 to 2736505.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.2500\n",
      "Epoch 3/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736879.5000\n",
      "Epoch 3: loss improved from 2736505.25000 to 2736504.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736504.7500\n",
      "Epoch 4/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736708.7500\n",
      "Epoch 4: loss improved from 2736504.75000 to 2736503.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736503.5000\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737004.0000\n",
      "Epoch 5: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736505.2500\n",
      "Epoch 6/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2737117.2500\n",
      "Epoch 6: loss improved from 2736503.50000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.7500\n",
      "Epoch 7/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2736350.7500\n",
      "Epoch 7: loss improved from 2736502.75000 to 2736500.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736500.5000\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735686.2500\n",
      "Epoch 8: loss improved from 2736500.50000 to 2736457.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2736457.2500\n",
      "Epoch 9/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2712281.7500\n",
      "Epoch 9: loss improved from 2736457.25000 to 2711936.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2711936.7500\n",
      "Epoch 10/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2592624.2500\n",
      "Epoch 10: loss improved from 2711936.75000 to 2591686.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2591686.7500\n",
      "Epoch 11/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2426374.0000\n",
      "Epoch 11: loss improved from 2591686.75000 to 2425918.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2425918.5000\n",
      "Epoch 12/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2313225.2500\n",
      "Epoch 12: loss improved from 2425918.50000 to 2314266.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2314266.5000\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2230242.2500\n",
      "Epoch 13: loss improved from 2314266.50000 to 2230752.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2230752.0000\n",
      "Epoch 14/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2158032.5000\n",
      "Epoch 14: loss improved from 2230752.00000 to 2157928.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2157928.0000\n",
      "Epoch 15/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2103704.0000\n",
      "Epoch 15: loss improved from 2157928.00000 to 2103756.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2103756.7500\n",
      "Epoch 16/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2054287.6250\n",
      "Epoch 16: loss improved from 2103756.75000 to 2054096.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2054096.8750\n",
      "Epoch 17/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2012018.1250\n",
      "Epoch 17: loss improved from 2054096.87500 to 2011955.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2011955.1250\n",
      "Epoch 18/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1974892.2500\n",
      "Epoch 18: loss improved from 2011955.12500 to 1975849.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 1975849.6250\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1945165.3750\n",
      "Epoch 19: loss improved from 1975849.62500 to 1944839.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 1944839.0000\n",
      "Epoch 20/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1916722.2500\n",
      "Epoch 20: loss improved from 1944839.00000 to 1917358.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1917358.1250\n",
      "Epoch 21/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1890127.7500\n",
      "Epoch 21: loss improved from 1917358.12500 to 1890463.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1890463.5000\n",
      "Epoch 22/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1864895.6250\n",
      "Epoch 22: loss improved from 1890463.50000 to 1865216.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 1865216.8750\n",
      "Epoch 23/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1842959.0000\n",
      "Epoch 23: loss improved from 1865216.87500 to 1843672.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1843672.1250\n",
      "Epoch 24/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1829169.2500\n",
      "Epoch 24: loss improved from 1843672.12500 to 1828230.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1828230.6250\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1805806.1250\n",
      "Epoch 25: loss improved from 1828230.62500 to 1806885.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1806885.0000\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1794375.6250\n",
      "Epoch 26: loss improved from 1806885.00000 to 1794375.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1794375.6250\n",
      "Epoch 27/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1775834.0000\n",
      "Epoch 27: loss improved from 1794375.62500 to 1776014.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1776014.5000\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1761623.6250\n",
      "Epoch 28: loss improved from 1776014.50000 to 1761623.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1761623.6250\n",
      "Epoch 29/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1748271.7500\n",
      "Epoch 29: loss improved from 1761623.62500 to 1748245.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1748245.3750\n",
      "Epoch 30/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1734180.8750\n",
      "Epoch 30: loss improved from 1748245.37500 to 1734419.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1734419.3750\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1725191.6250\n",
      "Epoch 31: loss improved from 1734419.37500 to 1725614.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1725614.0000\n",
      "Epoch 32/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1715219.6250\n",
      "Epoch 32: loss improved from 1725614.00000 to 1715040.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1715040.1250\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1703587.2500\n",
      "Epoch 33: loss improved from 1715040.12500 to 1703587.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1703587.2500\n",
      "Epoch 34/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1692979.1250\n",
      "Epoch 34: loss improved from 1703587.25000 to 1693043.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1693043.3750\n",
      "Epoch 35/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1685125.7500\n",
      "Epoch 35: loss improved from 1693043.37500 to 1685753.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1685753.0000\n",
      "Epoch 36/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1673600.1250\n",
      "Epoch 36: loss improved from 1685753.00000 to 1673367.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1673367.6250\n",
      "Epoch 37/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1665712.1250\n",
      "Epoch 37: loss improved from 1673367.62500 to 1666522.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1666522.3750\n",
      "Epoch 38/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1658402.1250\n",
      "Epoch 38: loss improved from 1666522.37500 to 1658514.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1658514.7500\n",
      "Epoch 39/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1651478.7500\n",
      "Epoch 39: loss improved from 1658514.75000 to 1651187.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1651187.0000\n",
      "Epoch 40/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1643982.3750\n",
      "Epoch 40: loss improved from 1651187.00000 to 1642858.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1642858.1250\n",
      "Epoch 41/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1637078.5000\n",
      "Epoch 41: loss improved from 1642858.12500 to 1636400.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1636400.3750\n",
      "Epoch 42/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1631546.1250\n",
      "Epoch 42: loss improved from 1636400.37500 to 1631824.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1631824.1250\n",
      "Epoch 43/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1626029.7500\n",
      "Epoch 43: loss improved from 1631824.12500 to 1625591.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1625591.7500\n",
      "Epoch 44/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1617191.3750\n",
      "Epoch 44: loss improved from 1625591.75000 to 1616923.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1616923.8750\n",
      "Epoch 45/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1611799.7500\n",
      "Epoch 45: loss improved from 1616923.87500 to 1611654.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1611654.3750\n",
      "Epoch 46/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1606607.8750\n",
      "Epoch 46: loss improved from 1611654.37500 to 1606062.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1606062.0000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1600622.0000\n",
      "Epoch 47: loss improved from 1606062.00000 to 1600622.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1600622.0000\n",
      "Epoch 48/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1598586.7500\n",
      "Epoch 48: loss improved from 1600622.00000 to 1597788.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1597788.2500\n",
      "Epoch 49/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1591242.3750\n",
      "Epoch 49: loss improved from 1597788.25000 to 1591440.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1591440.7500\n",
      "Epoch 50/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1587485.7500\n",
      "Epoch 50: loss improved from 1591440.75000 to 1587485.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1587485.7500\n",
      "Epoch 51/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1580974.2500\n",
      "Epoch 51: loss improved from 1587485.75000 to 1580570.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1580570.0000\n",
      "Epoch 52/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1576367.0000\n",
      "Epoch 52: loss improved from 1580570.00000 to 1576367.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1576367.0000\n",
      "Epoch 53/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1572101.6250\n",
      "Epoch 53: loss improved from 1576367.00000 to 1572326.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1572326.1250\n",
      "Epoch 54/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1564625.5000\n",
      "Epoch 54: loss improved from 1572326.12500 to 1564658.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1564658.2500\n",
      "Epoch 55/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1559151.8750\n",
      "Epoch 55: loss improved from 1564658.25000 to 1559404.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1559404.6250\n",
      "Epoch 56/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1555914.5000\n",
      "Epoch 56: loss improved from 1559404.62500 to 1555913.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1555913.8750\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1555625.5000\n",
      "Epoch 57: loss improved from 1555913.87500 to 1555351.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1555351.6250\n",
      "Epoch 58/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1552623.6250\n",
      "Epoch 58: loss improved from 1555351.62500 to 1552530.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1552530.2500\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1546322.0000\n",
      "Epoch 59: loss improved from 1552530.25000 to 1546095.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1546095.2500\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1542712.0000\n",
      "Epoch 60: loss improved from 1546095.25000 to 1542712.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1542712.0000\n",
      "Epoch 61/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1540378.8750\n",
      "Epoch 61: loss improved from 1542712.00000 to 1540301.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1540301.5000\n",
      "Epoch 62/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1535703.1250\n",
      "Epoch 62: loss improved from 1540301.50000 to 1535785.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1535785.5000\n",
      "Epoch 63/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1529692.2500\n",
      "Epoch 63: loss improved from 1535785.50000 to 1530005.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1530005.7500\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1530826.1250\n",
      "Epoch 64: loss did not improve from 1530005.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1530588.1250\n",
      "Epoch 65/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1525980.2500\n",
      "Epoch 65: loss improved from 1530005.75000 to 1525994.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1525994.8750\n",
      "Epoch 66/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1523983.5000\n",
      "Epoch 66: loss improved from 1525994.87500 to 1523359.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1523359.8750\n",
      "Epoch 67/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1517808.6250\n",
      "Epoch 67: loss improved from 1523359.87500 to 1518450.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1518450.1250\n",
      "Epoch 68/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1517530.2500\n",
      "Epoch 68: loss improved from 1518450.12500 to 1516932.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1516932.8750\n",
      "Epoch 69/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1513301.2500\n",
      "Epoch 69: loss improved from 1516932.87500 to 1512569.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1512569.8750\n",
      "Epoch 70/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1511622.3750\n",
      "Epoch 70: loss improved from 1512569.87500 to 1510849.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1510849.3750\n",
      "Epoch 71/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1505586.5000\n",
      "Epoch 71: loss improved from 1510849.37500 to 1505605.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1505605.7500\n",
      "Epoch 72/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1502768.1250\n",
      "Epoch 72: loss improved from 1505605.75000 to 1502960.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1502960.5000\n",
      "Epoch 73/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1498809.3750\n",
      "Epoch 73: loss improved from 1502960.50000 to 1500112.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1500112.7500\n",
      "Epoch 74/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1500857.8750\n",
      "Epoch 74: loss did not improve from 1500112.75000\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1500185.0000\n",
      "Epoch 75/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1497160.1250\n",
      "Epoch 75: loss improved from 1500112.75000 to 1496521.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1496521.2500\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1492960.0000\n",
      "Epoch 76: loss improved from 1496521.25000 to 1492960.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1492960.0000\n",
      "Epoch 77/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1492493.7500\n",
      "Epoch 77: loss improved from 1492960.00000 to 1492463.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1492463.1250\n",
      "Epoch 78/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1488577.8750\n",
      "Epoch 78: loss improved from 1492463.12500 to 1488188.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1488188.8750\n",
      "Epoch 79/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1488041.3750\n",
      "Epoch 79: loss improved from 1488188.87500 to 1488013.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1488013.0000\n",
      "Epoch 80/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1482831.6250\n",
      "Epoch 80: loss improved from 1488013.00000 to 1482115.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1482115.0000\n",
      "Epoch 81/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1480339.0000\n",
      "Epoch 81: loss improved from 1482115.00000 to 1481972.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1481972.7500\n",
      "Epoch 82/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1478803.6250\n",
      "Epoch 82: loss improved from 1481972.75000 to 1478818.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1478818.0000\n",
      "Epoch 83/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1476501.8750\n",
      "Epoch 83: loss improved from 1478818.00000 to 1475697.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1475697.3750\n",
      "Epoch 84/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1475082.3750\n",
      "Epoch 84: loss improved from 1475697.37500 to 1475082.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1475082.3750\n",
      "Epoch 85/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1475018.0000\n",
      "Epoch 85: loss improved from 1475082.37500 to 1474342.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1474342.8750\n",
      "Epoch 86/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1473714.7500\n",
      "Epoch 86: loss improved from 1474342.87500 to 1473314.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1473314.7500\n",
      "Epoch 87/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1472092.1250\n",
      "Epoch 87: loss improved from 1473314.75000 to 1471350.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1471350.8750\n",
      "Epoch 88/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1469596.2500\n",
      "Epoch 88: loss improved from 1471350.87500 to 1468652.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1468652.2500\n",
      "Epoch 89/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1466246.0000\n",
      "Epoch 89: loss improved from 1468652.25000 to 1465703.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1465703.2500\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1460082.1250\n",
      "Epoch 90: loss improved from 1465703.25000 to 1460082.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1460082.1250\n",
      "Epoch 91/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1460518.2500\n",
      "Epoch 91: loss did not improve from 1460082.12500\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1460942.5000\n",
      "Epoch 92/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1459319.7500\n",
      "Epoch 92: loss improved from 1460082.12500 to 1459054.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1459054.3750\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1458140.3750\n",
      "Epoch 93: loss improved from 1459054.37500 to 1458140.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1458140.3750\n",
      "Epoch 94/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1455779.3750\n",
      "Epoch 94: loss improved from 1458140.37500 to 1455769.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1455769.3750\n",
      "Epoch 95/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1455885.8750\n",
      "Epoch 95: loss did not improve from 1455769.37500\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1455771.8750\n",
      "Epoch 96/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1450295.8750\n",
      "Epoch 96: loss improved from 1455769.37500 to 1450664.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1450664.3750\n",
      "Epoch 97/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1449873.2500\n",
      "Epoch 97: loss improved from 1450664.37500 to 1450626.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1450626.6250\n",
      "Epoch 98/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1448585.2500\n",
      "Epoch 98: loss improved from 1450626.62500 to 1448553.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1448553.5000\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1446657.6250\n",
      "Epoch 99: loss improved from 1448553.50000 to 1446657.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1446657.6250\n",
      "Epoch 100/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1447092.1250\n",
      "Epoch 100: loss did not improve from 1446657.62500\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1447092.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 02:59:19 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 02:59:19 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpb_92g71b\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpb_92g71b\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_6 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_48 (Conv2D)          (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 25, 25, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 25, 25, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 6, 6, 32)          18464     \n",
      "                                                                 \n",
      " up_sampling2d_18 (UpSamplin  (None, 12, 12, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_52 (Conv2D)          (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " up_sampling2d_19 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_20 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,132,005\n",
      "Trainable params: 6,132,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 2:32 - loss: 1759726.6250  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0200s vs `on_train_batch_end` time: 0.0257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0200s vs `on_train_batch_end` time: 0.0257s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737587.2500\n",
      "Epoch 1: loss improved from inf to 2736502.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 34s 9ms/step - loss: 2736502.5000\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 2: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736504.5000\n",
      "Epoch 3/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2737407.2500\n",
      "Epoch 3: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736503.0000\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 4: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736506.0000\n",
      "Epoch 5/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2736430.2500\n",
      "Epoch 5: loss did not improve from 2736502.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736503.0000\n",
      "Epoch 6/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736722.2500\n",
      "Epoch 6: loss improved from 2736502.50000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736501.5000\n",
      "Epoch 7/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736917.0000\n",
      "Epoch 7: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736511.0000\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2736972.7500\n",
      "Epoch 8: loss improved from 2736501.50000 to 2736490.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736490.2500\n",
      "Epoch 9/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737230.7500\n",
      "Epoch 9: loss improved from 2736490.25000 to 2736465.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736465.0000\n",
      "Epoch 10/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2735948.0000\n",
      "Epoch 10: loss improved from 2736465.00000 to 2736157.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2736157.5000\n",
      "Epoch 11/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2667340.0000\n",
      "Epoch 11: loss improved from 2736157.50000 to 2666695.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2666695.7500\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2412635.2500\n",
      "Epoch 12: loss improved from 2666695.75000 to 2412635.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2412635.2500\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2186094.5000\n",
      "Epoch 13: loss improved from 2412635.25000 to 2185657.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2185657.7500\n",
      "Epoch 14/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2046262.7500\n",
      "Epoch 14: loss improved from 2185657.75000 to 2046137.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 2046137.8750\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1934974.3750\n",
      "Epoch 15: loss improved from 2046137.87500 to 1934852.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1934852.8750\n",
      "Epoch 16/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1839960.5000\n",
      "Epoch 16: loss improved from 1934852.87500 to 1839960.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1839960.5000\n",
      "Epoch 17/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1764416.6250\n",
      "Epoch 17: loss improved from 1839960.50000 to 1764455.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1764455.0000\n",
      "Epoch 18/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1697337.3750\n",
      "Epoch 18: loss improved from 1764455.00000 to 1697020.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1697020.5000\n",
      "Epoch 19/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1651575.2500\n",
      "Epoch 19: loss improved from 1697020.50000 to 1651536.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1651536.7500\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1603852.5000\n",
      "Epoch 20: loss improved from 1651536.75000 to 1603170.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1603170.0000\n",
      "Epoch 21/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1567191.2500\n",
      "Epoch 21: loss improved from 1603170.00000 to 1566833.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1566833.1250\n",
      "Epoch 22/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1530925.8750\n",
      "Epoch 22: loss improved from 1566833.12500 to 1531247.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1531247.2500\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1504048.1250\n",
      "Epoch 23: loss improved from 1531247.25000 to 1503683.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1503683.3750\n",
      "Epoch 24/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1475016.6250\n",
      "Epoch 24: loss improved from 1503683.37500 to 1475301.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1475301.6250\n",
      "Epoch 25/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1445265.8750\n",
      "Epoch 25: loss improved from 1475301.62500 to 1444888.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1444888.7500\n",
      "Epoch 26/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1424856.3750\n",
      "Epoch 26: loss improved from 1444888.75000 to 1424681.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1424681.6250\n",
      "Epoch 27/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1404147.0000\n",
      "Epoch 27: loss improved from 1424681.62500 to 1405311.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1405311.6250\n",
      "Epoch 28/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1386553.8750\n",
      "Epoch 28: loss improved from 1405311.62500 to 1385991.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1385991.3750\n",
      "Epoch 29/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1363698.1250\n",
      "Epoch 29: loss improved from 1385991.37500 to 1363883.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1363883.7500\n",
      "Epoch 30/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1354457.2500\n",
      "Epoch 30: loss improved from 1363883.75000 to 1354457.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1354457.2500\n",
      "Epoch 31/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1342219.3750\n",
      "Epoch 31: loss improved from 1354457.25000 to 1342349.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 1342349.7500\n",
      "Epoch 32/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1325160.0000\n",
      "Epoch 32: loss improved from 1342349.75000 to 1325148.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1325148.6250\n",
      "Epoch 33/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1315740.7500\n",
      "Epoch 33: loss improved from 1325148.62500 to 1316114.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1316114.2500\n",
      "Epoch 34/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1303959.6250\n",
      "Epoch 34: loss improved from 1316114.25000 to 1303264.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1303264.8750\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1291568.0000\n",
      "Epoch 35: loss improved from 1303264.87500 to 1291601.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1291601.5000\n",
      "Epoch 36/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1283220.7500\n",
      "Epoch 36: loss improved from 1291601.50000 to 1283009.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1283009.5000\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1274087.6250\n",
      "Epoch 37: loss improved from 1283009.50000 to 1274572.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1274572.5000\n",
      "Epoch 38/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1264495.6250\n",
      "Epoch 38: loss improved from 1274572.50000 to 1264732.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1264732.2500\n",
      "Epoch 39/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1255409.2500\n",
      "Epoch 39: loss improved from 1264732.25000 to 1255021.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1255021.8750\n",
      "Epoch 40/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1245696.0000\n",
      "Epoch 40: loss improved from 1255021.87500 to 1245782.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1245782.6250\n",
      "Epoch 41/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1236250.5000\n",
      "Epoch 41: loss improved from 1245782.62500 to 1235870.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1235870.8750\n",
      "Epoch 42/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1230480.6250\n",
      "Epoch 42: loss improved from 1235870.87500 to 1229958.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1229958.8750\n",
      "Epoch 43/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1227169.1250\n",
      "Epoch 43: loss improved from 1229958.87500 to 1227203.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1227203.8750\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1216095.8750\n",
      "Epoch 44: loss improved from 1227203.87500 to 1215798.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1215798.8750\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1209225.2500\n",
      "Epoch 45: loss improved from 1215798.87500 to 1208458.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1208458.8750\n",
      "Epoch 46/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1204873.0000\n",
      "Epoch 46: loss improved from 1208458.87500 to 1204918.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1204918.1250\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1198221.3750\n",
      "Epoch 47: loss improved from 1204918.12500 to 1199046.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1199046.3750\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1188430.1250\n",
      "Epoch 48: loss improved from 1199046.37500 to 1188206.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1188206.3750\n",
      "Epoch 49/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1186744.3750\n",
      "Epoch 49: loss improved from 1188206.37500 to 1186974.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1186974.1250\n",
      "Epoch 50/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1176911.8750\n",
      "Epoch 50: loss improved from 1186974.12500 to 1177166.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1177166.7500\n",
      "Epoch 51/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1173856.2500\n",
      "Epoch 51: loss improved from 1177166.75000 to 1173553.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1173553.2500\n",
      "Epoch 52/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1170965.0000\n",
      "Epoch 52: loss improved from 1173553.25000 to 1170943.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1170943.8750\n",
      "Epoch 53/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1162135.7500\n",
      "Epoch 53: loss improved from 1170943.87500 to 1161970.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1161970.1250\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1159902.5000\n",
      "Epoch 54: loss improved from 1161970.12500 to 1159467.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1159467.2500\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1156101.0000\n",
      "Epoch 55: loss improved from 1159467.25000 to 1155836.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1155836.5000\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1151116.0000\n",
      "Epoch 56: loss improved from 1155836.50000 to 1151213.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1151213.2500\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1146445.6250\n",
      "Epoch 57: loss improved from 1151213.25000 to 1146445.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1146445.6250\n",
      "Epoch 58/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1140343.5000\n",
      "Epoch 58: loss improved from 1146445.62500 to 1140853.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1140853.8750\n",
      "Epoch 59/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1139425.6250\n",
      "Epoch 59: loss improved from 1140853.87500 to 1139223.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1139223.1250\n",
      "Epoch 60/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1133151.2500\n",
      "Epoch 60: loss improved from 1139223.12500 to 1132825.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1132825.3750\n",
      "Epoch 61/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1130762.1250\n",
      "Epoch 61: loss improved from 1132825.37500 to 1130554.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1130554.6250\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1125948.1250\n",
      "Epoch 62: loss improved from 1130554.62500 to 1125948.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1125948.1250\n",
      "Epoch 63/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1122156.3750\n",
      "Epoch 63: loss improved from 1125948.12500 to 1122139.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1122139.1250\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1116826.1250\n",
      "Epoch 64: loss improved from 1122139.12500 to 1116694.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1116694.7500\n",
      "Epoch 65/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1121636.7500\n",
      "Epoch 65: loss did not improve from 1116694.75000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1121647.1250\n",
      "Epoch 66/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1108597.7500\n",
      "Epoch 66: loss improved from 1116694.75000 to 1108520.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1108520.5000\n",
      "Epoch 67/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1110740.5000\n",
      "Epoch 67: loss did not improve from 1108520.50000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1110534.1250\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1102590.0000\n",
      "Epoch 68: loss improved from 1108520.50000 to 1102519.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1102519.0000\n",
      "Epoch 69/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1103417.6250\n",
      "Epoch 69: loss did not improve from 1102519.00000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1103227.3750\n",
      "Epoch 70/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1102250.8750\n",
      "Epoch 70: loss improved from 1102519.00000 to 1101904.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1101904.5000\n",
      "Epoch 71/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1101039.0000\n",
      "Epoch 71: loss improved from 1101904.50000 to 1100642.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1100642.7500\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1093749.6250\n",
      "Epoch 72: loss improved from 1100642.75000 to 1093749.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1093749.6250\n",
      "Epoch 73/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1090338.8750\n",
      "Epoch 73: loss improved from 1093749.62500 to 1093145.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1093145.6250\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1090763.8750\n",
      "Epoch 74: loss improved from 1093145.62500 to 1090754.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1090754.8750\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1084689.8750\n",
      "Epoch 75: loss improved from 1090754.87500 to 1084732.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1084732.0000\n",
      "Epoch 76/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1081160.7500\n",
      "Epoch 76: loss improved from 1084732.00000 to 1081170.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1081170.6250\n",
      "Epoch 77/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1080737.0000\n",
      "Epoch 77: loss improved from 1081170.62500 to 1080595.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1080595.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1078029.6250\n",
      "Epoch 78: loss improved from 1080595.50000 to 1078117.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 1078117.7500\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1075496.8750\n",
      "Epoch 79: loss improved from 1078117.75000 to 1075863.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1075863.2500\n",
      "Epoch 80/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1070754.8750\n",
      "Epoch 80: loss improved from 1075863.25000 to 1070754.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1070754.8750\n",
      "Epoch 81/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1068430.2500\n",
      "Epoch 81: loss improved from 1070754.87500 to 1067887.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 1067887.6250\n",
      "Epoch 82/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1072490.8750\n",
      "Epoch 82: loss did not improve from 1067887.62500\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1072693.1250\n",
      "Epoch 83/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1066806.5000\n",
      "Epoch 83: loss improved from 1067887.62500 to 1066529.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1066529.6250\n",
      "Epoch 84/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1064955.5000\n",
      "Epoch 84: loss improved from 1066529.62500 to 1064955.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1064955.5000\n",
      "Epoch 85/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1060883.8750\n",
      "Epoch 85: loss improved from 1064955.50000 to 1060435.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1060435.0000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1064520.3750\n",
      "Epoch 86: loss did not improve from 1060435.00000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1064520.3750\n",
      "Epoch 87/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1056904.7500\n",
      "Epoch 87: loss improved from 1060435.00000 to 1056673.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 31s 10ms/step - loss: 1056673.3750\n",
      "Epoch 88/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1056260.6250\n",
      "Epoch 88: loss improved from 1056673.37500 to 1056184.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1056184.1250\n",
      "Epoch 89/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1054712.6250\n",
      "Epoch 89: loss improved from 1056184.12500 to 1054712.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1054712.6250\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1051810.0000\n",
      "Epoch 90: loss improved from 1054712.62500 to 1051810.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1051810.0000\n",
      "Epoch 91/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1051361.2500\n",
      "Epoch 91: loss did not improve from 1051810.00000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1051956.6250\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1049212.8750\n",
      "Epoch 92: loss improved from 1051810.00000 to 1049131.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1049131.7500\n",
      "Epoch 93/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1044642.7500\n",
      "Epoch 93: loss improved from 1049131.75000 to 1044709.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1044709.0000\n",
      "Epoch 94/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1047303.5000\n",
      "Epoch 94: loss did not improve from 1044709.00000\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1047125.0625\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1043940.8750\n",
      "Epoch 95: loss improved from 1044709.00000 to 1043940.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1043940.8750\n",
      "Epoch 96/100\n",
      "2863/3189 [=========================>....] - ETA: 2s - loss: 1042831.1250"
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=3\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\"vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 06:54:01 INFO mlflow.tracking.fluent: Experiment with name 'dnn3vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_111 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 2:01:38 - loss: 3577546.7500WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0381s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0381s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3189 [============================>.] - ETA: 0s - loss: 2687807.5000\n",
      "Epoch 1: loss improved from inf to 2687672.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 4ms/step - loss: 2687672.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2596229.2500\n",
      "Epoch 2: loss improved from 2687672.50000 to 2596205.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2596205.0000\n",
      "Epoch 3/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2538013.5000\n",
      "Epoch 3: loss improved from 2596205.00000 to 2537736.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2537736.7500\n",
      "Epoch 4/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2492994.5000\n",
      "Epoch 4: loss improved from 2537736.75000 to 2492932.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2492932.7500\n",
      "Epoch 5/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2452536.5000\n",
      "Epoch 5: loss improved from 2492932.75000 to 2452682.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2452682.2500\n",
      "Epoch 6/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2417745.0000\n",
      "Epoch 6: loss improved from 2452682.25000 to 2417568.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2417568.2500\n",
      "Epoch 7/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2388616.7500\n",
      "Epoch 7: loss improved from 2417568.25000 to 2388050.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2388050.7500\n",
      "Epoch 8/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2362039.5000\n",
      "Epoch 8: loss improved from 2388050.75000 to 2361705.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2361705.7500\n",
      "Epoch 9/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2336990.2500\n",
      "Epoch 9: loss improved from 2361705.75000 to 2338327.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2338327.7500\n",
      "Epoch 10/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2315327.5000\n",
      "Epoch 10: loss improved from 2338327.75000 to 2315918.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2315918.0000\n",
      "Epoch 11/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2296146.2500\n",
      "Epoch 11: loss improved from 2315918.00000 to 2295780.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2295780.0000\n",
      "Epoch 12/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2276998.5000\n",
      "Epoch 12: loss improved from 2295780.00000 to 2277084.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2277084.0000\n",
      "Epoch 13/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2258753.0000\n",
      "Epoch 13: loss improved from 2277084.00000 to 2258755.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2258755.2500\n",
      "Epoch 14/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2242001.0000\n",
      "Epoch 14: loss improved from 2258755.25000 to 2242001.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2242001.0000\n",
      "Epoch 15/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2226043.2500\n",
      "Epoch 15: loss improved from 2242001.00000 to 2225932.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2225932.2500\n",
      "Epoch 16/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2211845.5000\n",
      "Epoch 16: loss improved from 2225932.25000 to 2211667.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2211667.2500\n",
      "Epoch 17/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2197039.0000\n",
      "Epoch 17: loss improved from 2211667.25000 to 2197138.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2197138.7500\n",
      "Epoch 18/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2185156.2500\n",
      "Epoch 18: loss improved from 2197138.75000 to 2185959.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2185959.0000\n",
      "Epoch 19/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2173387.2500\n",
      "Epoch 19: loss improved from 2185959.00000 to 2173607.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2173607.2500\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2162170.0000\n",
      "Epoch 20: loss improved from 2173607.25000 to 2161214.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2161214.7500\n",
      "Epoch 21/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2152593.5000\n",
      "Epoch 21: loss improved from 2161214.75000 to 2153117.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2153117.2500\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2140848.0000\n",
      "Epoch 22: loss improved from 2153117.25000 to 2141345.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2141345.2500\n",
      "Epoch 23/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2132788.2500\n",
      "Epoch 23: loss improved from 2141345.25000 to 2132183.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2132183.5000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2122215.0000\n",
      "Epoch 24: loss improved from 2132183.50000 to 2123399.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2123399.2500\n",
      "Epoch 25/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2117789.2500\n",
      "Epoch 25: loss improved from 2123399.25000 to 2117526.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2117526.0000\n",
      "Epoch 26/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2107234.5000\n",
      "Epoch 26: loss improved from 2117526.00000 to 2108733.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2108733.2500\n",
      "Epoch 27/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2101747.2500\n",
      "Epoch 27: loss improved from 2108733.25000 to 2101903.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2101903.5000\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2096543.8750\n",
      "Epoch 28: loss improved from 2101903.50000 to 2096543.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2096543.8750\n",
      "Epoch 29/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2086963.5000\n",
      "Epoch 29: loss improved from 2096543.87500 to 2089090.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2089090.8750\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2083670.1250\n",
      "Epoch 30: loss improved from 2089090.87500 to 2083478.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2083478.3750\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2077010.7500\n",
      "Epoch 31: loss improved from 2083478.37500 to 2077516.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2077516.2500\n",
      "Epoch 32/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2069330.3750\n",
      "Epoch 32: loss improved from 2077516.25000 to 2070180.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2070180.1250\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2065142.2500\n",
      "Epoch 33: loss improved from 2070180.12500 to 2065142.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2065142.2500\n",
      "Epoch 34/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2060730.6250\n",
      "Epoch 34: loss improved from 2065142.25000 to 2060604.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2060604.6250\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2055208.3750\n",
      "Epoch 35: loss improved from 2060604.62500 to 2055208.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2055208.3750\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2050071.5000\n",
      "Epoch 36: loss improved from 2055208.37500 to 2049879.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2049879.7500\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2045495.5000\n",
      "Epoch 37: loss improved from 2049879.75000 to 2046359.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2046359.1250\n",
      "Epoch 38/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2040377.6250\n",
      "Epoch 38: loss improved from 2046359.12500 to 2040648.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2040648.0000\n",
      "Epoch 39/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2036224.3750\n",
      "Epoch 39: loss improved from 2040648.00000 to 2036267.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2036267.1250\n",
      "Epoch 40/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2030791.0000\n",
      "Epoch 40: loss improved from 2036267.12500 to 2031258.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2031258.7500\n",
      "Epoch 41/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2026679.2500\n",
      "Epoch 41: loss improved from 2031258.75000 to 2026790.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2026790.8750\n",
      "Epoch 42/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2022820.5000\n",
      "Epoch 42: loss improved from 2026790.87500 to 2023465.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2023465.1250\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2018969.6250\n",
      "Epoch 43: loss improved from 2023465.12500 to 2019996.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2019996.0000\n",
      "Epoch 44/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2017003.1250\n",
      "Epoch 44: loss improved from 2019996.00000 to 2016786.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2016786.1250\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2013293.7500\n",
      "Epoch 45: loss improved from 2016786.12500 to 2013293.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2013293.7500\n",
      "Epoch 46/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2009508.0000\n",
      "Epoch 46: loss improved from 2013293.75000 to 2009419.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2009419.5000\n",
      "Epoch 47/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2007460.8750\n",
      "Epoch 47: loss improved from 2009419.50000 to 2006738.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2006738.0000\n",
      "Epoch 48/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2003283.3750\n",
      "Epoch 48: loss improved from 2006738.00000 to 2003924.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2003924.6250\n",
      "Epoch 49/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2000731.6250\n",
      "Epoch 49: loss improved from 2003924.62500 to 2000765.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2000765.0000\n",
      "Epoch 50/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1996815.5000\n",
      "Epoch 50: loss improved from 2000765.00000 to 1997808.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1997808.5000\n",
      "Epoch 51/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1992944.5000\n",
      "Epoch 51: loss improved from 1997808.50000 to 1993104.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1993104.5000\n",
      "Epoch 52/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1991359.6250\n",
      "Epoch 52: loss improved from 1993104.50000 to 1991190.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1991190.7500\n",
      "Epoch 53/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1989336.0000\n",
      "Epoch 53: loss improved from 1991190.75000 to 1987855.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1987855.1250\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1986433.5000\n",
      "Epoch 54: loss improved from 1987855.12500 to 1986630.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1986630.1250\n",
      "Epoch 55/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1981570.3750\n",
      "Epoch 55: loss improved from 1986630.12500 to 1981611.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1981611.0000\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1978213.5000\n",
      "Epoch 56: loss improved from 1981611.00000 to 1978434.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1978434.3750\n",
      "Epoch 57/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1977635.8750\n",
      "Epoch 57: loss improved from 1978434.37500 to 1978246.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1978246.5000\n",
      "Epoch 58/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1974048.8750\n",
      "Epoch 58: loss improved from 1978246.50000 to 1973310.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1973310.3750\n",
      "Epoch 59/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1972422.0000\n",
      "Epoch 59: loss improved from 1973310.37500 to 1972218.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1972218.2500\n",
      "Epoch 60/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1968857.5000\n",
      "Epoch 60: loss improved from 1972218.25000 to 1968803.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1968803.8750\n",
      "Epoch 61/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1966651.2500\n",
      "Epoch 61: loss improved from 1968803.87500 to 1966065.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1966065.7500\n",
      "Epoch 62/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1965104.7500\n",
      "Epoch 62: loss improved from 1966065.75000 to 1965393.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1965393.5000\n",
      "Epoch 63/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1962917.2500\n",
      "Epoch 63: loss improved from 1965393.50000 to 1962917.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1962917.2500\n",
      "Epoch 64/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1959908.0000\n",
      "Epoch 64: loss improved from 1962917.25000 to 1959630.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1959630.2500\n",
      "Epoch 65/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1959298.1250\n",
      "Epoch 65: loss improved from 1959630.25000 to 1958594.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1958594.3750\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1955499.5000\n",
      "Epoch 66: loss improved from 1958594.37500 to 1955499.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 1955499.5000\n",
      "Epoch 67/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1951248.0000\n",
      "Epoch 67: loss improved from 1955499.50000 to 1952766.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1952766.7500\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1952107.0000\n",
      "Epoch 68: loss improved from 1952766.75000 to 1951630.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1951630.3750\n",
      "Epoch 69/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1948620.3750\n",
      "Epoch 69: loss improved from 1951630.37500 to 1948855.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1948855.1250\n",
      "Epoch 70/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1946400.6250\n",
      "Epoch 70: loss improved from 1948855.12500 to 1947273.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1947273.0000\n",
      "Epoch 71/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1944955.1250\n",
      "Epoch 71: loss improved from 1947273.00000 to 1945437.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1945437.1250\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1942304.1250\n",
      "Epoch 72: loss improved from 1945437.12500 to 1942741.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1942741.5000\n",
      "Epoch 73/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1941499.5000\n",
      "Epoch 73: loss improved from 1942741.50000 to 1940844.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1940844.3750\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1937034.2500\n",
      "Epoch 74: loss improved from 1940844.37500 to 1937034.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1937034.2500\n",
      "Epoch 75/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1938589.0000\n",
      "Epoch 75: loss did not improve from 1937034.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1938348.7500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1936139.5000\n",
      "Epoch 76: loss improved from 1937034.25000 to 1935134.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1935134.7500\n",
      "Epoch 77/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1936239.0000\n",
      "Epoch 77: loss improved from 1935134.75000 to 1934523.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1934523.6250\n",
      "Epoch 78/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1932567.0000\n",
      "Epoch 78: loss improved from 1934523.62500 to 1930983.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1930983.1250\n",
      "Epoch 79/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1929309.6250\n",
      "Epoch 79: loss improved from 1930983.12500 to 1928756.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1928756.0000\n",
      "Epoch 80/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1929085.8750\n",
      "Epoch 80: loss did not improve from 1928756.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1928936.7500\n",
      "Epoch 81/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1926346.1250\n",
      "Epoch 81: loss improved from 1928756.00000 to 1926478.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1926478.1250\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1925085.5000\n",
      "Epoch 82: loss improved from 1926478.12500 to 1924823.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1924823.0000\n",
      "Epoch 83/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1922673.0000\n",
      "Epoch 83: loss improved from 1924823.00000 to 1922730.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1922730.7500\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1921605.8750\n",
      "Epoch 84: loss improved from 1922730.75000 to 1921263.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1921263.6250\n",
      "Epoch 85/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1920814.5000\n",
      "Epoch 85: loss improved from 1921263.62500 to 1920375.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1920375.0000\n",
      "Epoch 86/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1918135.5000\n",
      "Epoch 86: loss improved from 1920375.00000 to 1917819.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1917819.0000\n",
      "Epoch 87/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1916905.2500\n",
      "Epoch 87: loss improved from 1917819.00000 to 1917087.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1917087.5000\n",
      "Epoch 88/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1914907.8750\n",
      "Epoch 88: loss improved from 1917087.50000 to 1914451.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1914451.6250\n",
      "Epoch 89/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1914248.8750\n",
      "Epoch 89: loss did not improve from 1914451.62500\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1915215.6250\n",
      "Epoch 90/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1909997.5000\n",
      "Epoch 90: loss improved from 1914451.62500 to 1912434.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1912434.1250\n",
      "Epoch 91/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1911034.7500\n",
      "Epoch 91: loss improved from 1912434.12500 to 1909486.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1909486.5000\n",
      "Epoch 92/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1907229.8750\n",
      "Epoch 92: loss improved from 1909486.50000 to 1907013.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1907013.8750\n",
      "Epoch 93/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1908672.6250\n",
      "Epoch 93: loss did not improve from 1907013.87500\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1907233.7500\n",
      "Epoch 94/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1907616.3750\n",
      "Epoch 94: loss improved from 1907013.87500 to 1906599.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1906599.8750\n",
      "Epoch 95/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1904303.5000\n",
      "Epoch 95: loss improved from 1906599.87500 to 1903584.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1903584.3750\n",
      "Epoch 96/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1903716.2500\n",
      "Epoch 96: loss improved from 1903584.37500 to 1903318.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1903318.8750\n",
      "Epoch 97/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1903069.0000\n",
      "Epoch 97: loss did not improve from 1903318.87500\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1903446.6250\n",
      "Epoch 98/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1901589.7500\n",
      "Epoch 98: loss improved from 1903318.87500 to 1901067.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1901067.5000\n",
      "Epoch 99/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1900462.8750\n",
      "Epoch 99: loss improved from 1901067.50000 to 1900369.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1900369.1250\n",
      "Epoch 100/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1896863.3750\n",
      "Epoch 100: loss improved from 1900369.12500 to 1898245.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1898245.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 07:12:33 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 07:12:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp8fwp_c1d\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp8fwp_c1d\\model\\data\\model\\assets\n",
      "2023/05/28 07:12:55 INFO mlflow.tracking.fluent: Experiment with name 'dnn3vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_119 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 1:50:19 - loss: 4168649.2500WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0067s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0067s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184/3189 [============================>.] - ETA: 0s - loss: 2731933.5000\n",
      "Epoch 1: loss improved from inf to 2732545.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2732545.0000\n",
      "Epoch 2/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2716018.2500\n",
      "Epoch 2: loss improved from 2732545.00000 to 2716386.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2716386.7500\n",
      "Epoch 3/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2698345.0000\n",
      "Epoch 3: loss improved from 2716386.75000 to 2700120.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2700120.2500\n",
      "Epoch 4/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2687057.0000\n",
      "Epoch 4: loss improved from 2700120.25000 to 2688722.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2688722.5000\n",
      "Epoch 5/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2680513.7500\n",
      "Epoch 5: loss improved from 2688722.50000 to 2678877.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2678877.7500\n",
      "Epoch 6/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2670008.5000\n",
      "Epoch 6: loss improved from 2678877.75000 to 2670238.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2670238.7500\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2662010.5000\n",
      "Epoch 7: loss improved from 2670238.75000 to 2662010.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2662010.5000\n",
      "Epoch 8/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2657186.5000\n",
      "Epoch 8: loss improved from 2662010.50000 to 2655843.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2655843.5000\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2649745.7500\n",
      "Epoch 9: loss improved from 2655843.50000 to 2649460.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2649460.0000\n",
      "Epoch 10/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2642899.5000\n",
      "Epoch 10: loss improved from 2649460.00000 to 2642775.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2642775.0000\n",
      "Epoch 11/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2637387.5000\n",
      "Epoch 11: loss improved from 2642775.00000 to 2636516.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2636516.2500\n",
      "Epoch 12/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2627497.7500\n",
      "Epoch 12: loss improved from 2636516.25000 to 2631773.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2631773.2500\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2626645.7500\n",
      "Epoch 13: loss improved from 2631773.25000 to 2626645.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2626645.7500\n",
      "Epoch 14/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2621888.7500\n",
      "Epoch 14: loss improved from 2626645.75000 to 2621669.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2621669.2500\n",
      "Epoch 15/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2618362.7500\n",
      "Epoch 15: loss improved from 2621669.25000 to 2617985.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2617985.2500\n",
      "Epoch 16/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2611742.2500\n",
      "Epoch 16: loss improved from 2617985.25000 to 2612644.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2612644.0000\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2607311.5000\n",
      "Epoch 17: loss improved from 2612644.00000 to 2607534.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2607534.0000\n",
      "Epoch 18/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2605008.2500\n",
      "Epoch 18: loss improved from 2607534.00000 to 2604568.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2604568.2500\n",
      "Epoch 19/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2598718.0000\n",
      "Epoch 19: loss improved from 2604568.25000 to 2599752.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2599752.2500\n",
      "Epoch 20/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2593756.7500\n",
      "Epoch 20: loss improved from 2599752.25000 to 2595265.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2595265.2500\n",
      "Epoch 21/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2591288.2500\n",
      "Epoch 21: loss improved from 2595265.25000 to 2591173.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2591173.0000\n",
      "Epoch 22/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2587043.0000\n",
      "Epoch 22: loss improved from 2591173.00000 to 2587172.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2587172.0000\n",
      "Epoch 23/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2583164.2500\n",
      "Epoch 23: loss improved from 2587172.00000 to 2583755.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2583755.5000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2581924.2500\n",
      "Epoch 24: loss improved from 2583755.50000 to 2580774.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2580774.5000\n",
      "Epoch 25/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2576533.2500\n",
      "Epoch 25: loss improved from 2580774.50000 to 2576879.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2576879.2500\n",
      "Epoch 26/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2573689.2500\n",
      "Epoch 26: loss improved from 2576879.25000 to 2572837.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2572837.7500\n",
      "Epoch 27/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2571396.0000\n",
      "Epoch 27: loss improved from 2572837.75000 to 2569638.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2569638.5000\n",
      "Epoch 28/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2567429.0000\n",
      "Epoch 28: loss improved from 2569638.50000 to 2567041.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2567041.7500\n",
      "Epoch 29/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2562203.5000\n",
      "Epoch 29: loss improved from 2567041.75000 to 2564331.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2564331.2500\n",
      "Epoch 30/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2561242.2500\n",
      "Epoch 30: loss improved from 2564331.25000 to 2561242.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2561242.2500\n",
      "Epoch 31/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2558278.5000\n",
      "Epoch 31: loss improved from 2561242.25000 to 2557811.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2557811.7500\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2553575.5000\n",
      "Epoch 32: loss improved from 2557811.75000 to 2553575.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2553575.5000\n",
      "Epoch 33/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2549841.5000\n",
      "Epoch 33: loss improved from 2553575.50000 to 2550257.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550257.5000\n",
      "Epoch 34/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2548382.0000\n",
      "Epoch 34: loss improved from 2550257.50000 to 2549394.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2549394.5000\n",
      "Epoch 35/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2546140.5000\n",
      "Epoch 35: loss improved from 2549394.50000 to 2545918.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545918.5000\n",
      "Epoch 36/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2543423.5000\n",
      "Epoch 36: loss improved from 2545918.50000 to 2543207.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543207.2500\n",
      "Epoch 37/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2543452.0000\n",
      "Epoch 37: loss improved from 2543207.25000 to 2541771.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2541771.0000\n",
      "Epoch 38/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2538577.5000\n",
      "Epoch 38: loss improved from 2541771.00000 to 2537626.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2537626.7500\n",
      "Epoch 39/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2535034.0000\n",
      "Epoch 39: loss improved from 2537626.75000 to 2535079.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2535079.5000\n",
      "Epoch 40/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2533638.5000\n",
      "Epoch 40: loss improved from 2535079.50000 to 2533900.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533900.2500\n",
      "Epoch 41/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2529718.0000\n",
      "Epoch 41: loss improved from 2533900.25000 to 2529837.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2529837.7500\n",
      "Epoch 42/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2526275.7500\n",
      "Epoch 42: loss improved from 2529837.75000 to 2526903.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526903.7500\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2525542.2500\n",
      "Epoch 43: loss improved from 2526903.75000 to 2525445.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525445.7500\n",
      "Epoch 44/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2523321.2500\n",
      "Epoch 44: loss improved from 2525445.75000 to 2523139.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2523139.5000\n",
      "Epoch 45/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2522038.7500\n",
      "Epoch 45: loss improved from 2523139.50000 to 2520668.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2520668.7500\n",
      "Epoch 46/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2518455.0000\n",
      "Epoch 46: loss improved from 2520668.75000 to 2518229.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2518229.5000\n",
      "Epoch 47/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2513254.2500\n",
      "Epoch 47: loss improved from 2518229.50000 to 2514857.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2514857.0000\n",
      "Epoch 48/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2512243.0000\n",
      "Epoch 48: loss improved from 2514857.00000 to 2513127.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2513127.5000\n",
      "Epoch 49/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2511902.5000\n",
      "Epoch 49: loss improved from 2513127.50000 to 2511860.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2511860.5000\n",
      "Epoch 50/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2507940.5000\n",
      "Epoch 50: loss improved from 2511860.50000 to 2507950.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2507950.2500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2506169.7500\n",
      "Epoch 51: loss improved from 2507950.25000 to 2506169.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2506169.7500\n",
      "Epoch 52/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2504809.5000\n",
      "Epoch 52: loss improved from 2506169.75000 to 2506124.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2506124.5000\n",
      "Epoch 53/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2503734.2500\n",
      "Epoch 53: loss improved from 2506124.50000 to 2503375.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2503375.2500\n",
      "Epoch 54/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2498308.5000\n",
      "Epoch 54: loss improved from 2503375.25000 to 2499684.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2499684.2500\n",
      "Epoch 55/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2498890.5000\n",
      "Epoch 55: loss improved from 2499684.25000 to 2499097.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2499097.0000\n",
      "Epoch 56/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2496719.0000\n",
      "Epoch 56: loss improved from 2499097.00000 to 2496857.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2496857.2500\n",
      "Epoch 57/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2496748.7500\n",
      "Epoch 57: loss improved from 2496857.25000 to 2494883.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2494883.5000\n",
      "Epoch 58/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2492573.2500\n",
      "Epoch 58: loss improved from 2494883.50000 to 2492658.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2492658.0000\n",
      "Epoch 59/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2490885.0000\n",
      "Epoch 59: loss improved from 2492658.00000 to 2490004.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2490004.7500\n",
      "Epoch 60/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2490742.0000\n",
      "Epoch 60: loss improved from 2490004.75000 to 2489934.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2489934.2500\n",
      "Epoch 61/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2486240.5000\n",
      "Epoch 61: loss improved from 2489934.25000 to 2487566.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2487566.2500\n",
      "Epoch 62/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2488046.2500\n",
      "Epoch 62: loss improved from 2487566.25000 to 2486133.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2486133.0000\n",
      "Epoch 63/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2483601.7500\n",
      "Epoch 63: loss improved from 2486133.00000 to 2482786.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2482786.7500\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2482904.7500\n",
      "Epoch 64: loss did not improve from 2482786.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2483090.2500\n",
      "Epoch 65/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2480779.7500\n",
      "Epoch 65: loss improved from 2482786.75000 to 2480862.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2480862.2500\n",
      "Epoch 66/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2480065.0000\n",
      "Epoch 66: loss improved from 2480862.25000 to 2478372.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2478372.7500\n",
      "Epoch 67/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2476810.0000\n",
      "Epoch 67: loss improved from 2478372.75000 to 2477183.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2477183.0000\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2475000.5000\n",
      "Epoch 68: loss improved from 2477183.00000 to 2475000.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2475000.5000\n",
      "Epoch 69/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2473948.7500\n",
      "Epoch 69: loss improved from 2475000.50000 to 2473940.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2473940.7500\n",
      "Epoch 70/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2471788.2500\n",
      "Epoch 70: loss improved from 2473940.75000 to 2471660.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2471660.7500\n",
      "Epoch 71/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2472036.2500\n",
      "Epoch 71: loss did not improve from 2471660.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2471795.0000\n",
      "Epoch 72/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2468381.2500\n",
      "Epoch 72: loss improved from 2471660.75000 to 2468231.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2468231.5000\n",
      "Epoch 73/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2466314.7500\n",
      "Epoch 73: loss improved from 2468231.50000 to 2466080.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2466080.7500\n",
      "Epoch 74/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2467199.0000\n",
      "Epoch 74: loss improved from 2466080.75000 to 2465473.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2465473.2500\n",
      "Epoch 75/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2465356.7500\n",
      "Epoch 75: loss did not improve from 2465473.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2465752.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2462502.7500\n",
      "Epoch 76: loss improved from 2465473.25000 to 2462524.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2462524.7500\n",
      "Epoch 77/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2461704.7500\n",
      "Epoch 77: loss improved from 2462524.75000 to 2460571.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2460571.5000\n",
      "Epoch 78/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2458312.5000\n",
      "Epoch 78: loss improved from 2460571.50000 to 2458312.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2458312.5000\n",
      "Epoch 79/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2459141.0000\n",
      "Epoch 79: loss improved from 2458312.50000 to 2457908.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2457908.2500\n",
      "Epoch 80/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2456885.7500\n",
      "Epoch 80: loss did not improve from 2457908.25000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2458010.7500\n",
      "Epoch 81/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2454943.2500\n",
      "Epoch 81: loss improved from 2457908.25000 to 2455025.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2455025.2500\n",
      "Epoch 82/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2452839.2500\n",
      "Epoch 82: loss improved from 2455025.25000 to 2453296.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2453296.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2452443.5000\n",
      "Epoch 83: loss improved from 2453296.75000 to 2452382.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2452382.5000\n",
      "Epoch 84/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2449365.2500\n",
      "Epoch 84: loss improved from 2452382.50000 to 2451950.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2451950.2500\n",
      "Epoch 85/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2446725.7500\n",
      "Epoch 85: loss improved from 2451950.25000 to 2448936.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2448936.0000\n",
      "Epoch 86/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2448628.7500\n",
      "Epoch 86: loss improved from 2448936.00000 to 2448142.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2448142.7500\n",
      "Epoch 87/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2446137.5000\n",
      "Epoch 87: loss improved from 2448142.75000 to 2445701.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2445701.0000\n",
      "Epoch 88/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2445743.2500\n",
      "Epoch 88: loss improved from 2445701.00000 to 2445550.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2445550.0000\n",
      "Epoch 89/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2444241.0000\n",
      "Epoch 89: loss improved from 2445550.00000 to 2444243.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2444243.0000\n",
      "Epoch 90/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2443554.7500\n",
      "Epoch 90: loss improved from 2444243.00000 to 2443306.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2443306.5000\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2440927.5000\n",
      "Epoch 91: loss improved from 2443306.50000 to 2440311.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2440311.2500\n",
      "Epoch 92/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2440294.5000\n",
      "Epoch 92: loss improved from 2440311.25000 to 2440173.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2440173.0000\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2435878.7500\n",
      "Epoch 93: loss improved from 2440173.00000 to 2437962.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2437962.2500\n",
      "Epoch 94/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2438438.2500\n",
      "Epoch 94: loss improved from 2437962.25000 to 2437408.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2437408.5000\n",
      "Epoch 95/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2438917.7500\n",
      "Epoch 95: loss improved from 2437408.50000 to 2436881.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2436881.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2434674.2500\n",
      "Epoch 96: loss improved from 2436881.50000 to 2434674.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2434674.2500\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2435218.2500\n",
      "Epoch 97: loss did not improve from 2434674.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2434682.2500\n",
      "Epoch 98/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2433973.0000\n",
      "Epoch 98: loss improved from 2434674.25000 to 2433673.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2433673.2500\n",
      "Epoch 99/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2432353.5000\n",
      "Epoch 99: loss improved from 2433673.25000 to 2432637.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2432637.2500\n",
      "Epoch 100/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2430243.5000\n",
      "Epoch 100: loss improved from 2432637.25000 to 2431365.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2431365.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 07:33:28 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 07:33:28 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn2ol_3zo\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn2ol_3zo\\model\\data\\model\\assets\n",
      "2023/05/28 07:34:06 INFO mlflow.tracking.fluent: Experiment with name 'dnn4vector_min_size 2500 n_samples 2000 dropout 0.1 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_127 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   6/3189 [..............................] - ETA: 2:22 - loss: 4538164.5000 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0338s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0338s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3187/3189 [============================>.] - ETA: 0s - loss: 2596019.2500\n",
      "Epoch 1: loss improved from inf to 2595454.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2595454.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2258064.2500\n",
      "Epoch 2: loss improved from 2595454.50000 to 2257566.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2257566.0000\n",
      "Epoch 3/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2086660.7500\n",
      "Epoch 3: loss improved from 2257566.00000 to 2085539.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2085539.3750\n",
      "Epoch 4/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1955290.5000\n",
      "Epoch 4: loss improved from 2085539.37500 to 1956205.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1956205.5000\n",
      "Epoch 5/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1862521.3750\n",
      "Epoch 5: loss improved from 1956205.50000 to 1862484.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1862484.2500\n",
      "Epoch 6/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1782995.8750\n",
      "Epoch 6: loss improved from 1862484.25000 to 1783050.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1783050.5000\n",
      "Epoch 7/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1723534.6250\n",
      "Epoch 7: loss improved from 1783050.50000 to 1722423.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1722423.3750\n",
      "Epoch 8/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1666057.3750\n",
      "Epoch 8: loss improved from 1722423.37500 to 1666180.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1666180.8750\n",
      "Epoch 9/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1621481.0000\n",
      "Epoch 9: loss improved from 1666180.87500 to 1621456.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1621456.3750\n",
      "Epoch 10/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1583270.2500\n",
      "Epoch 10: loss improved from 1621456.37500 to 1582380.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1582380.8750\n",
      "Epoch 11/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1547770.2500\n",
      "Epoch 11: loss improved from 1582380.87500 to 1547616.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1547616.1250\n",
      "Epoch 12/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1517979.3750\n",
      "Epoch 12: loss improved from 1547616.12500 to 1517921.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1517921.6250\n",
      "Epoch 13/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1490277.2500\n",
      "Epoch 13: loss improved from 1517921.62500 to 1490155.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1490155.0000\n",
      "Epoch 14/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1466089.5000\n",
      "Epoch 14: loss improved from 1490155.00000 to 1466244.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1466244.2500\n",
      "Epoch 15/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1442282.2500\n",
      "Epoch 15: loss improved from 1466244.25000 to 1442185.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1442185.3750\n",
      "Epoch 16/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1424607.2500\n",
      "Epoch 16: loss improved from 1442185.37500 to 1424366.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1424366.2500\n",
      "Epoch 17/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1406978.5000\n",
      "Epoch 17: loss improved from 1424366.25000 to 1407307.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1407307.1250\n",
      "Epoch 18/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1390248.8750\n",
      "Epoch 18: loss improved from 1407307.12500 to 1390494.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1390494.5000\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1376292.6250\n",
      "Epoch 19: loss improved from 1390494.50000 to 1376347.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1376347.2500\n",
      "Epoch 20/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1361699.8750\n",
      "Epoch 20: loss improved from 1376347.25000 to 1361866.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 39s 12ms/step - loss: 1361866.7500\n",
      "Epoch 21/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1346481.0000\n",
      "Epoch 21: loss improved from 1361866.75000 to 1346292.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 6ms/step - loss: 1346292.8750\n",
      "Epoch 22/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1335795.5000\n",
      "Epoch 22: loss improved from 1346292.87500 to 1335928.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1335928.2500\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1326276.0000\n",
      "Epoch 23: loss improved from 1335928.25000 to 1325971.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1325971.5000\n",
      "Epoch 24/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1314146.2500\n",
      "Epoch 24: loss improved from 1325971.50000 to 1314289.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1314289.6250\n",
      "Epoch 25/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1301933.0000\n",
      "Epoch 25: loss improved from 1314289.62500 to 1302079.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1302079.2500\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1294151.2500\n",
      "Epoch 26: loss improved from 1302079.25000 to 1294151.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1294151.2500\n",
      "Epoch 27/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1284923.3750\n",
      "Epoch 27: loss improved from 1294151.25000 to 1284807.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1284807.6250\n",
      "Epoch 28/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1276406.1250\n",
      "Epoch 28: loss improved from 1284807.62500 to 1277011.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1277011.8750\n",
      "Epoch 29/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1266270.3750\n",
      "Epoch 29: loss improved from 1277011.87500 to 1266266.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1266266.0000\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1260004.6250\n",
      "Epoch 30: loss improved from 1266266.00000 to 1259951.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1259951.5000\n",
      "Epoch 31/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1255147.0000\n",
      "Epoch 31: loss improved from 1259951.50000 to 1255679.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1255679.6250\n",
      "Epoch 32/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1244789.0000\n",
      "Epoch 32: loss improved from 1255679.62500 to 1245009.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1245009.5000\n",
      "Epoch 33/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1240821.1250\n",
      "Epoch 33: loss improved from 1245009.50000 to 1240849.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1240849.0000\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1233538.0000\n",
      "Epoch 34: loss improved from 1240849.00000 to 1233538.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1233538.0000\n",
      "Epoch 35/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1227213.7500\n",
      "Epoch 35: loss improved from 1233538.00000 to 1226486.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1226486.7500\n",
      "Epoch 36/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1217893.3750\n",
      "Epoch 36: loss improved from 1226486.75000 to 1217981.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1217981.6250\n",
      "Epoch 37/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1212635.0000\n",
      "Epoch 37: loss improved from 1217981.62500 to 1212713.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1212713.2500\n",
      "Epoch 38/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1208750.3750\n",
      "Epoch 38: loss improved from 1212713.25000 to 1208890.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1208890.5000\n",
      "Epoch 39/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1203658.8750\n",
      "Epoch 39: loss improved from 1208890.50000 to 1203878.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1203878.0000\n",
      "Epoch 40/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1199557.3750\n",
      "Epoch 40: loss improved from 1203878.00000 to 1199720.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1199720.2500\n",
      "Epoch 41/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1194425.5000\n",
      "Epoch 41: loss improved from 1199720.25000 to 1194696.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1194696.3750\n",
      "Epoch 42/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1189404.3750\n",
      "Epoch 42: loss improved from 1194696.37500 to 1189153.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1189153.3750\n",
      "Epoch 43/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1185479.5000\n",
      "Epoch 43: loss improved from 1189153.37500 to 1184919.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1184919.7500\n",
      "Epoch 44/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1181446.6250\n",
      "Epoch 44: loss improved from 1184919.75000 to 1181339.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1181339.5000\n",
      "Epoch 45/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1175542.6250\n",
      "Epoch 45: loss improved from 1181339.50000 to 1175656.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1175656.0000\n",
      "Epoch 46/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1170546.3750\n",
      "Epoch 46: loss improved from 1175656.00000 to 1170643.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1170643.0000\n",
      "Epoch 47/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1169238.3750\n",
      "Epoch 47: loss improved from 1170643.00000 to 1169189.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1169189.7500\n",
      "Epoch 48/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1163786.8750\n",
      "Epoch 48: loss improved from 1169189.75000 to 1164751.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1164751.7500\n",
      "Epoch 49/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1160697.7500\n",
      "Epoch 49: loss improved from 1164751.75000 to 1161326.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1161326.6250\n",
      "Epoch 50/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1157621.1250\n",
      "Epoch 50: loss improved from 1161326.62500 to 1157435.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1157435.0000\n",
      "Epoch 51/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1154483.7500\n",
      "Epoch 51: loss improved from 1157435.00000 to 1154337.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1154337.5000\n",
      "Epoch 52/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1150819.1250\n",
      "Epoch 52: loss improved from 1154337.50000 to 1150606.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1150606.3750\n",
      "Epoch 53/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1146786.1250\n",
      "Epoch 53: loss improved from 1150606.37500 to 1146698.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1146698.0000\n",
      "Epoch 54/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1145573.5000\n",
      "Epoch 54: loss improved from 1146698.00000 to 1145508.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1145508.2500\n",
      "Epoch 55/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1142185.3750\n",
      "Epoch 55: loss improved from 1145508.25000 to 1141919.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1141919.2500\n",
      "Epoch 56/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1137603.7500\n",
      "Epoch 56: loss improved from 1141919.25000 to 1137425.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1137425.3750\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1135018.7500\n",
      "Epoch 57: loss improved from 1137425.37500 to 1134567.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1134567.0000\n",
      "Epoch 58/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1132322.8750\n",
      "Epoch 58: loss improved from 1134567.00000 to 1132379.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1132379.6250\n",
      "Epoch 59/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1129070.0000\n",
      "Epoch 59: loss improved from 1132379.62500 to 1129107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1129107.7500\n",
      "Epoch 60/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1126020.8750\n",
      "Epoch 60: loss improved from 1129107.75000 to 1126094.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1126094.8750\n",
      "Epoch 61/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1125416.7500\n",
      "Epoch 61: loss improved from 1126094.87500 to 1125352.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1125352.7500\n",
      "Epoch 62/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1121625.3750\n",
      "Epoch 62: loss improved from 1125352.75000 to 1121440.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1121440.2500\n",
      "Epoch 63/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1117533.5000\n",
      "Epoch 63: loss improved from 1121440.25000 to 1117380.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1117380.1250\n",
      "Epoch 64/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1114890.3750\n",
      "Epoch 64: loss improved from 1117380.12500 to 1115817.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1115817.1250\n",
      "Epoch 65/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1114267.5000\n",
      "Epoch 65: loss improved from 1115817.12500 to 1114048.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1114048.6250\n",
      "Epoch 66/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1110006.0000\n",
      "Epoch 66: loss improved from 1114048.62500 to 1109889.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1109889.7500\n",
      "Epoch 67/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1110212.2500\n",
      "Epoch 67: loss improved from 1109889.75000 to 1109481.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1109481.6250\n",
      "Epoch 68/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1105592.3750\n",
      "Epoch 68: loss improved from 1109481.62500 to 1105132.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1105132.0000\n",
      "Epoch 69/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1103052.7500\n",
      "Epoch 69: loss improved from 1105132.00000 to 1103196.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1103196.5000\n",
      "Epoch 70/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1100458.5000\n",
      "Epoch 70: loss improved from 1103196.50000 to 1100205.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1100205.2500\n",
      "Epoch 71/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1099404.5000\n",
      "Epoch 71: loss improved from 1100205.25000 to 1099513.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1099513.3750\n",
      "Epoch 72/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1096610.7500\n",
      "Epoch 72: loss improved from 1099513.37500 to 1096475.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1096475.7500\n",
      "Epoch 73/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1093604.0000\n",
      "Epoch 73: loss improved from 1096475.75000 to 1093727.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1093727.0000\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1091257.0000\n",
      "Epoch 74: loss improved from 1093727.00000 to 1091219.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1091219.2500\n",
      "Epoch 75/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1089011.5000\n",
      "Epoch 75: loss improved from 1091219.25000 to 1089671.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1089671.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1087367.7500\n",
      "Epoch 76: loss improved from 1089671.25000 to 1087714.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1087714.1250\n",
      "Epoch 77/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1086610.6250\n",
      "Epoch 77: loss improved from 1087714.12500 to 1086150.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1086150.8750\n",
      "Epoch 78/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1081321.8750\n",
      "Epoch 78: loss improved from 1086150.87500 to 1081203.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1081203.8750\n",
      "Epoch 79/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1080443.0000\n",
      "Epoch 79: loss improved from 1081203.87500 to 1080369.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1080369.5000\n",
      "Epoch 80/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1078001.0000\n",
      "Epoch 80: loss improved from 1080369.50000 to 1079598.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1079598.1250\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1077765.5000\n",
      "Epoch 81: loss improved from 1079598.12500 to 1077765.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1077765.5000\n",
      "Epoch 82/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1075415.0000\n",
      "Epoch 82: loss improved from 1077765.50000 to 1075450.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1075450.1250\n",
      "Epoch 83/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1073197.7500\n",
      "Epoch 83: loss improved from 1075450.12500 to 1073759.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1073759.6250\n",
      "Epoch 84/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1070943.2500\n",
      "Epoch 84: loss improved from 1073759.62500 to 1071547.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1071547.7500\n",
      "Epoch 85/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1069609.2500\n",
      "Epoch 85: loss improved from 1071547.75000 to 1069479.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1069479.7500\n",
      "Epoch 86/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1067331.8750\n",
      "Epoch 86: loss improved from 1069479.75000 to 1067390.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1067390.8750\n",
      "Epoch 87/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1066769.3750\n",
      "Epoch 87: loss improved from 1067390.87500 to 1066847.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1066847.2500\n",
      "Epoch 88/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1066327.5000\n",
      "Epoch 88: loss improved from 1066847.25000 to 1066168.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1066168.2500\n",
      "Epoch 89/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1063510.2500\n",
      "Epoch 89: loss improved from 1066168.25000 to 1063521.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1063521.7500\n",
      "Epoch 90/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1062636.0000\n",
      "Epoch 90: loss improved from 1063521.75000 to 1062495.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1062495.5000\n",
      "Epoch 91/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1059640.8750\n",
      "Epoch 91: loss improved from 1062495.50000 to 1059236.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1059236.1250\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1057793.6250\n",
      "Epoch 92: loss improved from 1059236.12500 to 1057693.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1057693.2500\n",
      "Epoch 93/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1056705.3750\n",
      "Epoch 93: loss improved from 1057693.25000 to 1056628.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1056628.7500\n",
      "Epoch 94/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1054557.0000\n",
      "Epoch 94: loss improved from 1056628.75000 to 1054474.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1054474.8750\n",
      "Epoch 95/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1054123.0000\n",
      "Epoch 95: loss improved from 1054474.87500 to 1053930.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1053930.6250\n",
      "Epoch 96/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1052081.6250\n",
      "Epoch 96: loss improved from 1053930.62500 to 1052010.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1052010.1250\n",
      "Epoch 97/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1052512.7500\n",
      "Epoch 97: loss did not improve from 1052010.12500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1052448.7500\n",
      "Epoch 98/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1047994.3750\n",
      "Epoch 98: loss improved from 1052010.12500 to 1048563.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1048563.0000\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1048900.0000\n",
      "Epoch 99: loss did not improve from 1048563.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1049010.6250\n",
      "Epoch 100/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1046036.0625\n",
      "Epoch 100: loss improved from 1048563.00000 to 1046479.93750, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1046479.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 07:54:45 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 07:54:45 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpk7933pww\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpk7933pww\\model\\data\\model\\assets\n",
      "2023/05/28 07:55:38 INFO mlflow.tracking.fluent: Experiment with name 'dnn4vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 100' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_131 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 2:21 - loss: 5275207.0000   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0694s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0694s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186/3189 [============================>.] - ETA: 0s - loss: 2744580.7500\n",
      "Epoch 1: loss improved from inf to 2744546.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 21s 5ms/step - loss: 2744546.5000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2730445.5000\n",
      "Epoch 2: loss improved from 2744546.50000 to 2729755.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2729755.2500\n",
      "Epoch 3/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2714177.2500\n",
      "Epoch 3: loss improved from 2729755.25000 to 2718289.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2718289.7500\n",
      "Epoch 4/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2705589.2500\n",
      "Epoch 4: loss improved from 2718289.75000 to 2705956.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2705956.7500\n",
      "Epoch 5/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2694961.0000\n",
      "Epoch 5: loss improved from 2705956.75000 to 2694166.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2694166.2500\n",
      "Epoch 6/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2685449.2500\n",
      "Epoch 6: loss improved from 2694166.25000 to 2683515.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2683515.2500\n",
      "Epoch 7/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2672900.2500\n",
      "Epoch 7: loss improved from 2683515.25000 to 2673945.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2673945.0000\n",
      "Epoch 8/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2666520.0000\n",
      "Epoch 8: loss improved from 2673945.00000 to 2665490.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2665490.7500\n",
      "Epoch 9/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2656355.2500\n",
      "Epoch 9: loss improved from 2665490.75000 to 2657501.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2657501.2500\n",
      "Epoch 10/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2651514.0000\n",
      "Epoch 10: loss improved from 2657501.25000 to 2650220.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2650220.2500\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2644412.7500\n",
      "Epoch 11: loss improved from 2650220.25000 to 2643899.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2643899.7500\n",
      "Epoch 12/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2636702.7500\n",
      "Epoch 12: loss improved from 2643899.75000 to 2636612.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2636612.5000\n",
      "Epoch 13/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2632378.0000\n",
      "Epoch 13: loss improved from 2636612.50000 to 2632930.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2632930.0000\n",
      "Epoch 14/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2626944.2500\n",
      "Epoch 14: loss improved from 2632930.00000 to 2628328.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2628328.7500\n",
      "Epoch 15/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2622293.5000\n",
      "Epoch 15: loss improved from 2628328.75000 to 2621627.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2621627.5000\n",
      "Epoch 16/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2617423.0000\n",
      "Epoch 16: loss improved from 2621627.50000 to 2617848.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2617848.0000\n",
      "Epoch 17/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2614248.5000\n",
      "Epoch 17: loss improved from 2617848.00000 to 2613207.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2613207.5000\n",
      "Epoch 18/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2608608.5000\n",
      "Epoch 18: loss improved from 2613207.50000 to 2608798.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2608798.5000\n",
      "Epoch 19/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2603230.7500\n",
      "Epoch 19: loss improved from 2608798.50000 to 2603025.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2603025.5000\n",
      "Epoch 20/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2601806.7500\n",
      "Epoch 20: loss improved from 2603025.50000 to 2601503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2601503.0000\n",
      "Epoch 21/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2595322.7500\n",
      "Epoch 21: loss improved from 2601503.00000 to 2594941.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2594941.2500\n",
      "Epoch 22/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2592789.2500\n",
      "Epoch 22: loss improved from 2594941.25000 to 2592672.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2592672.0000\n",
      "Epoch 23/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2588383.2500\n",
      "Epoch 23: loss improved from 2592672.00000 to 2587828.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2587828.7500\n",
      "Epoch 24/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2584296.7500\n",
      "Epoch 24: loss improved from 2587828.75000 to 2584871.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2584871.5000\n",
      "Epoch 25/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2581976.7500\n",
      "Epoch 25: loss improved from 2584871.50000 to 2581531.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2581531.2500\n",
      "Epoch 26/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2579262.5000\n",
      "Epoch 26: loss improved from 2581531.25000 to 2578653.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2578653.5000\n",
      "Epoch 27/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2574937.7500\n",
      "Epoch 27: loss improved from 2578653.50000 to 2576337.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2576337.2500\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2571284.0000\n",
      "Epoch 28: loss improved from 2576337.25000 to 2571284.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2571284.0000\n",
      "Epoch 29/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2571038.5000\n",
      "Epoch 29: loss improved from 2571284.00000 to 2570229.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2570229.7500\n",
      "Epoch 30/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2568489.5000\n",
      "Epoch 30: loss improved from 2570229.75000 to 2566478.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2566478.7500\n",
      "Epoch 31/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2564926.5000\n",
      "Epoch 31: loss improved from 2566478.75000 to 2565971.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2565971.7500\n",
      "Epoch 32/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2564982.5000\n",
      "Epoch 32: loss improved from 2565971.75000 to 2565118.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2565118.2500\n",
      "Epoch 33/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2563339.7500\n",
      "Epoch 33: loss improved from 2565118.25000 to 2562294.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2562294.2500\n",
      "Epoch 34/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2558177.5000\n",
      "Epoch 34: loss improved from 2562294.25000 to 2557573.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2557573.7500\n",
      "Epoch 35/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2557381.0000\n",
      "Epoch 35: loss improved from 2557573.75000 to 2557521.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2557521.5000\n",
      "Epoch 36/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2555317.0000\n",
      "Epoch 36: loss improved from 2557521.50000 to 2554856.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2554856.2500\n",
      "Epoch 37/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2553423.0000\n",
      "Epoch 37: loss improved from 2554856.25000 to 2553408.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2553408.7500\n",
      "Epoch 38/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2552794.0000\n",
      "Epoch 38: loss improved from 2553408.75000 to 2552164.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2552164.5000\n",
      "Epoch 39/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2548950.0000\n",
      "Epoch 39: loss improved from 2552164.50000 to 2549294.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2549294.5000\n",
      "Epoch 40/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2550370.2500\n",
      "Epoch 40: loss did not improve from 2549294.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2550890.7500\n",
      "Epoch 41/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2548474.7500\n",
      "Epoch 41: loss improved from 2549294.50000 to 2549032.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2549032.0000\n",
      "Epoch 42/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2545677.0000\n",
      "Epoch 42: loss improved from 2549032.00000 to 2546606.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2546606.0000\n",
      "Epoch 43/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2548640.0000\n",
      "Epoch 43: loss did not improve from 2546606.00000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2547613.7500\n",
      "Epoch 44/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2546245.5000\n",
      "Epoch 44: loss improved from 2546606.00000 to 2545800.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2545800.7500\n",
      "Epoch 45/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2542167.0000\n",
      "Epoch 45: loss improved from 2545800.75000 to 2540884.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2540884.7500\n",
      "Epoch 46/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2539308.0000\n",
      "Epoch 46: loss did not improve from 2540884.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2542129.0000\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2543184.2500\n",
      "Epoch 47: loss did not improve from 2540884.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2543038.7500\n",
      "Epoch 48/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2541575.2500\n",
      "Epoch 48: loss improved from 2540884.75000 to 2540370.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2540370.7500\n",
      "Epoch 49/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2540222.2500\n",
      "Epoch 49: loss improved from 2540370.75000 to 2539880.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2539880.5000\n",
      "Epoch 50/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2536372.0000\n",
      "Epoch 50: loss improved from 2539880.50000 to 2537431.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537431.7500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2537127.0000\n",
      "Epoch 51: loss improved from 2537431.75000 to 2537127.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2537127.0000\n",
      "Epoch 52/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2537358.2500\n",
      "Epoch 52: loss improved from 2537127.00000 to 2536961.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2536961.0000\n",
      "Epoch 53/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2538193.7500\n",
      "Epoch 53: loss did not improve from 2536961.00000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2537806.0000\n",
      "Epoch 54/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2534101.2500\n",
      "Epoch 54: loss improved from 2536961.00000 to 2534601.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2534601.2500\n",
      "Epoch 55/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2534482.2500\n",
      "Epoch 55: loss improved from 2534601.25000 to 2534469.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2534469.5000\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2536951.7500\n",
      "Epoch 56: loss did not improve from 2534469.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2535633.0000\n",
      "Epoch 57/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2533193.5000\n",
      "Epoch 57: loss improved from 2534469.50000 to 2532725.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2532725.5000\n",
      "Epoch 58/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2531907.2500\n",
      "Epoch 58: loss improved from 2532725.50000 to 2531753.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531753.7500\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2533147.7500\n",
      "Epoch 59: loss did not improve from 2531753.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533132.2500\n",
      "Epoch 60/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2530504.5000\n",
      "Epoch 60: loss improved from 2531753.75000 to 2531111.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531111.5000\n",
      "Epoch 61/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2531542.2500\n",
      "Epoch 61: loss did not improve from 2531111.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2531542.2500\n",
      "Epoch 62/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2527048.7500\n",
      "Epoch 62: loss improved from 2531111.50000 to 2528086.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2528086.2500\n",
      "Epoch 63/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2528758.7500\n",
      "Epoch 63: loss did not improve from 2528086.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2528608.0000\n",
      "Epoch 64/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2528695.5000\n",
      "Epoch 64: loss did not improve from 2528086.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530309.2500\n",
      "Epoch 65/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2529019.5000\n",
      "Epoch 65: loss did not improve from 2528086.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2528166.0000\n",
      "Epoch 66/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2526811.5000\n",
      "Epoch 66: loss improved from 2528086.25000 to 2526671.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2526671.7500\n",
      "Epoch 67/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2527240.5000\n",
      "Epoch 67: loss did not improve from 2526671.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2527244.0000\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2525794.7500\n",
      "Epoch 68: loss improved from 2526671.75000 to 2525727.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525727.5000\n",
      "Epoch 69/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2526621.2500\n",
      "Epoch 69: loss did not improve from 2525727.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526220.0000\n",
      "Epoch 70/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2522168.5000\n",
      "Epoch 70: loss improved from 2525727.50000 to 2523203.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2523203.2500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2526377.5000\n",
      "Epoch 71: loss did not improve from 2523203.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526377.5000\n",
      "Epoch 72/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2525698.0000\n",
      "Epoch 72: loss did not improve from 2523203.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525745.5000\n",
      "Epoch 73/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2523711.7500\n",
      "Epoch 73: loss did not improve from 2523203.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2523339.0000\n",
      "Epoch 74/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2523054.0000\n",
      "Epoch 74: loss improved from 2523203.25000 to 2523190.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2523190.7500\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2520425.0000\n",
      "Epoch 75: loss improved from 2523190.75000 to 2521263.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2521263.2500\n",
      "Epoch 76/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2521457.5000\n",
      "Epoch 76: loss improved from 2521263.25000 to 2521122.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2521122.7500\n",
      "Epoch 77/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2521818.2500\n",
      "Epoch 77: loss did not improve from 2521122.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2521580.5000\n",
      "Epoch 78/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2519989.0000\n",
      "Epoch 78: loss improved from 2521122.75000 to 2520374.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 2520374.7500\n",
      "Epoch 79/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2518512.7500\n",
      "Epoch 79: loss improved from 2520374.75000 to 2519039.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 2519039.5000\n",
      "Epoch 80/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2516507.5000\n",
      "Epoch 80: loss improved from 2519039.50000 to 2517639.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 7ms/step - loss: 2517639.2500\n",
      "Epoch 81/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2518443.5000\n",
      "Epoch 81: loss did not improve from 2517639.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2518568.0000\n",
      "Epoch 82/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2518111.0000\n",
      "Epoch 82: loss did not improve from 2517639.25000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2517969.5000\n",
      "Epoch 83/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2515816.2500\n",
      "Epoch 83: loss improved from 2517639.25000 to 2517192.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2517192.7500\n",
      "Epoch 84/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2516627.0000\n",
      "Epoch 84: loss did not improve from 2517192.75000\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2518094.0000\n",
      "Epoch 85/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2514200.7500\n",
      "Epoch 85: loss improved from 2517192.75000 to 2516138.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2516138.5000\n",
      "Epoch 86/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2515188.5000\n",
      "Epoch 86: loss improved from 2516138.50000 to 2516014.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2516014.0000\n",
      "Epoch 87/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2515345.2500\n",
      "Epoch 87: loss improved from 2516014.00000 to 2514453.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2514453.5000\n",
      "Epoch 88/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2514922.2500\n",
      "Epoch 88: loss did not improve from 2514453.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514922.2500\n",
      "Epoch 89/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2514896.0000\n",
      "Epoch 89: loss did not improve from 2514453.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514679.5000\n",
      "Epoch 90/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2514171.2500\n",
      "Epoch 90: loss did not improve from 2514453.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514773.7500\n",
      "Epoch 91/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2513232.2500\n",
      "Epoch 91: loss did not improve from 2514453.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514657.2500\n",
      "Epoch 92/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2513786.0000\n",
      "Epoch 92: loss improved from 2514453.50000 to 2513807.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2513807.2500\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2511517.5000\n",
      "Epoch 93: loss improved from 2513807.25000 to 2511517.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2511517.5000\n",
      "Epoch 94/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2513577.0000\n",
      "Epoch 94: loss did not improve from 2511517.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2512581.0000\n",
      "Epoch 95/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2513918.0000\n",
      "Epoch 95: loss did not improve from 2511517.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2513652.0000\n",
      "Epoch 96/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2510747.2500\n",
      "Epoch 96: loss improved from 2511517.50000 to 2510609.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2510609.7500\n",
      "Epoch 97/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2512847.5000\n",
      "Epoch 97: loss did not improve from 2510609.75000\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2511791.2500\n",
      "Epoch 98/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2511678.0000\n",
      "Epoch 98: loss improved from 2510609.75000 to 2509477.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509477.7500\n",
      "Epoch 99/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2508094.5000\n",
      "Epoch 99: loss did not improve from 2509477.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509635.5000\n",
      "Epoch 100/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2507879.0000\n",
      "Epoch 100: loss improved from 2509477.75000 to 2508397.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2508397.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 08:16:36 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 08:16:36 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpgdmh318z\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpgdmh318z\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_135 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   5/3189 [..............................] - ETA: 42s - loss: 1636397.5000    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0197s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0197s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3185/3189 [============================>.] - ETA: 0s - loss: 2684280.2500\n",
      "Epoch 1: loss improved from inf to 2684348.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 25s 6ms/step - loss: 2684348.2500\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2589847.2500\n",
      "Epoch 2: loss improved from 2684348.25000 to 2589847.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2589847.2500\n",
      "Epoch 3/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2531411.0000\n",
      "Epoch 3: loss improved from 2589847.25000 to 2531357.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2531357.2500\n",
      "Epoch 4/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2489526.7500\n",
      "Epoch 4: loss improved from 2531357.25000 to 2488829.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2488829.0000\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2452365.7500\n",
      "Epoch 5: loss improved from 2488829.00000 to 2452453.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2452453.5000\n",
      "Epoch 6/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2421078.0000\n",
      "Epoch 6: loss improved from 2452453.50000 to 2420373.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2420373.5000\n",
      "Epoch 7/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2391913.7500\n",
      "Epoch 7: loss improved from 2420373.50000 to 2391861.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2391861.7500\n",
      "Epoch 8/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2366266.5000\n",
      "Epoch 8: loss improved from 2391861.75000 to 2366065.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2366065.7500\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2345667.2500\n",
      "Epoch 9: loss improved from 2366065.75000 to 2344185.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2344185.7500\n",
      "Epoch 10/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2321719.5000\n",
      "Epoch 10: loss improved from 2344185.75000 to 2321942.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2321942.7500\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2302839.2500\n",
      "Epoch 11: loss improved from 2321942.75000 to 2302839.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2302839.2500\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2283568.2500\n",
      "Epoch 12: loss improved from 2302839.25000 to 2283810.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2283810.5000\n",
      "Epoch 13/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2269544.7500\n",
      "Epoch 13: loss improved from 2283810.50000 to 2269041.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2269041.5000\n",
      "Epoch 14/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2253141.5000\n",
      "Epoch 14: loss improved from 2269041.50000 to 2252151.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2252151.2500\n",
      "Epoch 15/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2235365.5000\n",
      "Epoch 15: loss improved from 2252151.25000 to 2237167.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2237167.2500\n",
      "Epoch 16/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2224045.0000\n",
      "Epoch 16: loss improved from 2237167.25000 to 2222453.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2222453.7500\n",
      "Epoch 17/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2208666.2500\n",
      "Epoch 17: loss improved from 2222453.75000 to 2208269.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2208269.5000\n",
      "Epoch 18/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2194260.7500\n",
      "Epoch 18: loss improved from 2208269.50000 to 2195171.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2195171.5000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2183049.2500\n",
      "Epoch 19: loss improved from 2195171.50000 to 2182995.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2182995.0000\n",
      "Epoch 20/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2171696.2500\n",
      "Epoch 20: loss improved from 2182995.00000 to 2170969.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2170969.7500\n",
      "Epoch 21/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2160245.2500\n",
      "Epoch 21: loss improved from 2170969.75000 to 2159648.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2159648.5000\n",
      "Epoch 22/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2150410.5000\n",
      "Epoch 22: loss improved from 2159648.50000 to 2150248.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2150248.5000\n",
      "Epoch 23/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2141194.7500\n",
      "Epoch 23: loss improved from 2150248.50000 to 2140486.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2140486.0000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2132215.2500\n",
      "Epoch 24: loss improved from 2140486.00000 to 2131364.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2131364.7500\n",
      "Epoch 25/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2124442.7500\n",
      "Epoch 25: loss improved from 2131364.75000 to 2122734.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2122734.2500\n",
      "Epoch 26/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2113597.2500\n",
      "Epoch 26: loss improved from 2122734.25000 to 2114659.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2114659.2500\n",
      "Epoch 27/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2105979.5000\n",
      "Epoch 27: loss improved from 2114659.25000 to 2105846.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2105846.5000\n",
      "Epoch 28/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2100748.5000\n",
      "Epoch 28: loss improved from 2105846.50000 to 2100746.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2100746.7500\n",
      "Epoch 29/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2092048.5000\n",
      "Epoch 29: loss improved from 2100746.75000 to 2091640.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2091640.2500\n",
      "Epoch 30/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2086637.0000\n",
      "Epoch 30: loss improved from 2091640.25000 to 2086468.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2086468.8750\n",
      "Epoch 31/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2079549.8750\n",
      "Epoch 31: loss improved from 2086468.87500 to 2079537.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2079537.3750\n",
      "Epoch 32/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2070146.7500\n",
      "Epoch 32: loss improved from 2079537.37500 to 2072630.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2072630.1250\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2066737.3750\n",
      "Epoch 33: loss improved from 2072630.12500 to 2066737.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2066737.3750\n",
      "Epoch 34/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2063707.0000\n",
      "Epoch 34: loss improved from 2066737.37500 to 2062521.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2062521.7500\n",
      "Epoch 35/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2056194.1250\n",
      "Epoch 35: loss improved from 2062521.75000 to 2056031.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2056031.0000\n",
      "Epoch 36/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2050772.1250\n",
      "Epoch 36: loss improved from 2056031.00000 to 2051571.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2051571.3750\n",
      "Epoch 37/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2044926.0000\n",
      "Epoch 37: loss improved from 2051571.37500 to 2045264.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2045264.6250\n",
      "Epoch 38/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2042667.1250\n",
      "Epoch 38: loss improved from 2045264.62500 to 2042679.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2042679.7500\n",
      "Epoch 39/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2036849.2500\n",
      "Epoch 39: loss improved from 2042679.75000 to 2036717.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2036717.5000\n",
      "Epoch 40/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2032348.0000\n",
      "Epoch 40: loss improved from 2036717.50000 to 2032336.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2032336.6250\n",
      "Epoch 41/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2029118.6250\n",
      "Epoch 41: loss improved from 2032336.62500 to 2028334.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2028334.3750\n",
      "Epoch 42/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2025081.3750\n",
      "Epoch 42: loss improved from 2028334.37500 to 2024806.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2024806.3750\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2020738.1250\n",
      "Epoch 43: loss improved from 2024806.37500 to 2020738.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2020738.1250\n",
      "Epoch 44/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2015454.8750\n",
      "Epoch 44: loss improved from 2020738.12500 to 2017321.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2017321.8750\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2014573.0000\n",
      "Epoch 45: loss improved from 2017321.87500 to 2014251.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2014251.8750\n",
      "Epoch 46/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2011940.5000\n",
      "Epoch 46: loss improved from 2014251.87500 to 2009734.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2009734.2500\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2007072.1250\n",
      "Epoch 47: loss improved from 2009734.25000 to 2007143.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2007143.3750\n",
      "Epoch 48/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2005846.3750\n",
      "Epoch 48: loss improved from 2007143.37500 to 2005349.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2005349.5000\n",
      "Epoch 49/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2002191.2500\n",
      "Epoch 49: loss improved from 2005349.50000 to 2000886.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2000886.0000\n",
      "Epoch 50/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1998599.0000\n",
      "Epoch 50: loss improved from 2000886.00000 to 1998835.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1998835.1250\n",
      "Epoch 51/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1994409.8750\n",
      "Epoch 51: loss improved from 1998835.12500 to 1993934.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1993934.8750\n",
      "Epoch 52/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1991472.7500\n",
      "Epoch 52: loss improved from 1993934.87500 to 1991763.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1991763.5000\n",
      "Epoch 53/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1988946.2500\n",
      "Epoch 53: loss improved from 1991763.50000 to 1987198.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1987198.1250\n",
      "Epoch 54/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1984530.1250\n",
      "Epoch 54: loss improved from 1987198.12500 to 1985022.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1985022.0000\n",
      "Epoch 55/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1983484.3750\n",
      "Epoch 55: loss improved from 1985022.00000 to 1983171.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1983171.6250\n",
      "Epoch 56/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1979255.3750\n",
      "Epoch 56: loss improved from 1983171.62500 to 1980326.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1980326.7500\n",
      "Epoch 57/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1978972.3750\n",
      "Epoch 57: loss improved from 1980326.75000 to 1979107.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1979107.1250\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1974345.2500\n",
      "Epoch 58: loss improved from 1979107.12500 to 1974345.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1974345.2500\n",
      "Epoch 59/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1971307.0000\n",
      "Epoch 59: loss improved from 1974345.25000 to 1972346.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1972346.1250\n",
      "Epoch 60/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1970502.1250\n",
      "Epoch 60: loss improved from 1972346.12500 to 1970004.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 1970004.0000\n",
      "Epoch 61/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1967989.5000\n",
      "Epoch 61: loss improved from 1970004.00000 to 1967730.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1967730.0000\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1964638.1250\n",
      "Epoch 62: loss improved from 1967730.00000 to 1964638.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1964638.1250\n",
      "Epoch 63/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1962006.8750\n",
      "Epoch 63: loss improved from 1964638.12500 to 1963222.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1963222.6250\n",
      "Epoch 64/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1960613.5000\n",
      "Epoch 64: loss improved from 1963222.62500 to 1960445.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1960445.6250\n",
      "Epoch 65/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1959279.5000\n",
      "Epoch 65: loss improved from 1960445.62500 to 1959141.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1959141.1250\n",
      "Epoch 66/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1957508.7500\n",
      "Epoch 66: loss improved from 1959141.12500 to 1957297.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1957297.3750\n",
      "Epoch 67/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1956047.5000\n",
      "Epoch 67: loss improved from 1957297.37500 to 1955656.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1955656.5000\n",
      "Epoch 68/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1951119.3750\n",
      "Epoch 68: loss improved from 1955656.50000 to 1951615.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1951615.2500\n",
      "Epoch 69/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1950558.1250\n",
      "Epoch 69: loss improved from 1951615.25000 to 1950260.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1950260.8750\n",
      "Epoch 70/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1947469.7500\n",
      "Epoch 70: loss improved from 1950260.87500 to 1948463.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1948463.0000\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1947446.8750\n",
      "Epoch 71: loss improved from 1948463.00000 to 1947398.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1947398.7500\n",
      "Epoch 72/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1946292.0000\n",
      "Epoch 72: loss improved from 1947398.75000 to 1946029.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1946029.0000\n",
      "Epoch 73/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1942930.5000\n",
      "Epoch 73: loss improved from 1946029.00000 to 1942137.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1942137.1250\n",
      "Epoch 74/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1940911.3750\n",
      "Epoch 74: loss improved from 1942137.12500 to 1940873.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1940873.0000\n",
      "Epoch 75/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1937896.5000\n",
      "Epoch 75: loss improved from 1940873.00000 to 1938611.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1938611.3750\n",
      "Epoch 76/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1937869.0000\n",
      "Epoch 76: loss improved from 1938611.37500 to 1938059.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1938059.0000\n",
      "Epoch 77/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1934864.0000\n",
      "Epoch 77: loss improved from 1938059.00000 to 1935447.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1935447.2500\n",
      "Epoch 78/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1933820.3750\n",
      "Epoch 78: loss improved from 1935447.25000 to 1934664.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1934664.3750\n",
      "Epoch 79/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1932894.6250\n",
      "Epoch 79: loss improved from 1934664.37500 to 1932100.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1932100.1250\n",
      "Epoch 80/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1932422.2500\n",
      "Epoch 80: loss improved from 1932100.12500 to 1931124.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1931124.3750\n",
      "Epoch 81/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1930578.2500\n",
      "Epoch 81: loss improved from 1931124.37500 to 1929651.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1929651.2500\n",
      "Epoch 82/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1928529.0000\n",
      "Epoch 82: loss improved from 1929651.25000 to 1927682.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1927682.5000\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1926369.2500\n",
      "Epoch 83: loss improved from 1927682.50000 to 1925975.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1925975.0000\n",
      "Epoch 84/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1923975.5000\n",
      "Epoch 84: loss improved from 1925975.00000 to 1923735.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1923735.3750\n",
      "Epoch 85/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1922382.3750\n",
      "Epoch 85: loss improved from 1923735.37500 to 1923029.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1923029.0000\n",
      "Epoch 86/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1922770.2500\n",
      "Epoch 86: loss improved from 1923029.00000 to 1921945.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1921945.2500\n",
      "Epoch 87/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1918375.1250\n",
      "Epoch 87: loss improved from 1921945.25000 to 1919269.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1919269.0000\n",
      "Epoch 88/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1917874.5000\n",
      "Epoch 88: loss improved from 1919269.00000 to 1917677.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1917677.8750\n",
      "Epoch 89/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1918311.8750\n",
      "Epoch 89: loss improved from 1917677.87500 to 1916206.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1916206.5000\n",
      "Epoch 90/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1914892.0000\n",
      "Epoch 90: loss improved from 1916206.50000 to 1914526.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1914526.1250\n",
      "Epoch 91/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1913383.2500\n",
      "Epoch 91: loss improved from 1914526.12500 to 1913900.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1913900.6250\n",
      "Epoch 92/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1912368.7500\n",
      "Epoch 92: loss improved from 1913900.62500 to 1911888.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1911888.7500\n",
      "Epoch 93/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1911787.0000\n",
      "Epoch 93: loss improved from 1911888.75000 to 1911247.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1911247.5000\n",
      "Epoch 94/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1910505.8750\n",
      "Epoch 94: loss improved from 1911247.50000 to 1910054.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1910054.8750\n",
      "Epoch 95/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1908287.6250\n",
      "Epoch 95: loss improved from 1910054.87500 to 1907848.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1907848.0000\n",
      "Epoch 96/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1908438.2500\n",
      "Epoch 96: loss did not improve from 1907848.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1908245.1250\n",
      "Epoch 97/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1906379.1250\n",
      "Epoch 97: loss improved from 1907848.00000 to 1905876.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1905876.7500\n",
      "Epoch 98/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1904200.1250\n",
      "Epoch 98: loss improved from 1905876.75000 to 1904097.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1904097.8750\n",
      "Epoch 99/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1902223.5000\n",
      "Epoch 99: loss improved from 1904097.87500 to 1903222.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1903222.5000\n",
      "Epoch 100/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 1902183.6250\n",
      "Epoch 100: loss improved from 1903222.50000 to 1901299.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 1901299.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 08:35:44 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 08:35:44 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7r5no623\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7r5no623\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_143 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 2:40 - loss: 2898670.2500  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0232s vs `on_train_batch_end` time: 0.0334s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0232s vs `on_train_batch_end` time: 0.0334s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2733869.0000\n",
      "Epoch 1: loss improved from inf to 2733869.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 20s 5ms/step - loss: 2733869.0000\n",
      "Epoch 2/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2719079.0000\n",
      "Epoch 2: loss improved from 2733869.00000 to 2719731.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2719731.5000\n",
      "Epoch 3/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2707494.0000\n",
      "Epoch 3: loss improved from 2719731.50000 to 2706859.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2706859.5000\n",
      "Epoch 4/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2693064.5000\n",
      "Epoch 4: loss improved from 2706859.50000 to 2692324.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2692324.5000\n",
      "Epoch 5/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2678055.5000\n",
      "Epoch 5: loss improved from 2692324.50000 to 2679575.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2679575.7500\n",
      "Epoch 6/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2669756.5000\n",
      "Epoch 6: loss improved from 2679575.75000 to 2668242.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2668242.0000\n",
      "Epoch 7/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2661007.2500\n",
      "Epoch 7: loss improved from 2668242.00000 to 2659759.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2659759.2500\n",
      "Epoch 8/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2652872.5000\n",
      "Epoch 8: loss improved from 2659759.25000 to 2652450.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2652450.7500\n",
      "Epoch 9/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2644733.2500\n",
      "Epoch 9: loss improved from 2652450.75000 to 2644439.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2644439.7500\n",
      "Epoch 10/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2637453.7500\n",
      "Epoch 10: loss improved from 2644439.75000 to 2638224.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2638224.5000\n",
      "Epoch 11/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2632009.7500\n",
      "Epoch 11: loss improved from 2638224.50000 to 2631834.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2631834.0000\n",
      "Epoch 12/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2626567.7500\n",
      "Epoch 12: loss improved from 2631834.00000 to 2625640.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2625640.2500\n",
      "Epoch 13/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2621149.7500\n",
      "Epoch 13: loss improved from 2625640.25000 to 2620708.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2620708.7500\n",
      "Epoch 14/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2616285.7500\n",
      "Epoch 14: loss improved from 2620708.75000 to 2614857.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2614857.7500\n",
      "Epoch 15/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2606662.5000\n",
      "Epoch 15: loss improved from 2614857.75000 to 2610142.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2610142.5000\n",
      "Epoch 16/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2606553.5000\n",
      "Epoch 16: loss improved from 2610142.50000 to 2606874.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2606874.2500\n",
      "Epoch 17/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2602722.5000\n",
      "Epoch 17: loss improved from 2606874.25000 to 2602123.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2602123.2500\n",
      "Epoch 18/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2597547.5000\n",
      "Epoch 18: loss improved from 2602123.25000 to 2597658.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2597658.5000\n",
      "Epoch 19/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2596344.5000\n",
      "Epoch 19: loss improved from 2597658.50000 to 2595439.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2595439.7500\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2589556.2500\n",
      "Epoch 20: loss improved from 2595439.75000 to 2591351.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2591351.5000\n",
      "Epoch 21/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2587522.5000\n",
      "Epoch 21: loss improved from 2591351.50000 to 2586589.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2586589.5000\n",
      "Epoch 22/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2584341.7500\n",
      "Epoch 22: loss improved from 2586589.50000 to 2583650.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2583650.5000\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2581467.0000\n",
      "Epoch 23: loss improved from 2583650.50000 to 2581584.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2581584.0000\n",
      "Epoch 24/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2577587.5000\n",
      "Epoch 24: loss improved from 2581584.00000 to 2576806.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2576806.2500\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2573724.0000\n",
      "Epoch 25: loss improved from 2576806.25000 to 2573876.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2573876.2500\n",
      "Epoch 26/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2570599.5000\n",
      "Epoch 26: loss improved from 2573876.25000 to 2570160.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2570160.5000\n",
      "Epoch 27/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2567234.5000\n",
      "Epoch 27: loss improved from 2570160.50000 to 2567051.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2567051.7500\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2563993.5000\n",
      "Epoch 28: loss improved from 2567051.75000 to 2563924.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2563924.5000\n",
      "Epoch 29/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2561099.7500\n",
      "Epoch 29: loss improved from 2563924.50000 to 2562090.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2562090.2500\n",
      "Epoch 30/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2556562.0000\n",
      "Epoch 30: loss improved from 2562090.25000 to 2557276.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2557276.0000\n",
      "Epoch 31/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2555675.5000\n",
      "Epoch 31: loss improved from 2557276.00000 to 2554999.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2554999.2500\n",
      "Epoch 32/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2550290.0000\n",
      "Epoch 32: loss improved from 2554999.25000 to 2550984.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2550984.7500\n",
      "Epoch 33/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2549676.0000\n",
      "Epoch 33: loss improved from 2550984.75000 to 2549038.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2549038.0000\n",
      "Epoch 34/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2546032.0000\n",
      "Epoch 34: loss improved from 2549038.00000 to 2546000.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2546000.5000\n",
      "Epoch 35/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2542175.2500\n",
      "Epoch 35: loss improved from 2546000.50000 to 2542434.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2542434.0000\n",
      "Epoch 36/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2541080.0000\n",
      "Epoch 36: loss improved from 2542434.00000 to 2539732.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2539732.7500\n",
      "Epoch 37/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2536927.5000\n",
      "Epoch 37: loss improved from 2539732.75000 to 2536805.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2536805.2500\n",
      "Epoch 38/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2534972.5000\n",
      "Epoch 38: loss improved from 2536805.25000 to 2534270.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2534270.5000\n",
      "Epoch 39/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2532367.5000\n",
      "Epoch 39: loss improved from 2534270.50000 to 2532327.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2532327.0000\n",
      "Epoch 40/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2530657.2500\n",
      "Epoch 40: loss improved from 2532327.00000 to 2529126.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2529126.5000\n",
      "Epoch 41/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2528210.0000\n",
      "Epoch 41: loss improved from 2529126.50000 to 2527547.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2527547.7500\n",
      "Epoch 42/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2523625.0000\n",
      "Epoch 42: loss improved from 2527547.75000 to 2524568.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2524568.2500\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2521127.2500\n",
      "Epoch 43: loss improved from 2524568.25000 to 2521127.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2521127.2500\n",
      "Epoch 44/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2519904.5000\n",
      "Epoch 44: loss improved from 2521127.25000 to 2519852.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2519852.5000\n",
      "Epoch 45/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2520361.7500\n",
      "Epoch 45: loss improved from 2519852.50000 to 2518675.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2518675.5000\n",
      "Epoch 46/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2516360.7500\n",
      "Epoch 46: loss improved from 2518675.50000 to 2515383.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2515383.7500\n",
      "Epoch 47/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2512836.2500\n",
      "Epoch 47: loss improved from 2515383.75000 to 2512296.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2512296.7500\n",
      "Epoch 48/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2509898.2500\n",
      "Epoch 48: loss improved from 2512296.75000 to 2510324.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2510324.0000\n",
      "Epoch 49/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2508856.5000\n",
      "Epoch 49: loss improved from 2510324.00000 to 2508694.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2508694.2500\n",
      "Epoch 50/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2507518.2500\n",
      "Epoch 50: loss improved from 2508694.25000 to 2506418.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2506418.2500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2503463.7500\n",
      "Epoch 51: loss improved from 2506418.25000 to 2503463.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2503463.7500\n",
      "Epoch 52/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2502798.7500\n",
      "Epoch 52: loss improved from 2503463.75000 to 2502580.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2502580.7500\n",
      "Epoch 53/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2499054.2500\n",
      "Epoch 53: loss improved from 2502580.75000 to 2498379.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2498379.5000\n",
      "Epoch 54/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2497048.7500\n",
      "Epoch 54: loss improved from 2498379.50000 to 2497532.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2497532.2500\n",
      "Epoch 55/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2497388.5000\n",
      "Epoch 55: loss improved from 2497532.25000 to 2496709.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2496709.5000\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2491869.0000\n",
      "Epoch 56: loss improved from 2496709.50000 to 2492360.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 5ms/step - loss: 2492360.0000\n",
      "Epoch 57/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2491984.7500\n",
      "Epoch 57: loss improved from 2492360.00000 to 2491680.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 5ms/step - loss: 2491680.2500\n",
      "Epoch 58/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2490333.7500\n",
      "Epoch 58: loss improved from 2491680.25000 to 2490576.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2490576.5000\n",
      "Epoch 59/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2487651.0000\n",
      "Epoch 59: loss improved from 2490576.50000 to 2487382.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2487382.2500\n",
      "Epoch 60/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2486921.7500\n",
      "Epoch 60: loss improved from 2487382.25000 to 2486698.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2486698.2500\n",
      "Epoch 61/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2484504.5000\n",
      "Epoch 61: loss improved from 2486698.25000 to 2485102.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2485102.7500\n",
      "Epoch 62/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2480525.0000\n",
      "Epoch 62: loss improved from 2485102.75000 to 2481802.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2481802.7500\n",
      "Epoch 63/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2480702.5000\n",
      "Epoch 63: loss improved from 2481802.75000 to 2481182.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2481182.0000\n",
      "Epoch 64/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2477457.0000\n",
      "Epoch 64: loss improved from 2481182.00000 to 2479988.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2479988.5000\n",
      "Epoch 65/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2477838.5000\n",
      "Epoch 65: loss improved from 2479988.50000 to 2477938.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2477938.7500\n",
      "Epoch 66/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2476670.7500\n",
      "Epoch 66: loss improved from 2477938.75000 to 2476981.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2476981.2500\n",
      "Epoch 67/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2474269.2500\n",
      "Epoch 67: loss improved from 2476981.25000 to 2474566.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2474566.0000\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2472793.2500\n",
      "Epoch 68: loss improved from 2474566.00000 to 2471928.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2471928.7500\n",
      "Epoch 69/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2471380.7500\n",
      "Epoch 69: loss improved from 2471928.75000 to 2471391.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2471391.0000\n",
      "Epoch 70/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2470703.5000\n",
      "Epoch 70: loss improved from 2471391.00000 to 2469989.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2469989.2500\n",
      "Epoch 71/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2466078.2500\n",
      "Epoch 71: loss improved from 2469989.25000 to 2466873.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2466873.7500\n",
      "Epoch 72/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2466689.7500\n",
      "Epoch 72: loss improved from 2466873.75000 to 2466777.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2466777.5000\n",
      "Epoch 73/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2466787.7500\n",
      "Epoch 73: loss did not improve from 2466777.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2467305.7500\n",
      "Epoch 74/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2463539.0000\n",
      "Epoch 74: loss improved from 2466777.50000 to 2465448.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2465448.0000\n",
      "Epoch 75/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2463013.0000\n",
      "Epoch 75: loss improved from 2465448.00000 to 2463425.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2463425.5000\n",
      "Epoch 76/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2458983.7500\n",
      "Epoch 76: loss improved from 2463425.50000 to 2461788.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2461788.7500\n",
      "Epoch 77/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2460945.5000\n",
      "Epoch 77: loss improved from 2461788.75000 to 2461094.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2461094.2500\n",
      "Epoch 78/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2458444.5000\n",
      "Epoch 78: loss improved from 2461094.25000 to 2459284.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2459284.5000\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2457370.7500\n",
      "Epoch 79: loss improved from 2459284.50000 to 2457671.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2457671.5000\n",
      "Epoch 80/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2456821.2500\n",
      "Epoch 80: loss did not improve from 2457671.50000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2458161.0000\n",
      "Epoch 81/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2455298.7500\n",
      "Epoch 81: loss improved from 2457671.50000 to 2455236.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2455236.0000\n",
      "Epoch 82/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2454136.2500\n",
      "Epoch 82: loss improved from 2455236.00000 to 2454041.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2454041.5000\n",
      "Epoch 83/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2454036.0000\n",
      "Epoch 83: loss improved from 2454041.50000 to 2453716.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2453716.2500\n",
      "Epoch 84/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2450147.0000\n",
      "Epoch 84: loss improved from 2453716.25000 to 2451062.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2451062.2500\n",
      "Epoch 85/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2450843.0000\n",
      "Epoch 85: loss improved from 2451062.25000 to 2450634.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2450634.5000\n",
      "Epoch 86/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2450965.0000\n",
      "Epoch 86: loss improved from 2450634.50000 to 2450291.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2450291.2500\n",
      "Epoch 87/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2447400.5000\n",
      "Epoch 87: loss improved from 2450291.25000 to 2447577.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2447577.5000\n",
      "Epoch 88/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2448215.2500\n",
      "Epoch 88: loss improved from 2447577.50000 to 2446780.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2446780.0000\n",
      "Epoch 89/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2446080.2500\n",
      "Epoch 89: loss improved from 2446780.00000 to 2445531.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2445531.7500\n",
      "Epoch 90/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2445374.2500\n",
      "Epoch 90: loss improved from 2445531.75000 to 2445190.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2445190.2500\n",
      "Epoch 91/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2444488.0000\n",
      "Epoch 91: loss improved from 2445190.25000 to 2443283.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2443283.2500\n",
      "Epoch 92/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2441762.7500\n",
      "Epoch 92: loss improved from 2443283.25000 to 2441434.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2441434.5000\n",
      "Epoch 93/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2440610.2500\n",
      "Epoch 93: loss improved from 2441434.50000 to 2441107.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2441107.0000\n",
      "Epoch 94/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2440740.5000\n",
      "Epoch 94: loss improved from 2441107.00000 to 2439720.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2439720.0000\n",
      "Epoch 95/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2440469.2500\n",
      "Epoch 95: loss did not improve from 2439720.00000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2440147.5000\n",
      "Epoch 96/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2436652.7500\n",
      "Epoch 96: loss improved from 2439720.00000 to 2437852.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2437852.2500\n",
      "Epoch 97/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2438526.5000\n",
      "Epoch 97: loss did not improve from 2437852.25000\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2437855.5000\n",
      "Epoch 98/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2436016.7500\n",
      "Epoch 98: loss improved from 2437852.25000 to 2436249.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2436249.7500\n",
      "Epoch 99/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2433304.7500\n",
      "Epoch 99: loss improved from 2436249.75000 to 2434332.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2434332.5000\n",
      "Epoch 100/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2433745.0000\n",
      "Epoch 100: loss improved from 2434332.50000 to 2434084.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2434084.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 08:54:31 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 08:54:31 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0j3t08rr\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0j3t08rr\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_151 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   2/3189 [..............................] - ETA: 4:06 - loss: 2908771.5000    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0216s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3180/3189 [============================>.] - ETA: 0s - loss: 2492098.7500\n",
      "Epoch 1: loss improved from inf to 2490334.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 67s 4ms/step - loss: 2490334.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2172377.5000\n",
      "Epoch 2: loss improved from 2490334.50000 to 2172394.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2172394.0000\n",
      "Epoch 3/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2013795.6250\n",
      "Epoch 3: loss improved from 2172394.00000 to 2013390.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2013390.6250\n",
      "Epoch 4/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1902048.0000\n",
      "Epoch 4: loss improved from 2013390.62500 to 1901825.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1901825.0000\n",
      "Epoch 5/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1815171.1250\n",
      "Epoch 5: loss improved from 1901825.00000 to 1815171.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1815171.1250\n",
      "Epoch 6/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1743666.0000\n",
      "Epoch 6: loss improved from 1815171.12500 to 1743567.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1743567.2500\n",
      "Epoch 7/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1681679.0000\n",
      "Epoch 7: loss improved from 1743567.25000 to 1681579.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1681579.8750\n",
      "Epoch 8/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1631756.3750\n",
      "Epoch 8: loss improved from 1681579.87500 to 1631855.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1631855.7500\n",
      "Epoch 9/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1583708.8750\n",
      "Epoch 9: loss improved from 1631855.75000 to 1583477.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1583477.8750\n",
      "Epoch 10/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1549069.7500\n",
      "Epoch 10: loss improved from 1583477.87500 to 1549004.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1549004.6250\n",
      "Epoch 11/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1515883.7500\n",
      "Epoch 11: loss improved from 1549004.62500 to 1515512.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1515512.8750\n",
      "Epoch 12/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1484364.2500\n",
      "Epoch 12: loss improved from 1515512.87500 to 1484491.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1484491.2500\n",
      "Epoch 13/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1459169.8750\n",
      "Epoch 13: loss improved from 1484491.25000 to 1458834.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1458834.5000\n",
      "Epoch 14/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1436601.8750\n",
      "Epoch 14: loss improved from 1458834.50000 to 1436044.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1436044.5000\n",
      "Epoch 15/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1416217.7500\n",
      "Epoch 15: loss improved from 1436044.50000 to 1416356.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1416356.5000\n",
      "Epoch 16/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1399013.7500\n",
      "Epoch 16: loss improved from 1416356.50000 to 1399192.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 1399192.1250\n",
      "Epoch 17/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1378609.3750\n",
      "Epoch 17: loss improved from 1399192.12500 to 1380462.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1380462.6250\n",
      "Epoch 18/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1366760.1250\n",
      "Epoch 18: loss improved from 1380462.62500 to 1365725.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1365725.5000\n",
      "Epoch 19/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1351340.0000\n",
      "Epoch 19: loss improved from 1365725.50000 to 1351681.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1351681.5000\n",
      "Epoch 20/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1335454.8750\n",
      "Epoch 20: loss improved from 1351681.50000 to 1336582.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1336582.1250\n",
      "Epoch 21/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1327464.8750\n",
      "Epoch 21: loss improved from 1336582.12500 to 1327359.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1327359.8750\n",
      "Epoch 22/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1315090.5000\n",
      "Epoch 22: loss improved from 1327359.87500 to 1315287.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1315287.3750\n",
      "Epoch 23/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1303672.2500\n",
      "Epoch 23: loss improved from 1315287.37500 to 1303271.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1303271.0000\n",
      "Epoch 24/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1292233.8750\n",
      "Epoch 24: loss improved from 1303271.00000 to 1292262.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1292262.5000\n",
      "Epoch 25/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1284592.8750\n",
      "Epoch 25: loss improved from 1292262.50000 to 1283932.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1283932.0000\n",
      "Epoch 26/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1275510.8750\n",
      "Epoch 26: loss improved from 1283932.00000 to 1274652.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1274652.8750\n",
      "Epoch 27/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1267807.2500\n",
      "Epoch 27: loss improved from 1274652.87500 to 1267373.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1267373.2500\n",
      "Epoch 28/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1258919.7500\n",
      "Epoch 28: loss improved from 1267373.25000 to 1258630.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1258630.2500\n",
      "Epoch 29/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1251302.0000\n",
      "Epoch 29: loss improved from 1258630.25000 to 1251279.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1251279.2500\n",
      "Epoch 30/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1244794.6250\n",
      "Epoch 30: loss improved from 1251279.25000 to 1244110.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1244110.3750\n",
      "Epoch 31/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1237876.7500\n",
      "Epoch 31: loss improved from 1244110.37500 to 1237854.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1237854.1250\n",
      "Epoch 32/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1229172.8750\n",
      "Epoch 32: loss improved from 1237854.12500 to 1229340.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1229340.3750\n",
      "Epoch 33/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1224365.6250\n",
      "Epoch 33: loss improved from 1229340.37500 to 1224873.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1224873.7500\n",
      "Epoch 34/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1217782.1250\n",
      "Epoch 34: loss improved from 1224873.75000 to 1217942.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1217942.6250\n",
      "Epoch 35/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1209942.0000\n",
      "Epoch 35: loss improved from 1217942.62500 to 1210198.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1210198.2500\n",
      "Epoch 36/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1204267.3750\n",
      "Epoch 36: loss improved from 1210198.25000 to 1204106.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1204106.7500\n",
      "Epoch 37/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1199810.8750\n",
      "Epoch 37: loss improved from 1204106.75000 to 1199850.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1199850.6250\n",
      "Epoch 38/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1194766.8750\n",
      "Epoch 38: loss improved from 1199850.62500 to 1195183.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1195183.1250\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1189315.6250\n",
      "Epoch 39: loss improved from 1195183.12500 to 1189315.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1189315.6250\n",
      "Epoch 40/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1181172.6250\n",
      "Epoch 40: loss improved from 1189315.62500 to 1183024.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1183024.6250\n",
      "Epoch 41/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1177401.3750\n",
      "Epoch 41: loss improved from 1183024.62500 to 1177297.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1177297.8750\n",
      "Epoch 42/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1176090.7500\n",
      "Epoch 42: loss improved from 1177297.87500 to 1175933.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1175933.3750\n",
      "Epoch 43/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1171622.1250\n",
      "Epoch 43: loss improved from 1175933.37500 to 1171433.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1171433.3750\n",
      "Epoch 44/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1166565.1250\n",
      "Epoch 44: loss improved from 1171433.37500 to 1167992.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1167992.3750\n",
      "Epoch 45/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1159475.7500\n",
      "Epoch 45: loss improved from 1167992.37500 to 1158960.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1158960.6250\n",
      "Epoch 46/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1157357.7500\n",
      "Epoch 46: loss improved from 1158960.62500 to 1156730.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1156730.5000\n",
      "Epoch 47/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1154796.5000\n",
      "Epoch 47: loss improved from 1156730.50000 to 1154661.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1154661.7500\n",
      "Epoch 48/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1149300.7500\n",
      "Epoch 48: loss improved from 1154661.75000 to 1150484.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1150484.3750\n",
      "Epoch 49/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1147533.1250\n",
      "Epoch 49: loss improved from 1150484.37500 to 1147228.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1147228.3750\n",
      "Epoch 50/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1141915.2500\n",
      "Epoch 50: loss improved from 1147228.37500 to 1142101.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1142101.1250\n",
      "Epoch 51/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1138110.2500\n",
      "Epoch 51: loss improved from 1142101.12500 to 1137478.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1137478.5000\n",
      "Epoch 52/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1135935.5000\n",
      "Epoch 52: loss improved from 1137478.50000 to 1136291.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1136291.3750\n",
      "Epoch 53/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1130016.6250\n",
      "Epoch 53: loss improved from 1136291.37500 to 1130140.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1130140.2500\n",
      "Epoch 54/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1130470.1250\n",
      "Epoch 54: loss improved from 1130140.25000 to 1130065.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1130065.3750\n",
      "Epoch 55/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1127123.2500\n",
      "Epoch 55: loss improved from 1130065.37500 to 1127321.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1127321.6250\n",
      "Epoch 56/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1123671.7500\n",
      "Epoch 56: loss improved from 1127321.62500 to 1123296.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1123296.0000\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1118589.1250\n",
      "Epoch 57: loss improved from 1123296.00000 to 1118664.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1118664.6250\n",
      "Epoch 58/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1116256.1250\n",
      "Epoch 58: loss improved from 1118664.62500 to 1116188.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1116188.7500\n",
      "Epoch 59/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1112727.2500\n",
      "Epoch 59: loss improved from 1116188.75000 to 1113668.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1113668.0000\n",
      "Epoch 60/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1109203.6250\n",
      "Epoch 60: loss improved from 1113668.00000 to 1109478.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1109478.1250\n",
      "Epoch 61/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1106581.8750\n",
      "Epoch 61: loss improved from 1109478.12500 to 1106883.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1106883.1250\n",
      "Epoch 62/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1105445.0000\n",
      "Epoch 62: loss improved from 1106883.12500 to 1105279.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1105279.0000\n",
      "Epoch 63/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1099632.7500\n",
      "Epoch 63: loss improved from 1105279.00000 to 1099396.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1099396.6250\n",
      "Epoch 64/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1098059.2500\n",
      "Epoch 64: loss improved from 1099396.62500 to 1098007.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1098007.3750\n",
      "Epoch 65/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1097476.6250\n",
      "Epoch 65: loss improved from 1098007.37500 to 1097543.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1097543.1250\n",
      "Epoch 66/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1096437.6250\n",
      "Epoch 66: loss improved from 1097543.12500 to 1096202.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 24s 7ms/step - loss: 1096202.8750\n",
      "Epoch 67/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1092394.5000\n",
      "Epoch 67: loss improved from 1096202.87500 to 1091996.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 1091996.1250\n",
      "Epoch 68/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1092284.0000\n",
      "Epoch 68: loss did not improve from 1091996.12500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1092465.5000\n",
      "Epoch 69/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1088517.5000\n",
      "Epoch 69: loss improved from 1091996.12500 to 1088393.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 19s 6ms/step - loss: 1088393.7500\n",
      "Epoch 70/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1087271.0000\n",
      "Epoch 70: loss improved from 1088393.75000 to 1087018.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1087018.0000\n",
      "Epoch 71/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1085913.5000\n",
      "Epoch 71: loss improved from 1087018.00000 to 1085288.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1085288.3750\n",
      "Epoch 72/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1082696.2500\n",
      "Epoch 72: loss improved from 1085288.37500 to 1082585.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1082585.7500\n",
      "Epoch 73/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1080433.6250\n",
      "Epoch 73: loss improved from 1082585.75000 to 1079958.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1079958.5000\n",
      "Epoch 74/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1076701.8750\n",
      "Epoch 74: loss improved from 1079958.50000 to 1076828.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1076828.2500\n",
      "Epoch 75/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1075884.1250\n",
      "Epoch 75: loss improved from 1076828.25000 to 1075839.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1075839.2500\n",
      "Epoch 76/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1071677.3750\n",
      "Epoch 76: loss improved from 1075839.25000 to 1072478.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1072478.7500\n",
      "Epoch 77/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1072389.5000\n",
      "Epoch 77: loss improved from 1072478.75000 to 1071959.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1071959.5000\n",
      "Epoch 78/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1071490.7500\n",
      "Epoch 78: loss improved from 1071959.50000 to 1071516.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1071516.0000\n",
      "Epoch 79/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1067196.6250\n",
      "Epoch 79: loss improved from 1071516.00000 to 1067099.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1067099.3750\n",
      "Epoch 80/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 1065093.0000\n",
      "Epoch 80: loss improved from 1067099.37500 to 1064785.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1064785.0000\n",
      "Epoch 81/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1062386.3750\n",
      "Epoch 81: loss improved from 1064785.00000 to 1063997.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1063997.8750\n",
      "Epoch 82/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1061925.6250\n",
      "Epoch 82: loss improved from 1063997.87500 to 1062317.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1062317.7500\n",
      "Epoch 83/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1060160.3750\n",
      "Epoch 83: loss improved from 1062317.75000 to 1059882.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1059882.0000\n",
      "Epoch 84/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1058103.6250\n",
      "Epoch 84: loss improved from 1059882.00000 to 1057944.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1057944.1250\n",
      "Epoch 85/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1058032.8750\n",
      "Epoch 85: loss did not improve from 1057944.12500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1057986.5000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1058202.0000\n",
      "Epoch 86: loss did not improve from 1057944.12500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1058202.0000\n",
      "Epoch 87/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1053124.1250\n",
      "Epoch 87: loss improved from 1057944.12500 to 1053044.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1053044.0000\n",
      "Epoch 88/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1053193.6250\n",
      "Epoch 88: loss did not improve from 1053044.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1053178.0000\n",
      "Epoch 89/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1050823.3750\n",
      "Epoch 89: loss improved from 1053044.00000 to 1051366.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1051366.6250\n",
      "Epoch 90/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1049506.5000\n",
      "Epoch 90: loss improved from 1051366.62500 to 1050373.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1050373.6250\n",
      "Epoch 91/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1049246.0000\n",
      "Epoch 91: loss improved from 1050373.62500 to 1049108.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1049108.0000\n",
      "Epoch 92/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1047152.8750\n",
      "Epoch 92: loss improved from 1049108.00000 to 1047118.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1047118.5000\n",
      "Epoch 93/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1042762.6875\n",
      "Epoch 93: loss improved from 1047118.50000 to 1042930.81250, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1042930.8125\n",
      "Epoch 94/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1043505.9375\n",
      "Epoch 94: loss did not improve from 1042930.81250\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1043634.5000\n",
      "Epoch 95/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1040299.8125\n",
      "Epoch 95: loss improved from 1042930.81250 to 1041123.18750, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1041123.1875\n",
      "Epoch 96/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1040647.0000\n",
      "Epoch 96: loss improved from 1041123.18750 to 1040519.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1040519.1250\n",
      "Epoch 97/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1037906.3125\n",
      "Epoch 97: loss improved from 1040519.12500 to 1037751.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1037751.6250\n",
      "Epoch 98/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1038860.9375\n",
      "Epoch 98: loss did not improve from 1037751.62500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1038657.9375\n",
      "Epoch 99/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1035472.8125\n",
      "Epoch 99: loss improved from 1037751.62500 to 1035480.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1035480.2500\n",
      "Epoch 100/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1033418.3125\n",
      "Epoch 100: loss improved from 1035480.25000 to 1033604.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1033604.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 09:16:02 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 09:16:02 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp9d5h88vu\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp9d5h88vu\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_155 (Dense)           (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   6/3189 [..............................] - ETA: 1:57 - loss: 5000311.5000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0244s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3177/3189 [============================>.] - ETA: 0s - loss: 2740867.0000\n",
      "Epoch 1: loss improved from inf to 2742079.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2742079.5000\n",
      "Epoch 2/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2730893.0000\n",
      "Epoch 2: loss improved from 2742079.50000 to 2730907.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2730907.0000\n",
      "Epoch 3/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2722665.7500\n",
      "Epoch 3: loss improved from 2730907.00000 to 2722665.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2722665.7500\n",
      "Epoch 4/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2713851.2500\n",
      "Epoch 4: loss improved from 2722665.75000 to 2713389.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2713389.2500\n",
      "Epoch 5/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2702796.2500\n",
      "Epoch 5: loss improved from 2713389.25000 to 2702418.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2702418.7500\n",
      "Epoch 6/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2692640.2500\n",
      "Epoch 6: loss improved from 2702418.75000 to 2692021.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2692021.0000\n",
      "Epoch 7/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2683032.2500\n",
      "Epoch 7: loss improved from 2692021.00000 to 2681494.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2681494.5000\n",
      "Epoch 8/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2671854.7500\n",
      "Epoch 8: loss improved from 2681494.50000 to 2671668.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2671668.2500\n",
      "Epoch 9/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2659646.7500\n",
      "Epoch 9: loss improved from 2671668.25000 to 2661593.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2661593.2500\n",
      "Epoch 10/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2654799.0000\n",
      "Epoch 10: loss improved from 2661593.25000 to 2654799.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2654799.0000\n",
      "Epoch 11/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2644282.5000\n",
      "Epoch 11: loss improved from 2654799.00000 to 2645417.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2645417.7500\n",
      "Epoch 12/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2638505.2500\n",
      "Epoch 12: loss improved from 2645417.75000 to 2638430.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2638430.0000\n",
      "Epoch 13/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2632201.7500\n",
      "Epoch 13: loss improved from 2638430.00000 to 2632150.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2632150.5000\n",
      "Epoch 14/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2624758.5000\n",
      "Epoch 14: loss improved from 2632150.50000 to 2625596.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2625596.0000\n",
      "Epoch 15/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2620814.7500\n",
      "Epoch 15: loss improved from 2625596.00000 to 2621316.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2621316.2500\n",
      "Epoch 16/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2615545.5000\n",
      "Epoch 16: loss improved from 2621316.25000 to 2615688.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2615688.7500\n",
      "Epoch 17/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2609488.0000\n",
      "Epoch 17: loss improved from 2615688.75000 to 2609658.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2609658.7500\n",
      "Epoch 18/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2606560.5000\n",
      "Epoch 18: loss improved from 2609658.75000 to 2606093.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2606093.5000\n",
      "Epoch 19/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2599352.0000\n",
      "Epoch 19: loss improved from 2606093.50000 to 2598870.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2598870.2500\n",
      "Epoch 20/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2594218.0000\n",
      "Epoch 20: loss improved from 2598870.25000 to 2594369.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2594369.5000\n",
      "Epoch 21/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2586428.2500\n",
      "Epoch 21: loss improved from 2594369.50000 to 2589606.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2589606.5000\n",
      "Epoch 22/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2585265.2500\n",
      "Epoch 22: loss improved from 2589606.50000 to 2585672.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2585672.5000\n",
      "Epoch 23/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2581118.2500\n",
      "Epoch 23: loss improved from 2585672.50000 to 2580579.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2580579.7500\n",
      "Epoch 24/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2576080.2500\n",
      "Epoch 24: loss improved from 2580579.75000 to 2576167.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2576167.0000\n",
      "Epoch 25/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2572053.0000\n",
      "Epoch 25: loss improved from 2576167.00000 to 2571974.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2571974.7500\n",
      "Epoch 26/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2568058.2500\n",
      "Epoch 26: loss improved from 2571974.75000 to 2566866.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2566866.0000\n",
      "Epoch 27/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2564664.2500\n",
      "Epoch 27: loss improved from 2566866.00000 to 2563773.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2563773.5000\n",
      "Epoch 28/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2559874.0000\n",
      "Epoch 28: loss improved from 2563773.50000 to 2560175.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2560175.2500\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2556762.0000\n",
      "Epoch 29: loss improved from 2560175.25000 to 2556762.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2556762.0000\n",
      "Epoch 30/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2550582.7500\n",
      "Epoch 30: loss improved from 2556762.00000 to 2551152.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2551152.7500\n",
      "Epoch 31/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2548436.5000\n",
      "Epoch 31: loss improved from 2551152.75000 to 2547715.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2547715.2500\n",
      "Epoch 32/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2548869.5000\n",
      "Epoch 32: loss did not improve from 2547715.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2548447.5000\n",
      "Epoch 33/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2543602.2500\n",
      "Epoch 33: loss improved from 2547715.25000 to 2544980.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2544980.2500\n",
      "Epoch 34/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2542531.0000\n",
      "Epoch 34: loss improved from 2544980.25000 to 2541514.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2541514.0000\n",
      "Epoch 35/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2536092.7500\n",
      "Epoch 35: loss improved from 2541514.00000 to 2536265.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 23s 7ms/step - loss: 2536265.0000\n",
      "Epoch 36/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2535829.2500\n",
      "Epoch 36: loss improved from 2536265.00000 to 2535321.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2535321.5000\n",
      "Epoch 37/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2532310.5000\n",
      "Epoch 37: loss improved from 2535321.50000 to 2533872.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2533872.2500\n",
      "Epoch 38/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2529981.0000\n",
      "Epoch 38: loss improved from 2533872.25000 to 2530850.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2530850.2500\n",
      "Epoch 39/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2526231.5000\n",
      "Epoch 39: loss improved from 2530850.25000 to 2526471.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2526471.5000\n",
      "Epoch 40/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2525717.7500\n",
      "Epoch 40: loss improved from 2526471.50000 to 2525803.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525803.5000\n",
      "Epoch 41/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2526101.2500\n",
      "Epoch 41: loss improved from 2525803.50000 to 2525661.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2525661.2500\n",
      "Epoch 42/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2523557.0000\n",
      "Epoch 42: loss improved from 2525661.25000 to 2524040.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2524040.7500\n",
      "Epoch 43/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2519247.7500\n",
      "Epoch 43: loss improved from 2524040.75000 to 2519466.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2519466.2500\n",
      "Epoch 44/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2519231.0000\n",
      "Epoch 44: loss improved from 2519466.25000 to 2518954.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2518954.2500\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2517181.2500\n",
      "Epoch 45: loss improved from 2518954.25000 to 2517181.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2517181.2500\n",
      "Epoch 46/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2515688.5000\n",
      "Epoch 46: loss improved from 2517181.25000 to 2514840.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2514840.0000\n",
      "Epoch 47/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2514720.0000\n",
      "Epoch 47: loss did not improve from 2514840.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2515664.7500\n",
      "Epoch 48/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2512887.2500\n",
      "Epoch 48: loss improved from 2514840.00000 to 2513109.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2513109.5000\n",
      "Epoch 49/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2511348.7500\n",
      "Epoch 49: loss improved from 2513109.50000 to 2511258.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2511258.0000\n",
      "Epoch 50/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2512285.5000\n",
      "Epoch 50: loss did not improve from 2511258.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2512379.2500\n",
      "Epoch 51/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2509464.7500\n",
      "Epoch 51: loss improved from 2511258.00000 to 2509758.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509758.7500\n",
      "Epoch 52/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2510429.0000\n",
      "Epoch 52: loss improved from 2509758.75000 to 2509064.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509064.0000\n",
      "Epoch 53/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2508939.0000\n",
      "Epoch 53: loss did not improve from 2509064.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2509242.2500\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2507603.0000\n",
      "Epoch 54: loss improved from 2509064.00000 to 2507631.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2507631.7500\n",
      "Epoch 55/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2505484.0000\n",
      "Epoch 55: loss improved from 2507631.75000 to 2505597.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2505597.2500\n",
      "Epoch 56/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2503041.0000\n",
      "Epoch 56: loss improved from 2505597.25000 to 2503597.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2503597.0000\n",
      "Epoch 57/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2505594.0000\n",
      "Epoch 57: loss did not improve from 2503597.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2505479.7500\n",
      "Epoch 58/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2502868.2500\n",
      "Epoch 58: loss improved from 2503597.00000 to 2503407.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2503407.2500\n",
      "Epoch 59/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2503537.7500\n",
      "Epoch 59: loss improved from 2503407.25000 to 2502670.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2502670.5000\n",
      "Epoch 60/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2501615.5000\n",
      "Epoch 60: loss did not improve from 2502670.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2503521.2500\n",
      "Epoch 61/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2499644.5000\n",
      "Epoch 61: loss improved from 2502670.50000 to 2500581.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2500581.7500\n",
      "Epoch 62/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2499738.7500\n",
      "Epoch 62: loss improved from 2500581.75000 to 2500441.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2500441.7500\n",
      "Epoch 63/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2499116.7500\n",
      "Epoch 63: loss improved from 2500441.75000 to 2499188.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2499188.7500\n",
      "Epoch 64/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2497379.0000\n",
      "Epoch 64: loss improved from 2499188.75000 to 2498296.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2498296.2500\n",
      "Epoch 65/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2497672.5000\n",
      "Epoch 65: loss did not improve from 2498296.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2498419.2500\n",
      "Epoch 66/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2498261.5000\n",
      "Epoch 66: loss improved from 2498296.25000 to 2495977.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2495977.5000\n",
      "Epoch 67/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2497812.2500\n",
      "Epoch 67: loss did not improve from 2495977.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2496936.7500\n",
      "Epoch 68/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2494618.7500\n",
      "Epoch 68: loss improved from 2495977.50000 to 2494987.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2494987.5000\n",
      "Epoch 69/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2495800.7500\n",
      "Epoch 69: loss did not improve from 2494987.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2496021.7500\n",
      "Epoch 70/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2495613.7500\n",
      "Epoch 70: loss did not improve from 2494987.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2496359.2500\n",
      "Epoch 71/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2494298.2500\n",
      "Epoch 71: loss improved from 2494987.50000 to 2494708.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2494708.0000\n",
      "Epoch 72/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2496695.0000\n",
      "Epoch 72: loss did not improve from 2494708.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2495770.0000\n",
      "Epoch 73/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2495900.0000\n",
      "Epoch 73: loss did not improve from 2494708.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2495850.7500\n",
      "Epoch 74/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2492890.5000\n",
      "Epoch 74: loss improved from 2494708.00000 to 2492452.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2492452.7500\n",
      "Epoch 75/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2494341.0000\n",
      "Epoch 75: loss improved from 2492452.75000 to 2491862.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491862.0000\n",
      "Epoch 76/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2493009.0000\n",
      "Epoch 76: loss did not improve from 2491862.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2492145.2500\n",
      "Epoch 77/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2490390.7500\n",
      "Epoch 77: loss improved from 2491862.00000 to 2491324.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491324.2500\n",
      "Epoch 78/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2489592.5000\n",
      "Epoch 78: loss improved from 2491324.25000 to 2491096.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491096.7500\n",
      "Epoch 79/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2488854.0000\n",
      "Epoch 79: loss did not improve from 2491096.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491265.5000\n",
      "Epoch 80/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2492033.0000\n",
      "Epoch 80: loss did not improve from 2491096.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491934.5000\n",
      "Epoch 81/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2492384.2500\n",
      "Epoch 81: loss did not improve from 2491096.75000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2492429.0000\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2491065.5000\n",
      "Epoch 82: loss improved from 2491096.75000 to 2491065.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2491065.5000\n",
      "Epoch 83/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2488175.2500\n",
      "Epoch 83: loss improved from 2491065.50000 to 2489561.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2489561.2500\n",
      "Epoch 84/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2491245.5000\n",
      "Epoch 84: loss did not improve from 2489561.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2490294.2500\n",
      "Epoch 85/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2486395.7500\n",
      "Epoch 85: loss improved from 2489561.25000 to 2486448.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2486448.5000\n",
      "Epoch 86/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2487833.5000\n",
      "Epoch 86: loss did not improve from 2486448.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2487525.0000\n",
      "Epoch 87/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2486477.7500\n",
      "Epoch 87: loss did not improve from 2486448.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2487378.0000\n",
      "Epoch 88/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2487053.5000\n",
      "Epoch 88: loss did not improve from 2486448.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2488123.5000\n",
      "Epoch 89/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2485827.0000\n",
      "Epoch 89: loss did not improve from 2486448.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2486616.0000\n",
      "Epoch 90/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2484874.7500\n",
      "Epoch 90: loss improved from 2486448.50000 to 2484201.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2484201.0000\n",
      "Epoch 91/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2487592.5000\n",
      "Epoch 91: loss did not improve from 2484201.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2485041.2500\n",
      "Epoch 92/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2485037.7500\n",
      "Epoch 92: loss did not improve from 2484201.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2485441.2500\n",
      "Epoch 93/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2484863.2500\n",
      "Epoch 93: loss did not improve from 2484201.00000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2484419.2500\n",
      "Epoch 94/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2482500.2500\n",
      "Epoch 94: loss improved from 2484201.00000 to 2483496.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2483496.5000\n",
      "Epoch 95/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2485154.5000\n",
      "Epoch 95: loss did not improve from 2483496.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2484965.7500\n",
      "Epoch 96/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2483790.7500\n",
      "Epoch 96: loss improved from 2483496.50000 to 2483189.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2483189.0000\n",
      "Epoch 97/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2482010.5000\n",
      "Epoch 97: loss improved from 2483189.00000 to 2481995.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2481995.2500\n",
      "Epoch 98/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2482495.5000\n",
      "Epoch 98: loss did not improve from 2481995.25000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2482466.7500\n",
      "Epoch 99/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2481950.7500\n",
      "Epoch 99: loss improved from 2481995.25000 to 2481635.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2481635.2500\n",
      "Epoch 100/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2481331.2500\n",
      "Epoch 100: loss improved from 2481635.25000 to 2481425.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2481425.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 09:36:27 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 09:36:27 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpwob7zk0s\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpwob7zk0s\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_159 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_160 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   4/3189 [..............................] - ETA: 1:01 - loss: 4607608.5000   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0091s vs `on_train_batch_end` time: 0.0291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0091s vs `on_train_batch_end` time: 0.0291s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183/3189 [============================>.] - ETA: 0s - loss: 2688613.7500\n",
      "Epoch 1: loss improved from inf to 2688449.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 15s 3ms/step - loss: 2688449.7500\n",
      "Epoch 2/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2597669.7500\n",
      "Epoch 2: loss improved from 2688449.75000 to 2598079.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2598079.0000\n",
      "Epoch 3/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2534536.0000\n",
      "Epoch 3: loss improved from 2598079.00000 to 2535251.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2535251.5000\n",
      "Epoch 4/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2491567.5000\n",
      "Epoch 4: loss improved from 2535251.50000 to 2489790.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2489790.0000\n",
      "Epoch 5/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2452731.7500\n",
      "Epoch 5: loss improved from 2489790.00000 to 2454005.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2454005.5000\n",
      "Epoch 6/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2420832.0000\n",
      "Epoch 6: loss improved from 2454005.50000 to 2420120.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2420120.7500\n",
      "Epoch 7/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2390480.5000\n",
      "Epoch 7: loss improved from 2420120.75000 to 2389174.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2389174.0000\n",
      "Epoch 8/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2360899.0000\n",
      "Epoch 8: loss improved from 2389174.00000 to 2360899.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2360899.0000\n",
      "Epoch 9/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2335179.2500\n",
      "Epoch 9: loss improved from 2360899.00000 to 2335130.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2335130.2500\n",
      "Epoch 10/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2315201.7500\n",
      "Epoch 10: loss improved from 2335130.25000 to 2314802.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2314802.5000\n",
      "Epoch 11/100\n",
      "3174/3189 [============================>.] - ETA: 0s - loss: 2293483.0000\n",
      "Epoch 11: loss improved from 2314802.50000 to 2295283.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2295283.0000\n",
      "Epoch 12/100\n",
      "3175/3189 [============================>.] - ETA: 0s - loss: 2275554.2500\n",
      "Epoch 12: loss improved from 2295283.00000 to 2275681.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2275681.7500\n",
      "Epoch 13/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2257365.5000\n",
      "Epoch 13: loss improved from 2275681.75000 to 2257563.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2257563.7500\n",
      "Epoch 14/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2241712.0000\n",
      "Epoch 14: loss improved from 2257563.75000 to 2240889.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2240889.2500\n",
      "Epoch 15/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2229905.5000\n",
      "Epoch 15: loss improved from 2240889.25000 to 2227277.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2227277.2500\n",
      "Epoch 16/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2212966.2500\n",
      "Epoch 16: loss improved from 2227277.25000 to 2212838.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2212838.2500\n",
      "Epoch 17/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2200909.0000\n",
      "Epoch 17: loss improved from 2212838.25000 to 2200545.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2200545.5000\n",
      "Epoch 18/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2189601.5000\n",
      "Epoch 18: loss improved from 2200545.50000 to 2189385.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2189385.2500\n",
      "Epoch 19/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2178493.0000\n",
      "Epoch 19: loss improved from 2189385.25000 to 2178642.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2178642.0000\n",
      "Epoch 20/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2165905.7500\n",
      "Epoch 20: loss improved from 2178642.00000 to 2166107.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2166107.7500\n",
      "Epoch 21/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2159144.0000\n",
      "Epoch 21: loss improved from 2166107.75000 to 2158823.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2158823.7500\n",
      "Epoch 22/100\n",
      "3173/3189 [============================>.] - ETA: 0s - loss: 2146992.7500\n",
      "Epoch 22: loss improved from 2158823.75000 to 2148081.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2148081.2500\n",
      "Epoch 23/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2138309.2500\n",
      "Epoch 23: loss improved from 2148081.25000 to 2140080.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2140080.0000\n",
      "Epoch 24/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2133915.2500\n",
      "Epoch 24: loss improved from 2140080.00000 to 2132543.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2132543.7500\n",
      "Epoch 25/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2122603.5000\n",
      "Epoch 25: loss improved from 2132543.75000 to 2122734.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2122734.5000\n",
      "Epoch 26/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2119063.7500\n",
      "Epoch 26: loss improved from 2122734.50000 to 2117776.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2117776.2500\n",
      "Epoch 27/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2108160.7500\n",
      "Epoch 27: loss improved from 2117776.25000 to 2109652.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2109652.5000\n",
      "Epoch 28/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2102782.0000\n",
      "Epoch 28: loss improved from 2109652.50000 to 2103978.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2103978.7500\n",
      "Epoch 29/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2096567.6250\n",
      "Epoch 29: loss improved from 2103978.75000 to 2096909.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2096909.5000\n",
      "Epoch 30/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2092016.7500\n",
      "Epoch 30: loss improved from 2096909.50000 to 2091152.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2091152.0000\n",
      "Epoch 31/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2083473.6250\n",
      "Epoch 31: loss improved from 2091152.00000 to 2083628.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2083628.8750\n",
      "Epoch 32/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2079298.8750\n",
      "Epoch 32: loss improved from 2083628.87500 to 2079415.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2079415.6250\n",
      "Epoch 33/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2072706.1250\n",
      "Epoch 33: loss improved from 2079415.62500 to 2072646.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2072646.1250\n",
      "Epoch 34/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 2068057.2500\n",
      "Epoch 34: loss improved from 2072646.12500 to 2067413.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2067413.0000\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2063402.6250\n",
      "Epoch 35: loss improved from 2067413.00000 to 2063402.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2063402.6250\n",
      "Epoch 36/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2057613.3750\n",
      "Epoch 36: loss improved from 2063402.62500 to 2058576.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2058576.5000\n",
      "Epoch 37/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2052693.7500\n",
      "Epoch 37: loss improved from 2058576.50000 to 2053518.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2053518.1250\n",
      "Epoch 38/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 2049932.8750\n",
      "Epoch 38: loss improved from 2053518.12500 to 2049082.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2049082.1250\n",
      "Epoch 39/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2047322.2500\n",
      "Epoch 39: loss improved from 2049082.12500 to 2046069.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2046069.6250\n",
      "Epoch 40/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 2040495.6250\n",
      "Epoch 40: loss improved from 2046069.62500 to 2040664.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2040664.7500\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2036216.6250\n",
      "Epoch 41: loss improved from 2040664.75000 to 2036216.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2036216.6250\n",
      "Epoch 42/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2032657.6250\n",
      "Epoch 42: loss improved from 2036216.62500 to 2032668.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2032668.0000\n",
      "Epoch 43/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2029901.7500\n",
      "Epoch 43: loss improved from 2032668.00000 to 2028660.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2028660.5000\n",
      "Epoch 44/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2024230.6250\n",
      "Epoch 44: loss improved from 2028660.50000 to 2023643.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2023643.1250\n",
      "Epoch 45/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 2020804.8750\n",
      "Epoch 45: loss improved from 2023643.12500 to 2020354.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2020354.5000\n",
      "Epoch 46/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2018301.6250\n",
      "Epoch 46: loss improved from 2020354.50000 to 2017568.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2017568.7500\n",
      "Epoch 47/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2014114.6250\n",
      "Epoch 47: loss improved from 2017568.75000 to 2014057.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2014057.6250\n",
      "Epoch 48/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2010640.0000\n",
      "Epoch 48: loss improved from 2014057.62500 to 2009960.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2009960.7500\n",
      "Epoch 49/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 2006132.7500\n",
      "Epoch 49: loss improved from 2009960.75000 to 2008105.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2008105.2500\n",
      "Epoch 50/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2003622.5000\n",
      "Epoch 50: loss improved from 2008105.25000 to 2003876.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2003876.6250\n",
      "Epoch 51/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2002407.7500\n",
      "Epoch 51: loss improved from 2003876.62500 to 2001284.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2001284.0000\n",
      "Epoch 52/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1996448.6250\n",
      "Epoch 52: loss improved from 2001284.00000 to 1996717.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1996717.2500\n",
      "Epoch 53/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1994147.5000\n",
      "Epoch 53: loss improved from 1996717.25000 to 1994177.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1994177.0000\n",
      "Epoch 54/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1992002.8750\n",
      "Epoch 54: loss improved from 1994177.00000 to 1991562.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 1991562.1250\n",
      "Epoch 55/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1987012.1250\n",
      "Epoch 55: loss improved from 1991562.12500 to 1988460.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1988460.0000\n",
      "Epoch 56/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1984770.6250\n",
      "Epoch 56: loss improved from 1988460.00000 to 1985227.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1985227.3750\n",
      "Epoch 57/100\n",
      "3185/3189 [============================>.] - ETA: 0s - loss: 1982924.2500\n",
      "Epoch 57: loss improved from 1985227.37500 to 1982418.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1982418.1250\n",
      "Epoch 58/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1982029.1250\n",
      "Epoch 58: loss improved from 1982418.12500 to 1982154.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1982154.1250\n",
      "Epoch 59/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1978609.0000\n",
      "Epoch 59: loss improved from 1982154.12500 to 1978609.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1978609.0000\n",
      "Epoch 60/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1976548.1250\n",
      "Epoch 60: loss improved from 1978609.00000 to 1976307.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1976307.3750\n",
      "Epoch 61/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1974021.5000\n",
      "Epoch 61: loss improved from 1976307.37500 to 1973420.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1973420.1250\n",
      "Epoch 62/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1972320.1250\n",
      "Epoch 62: loss improved from 1973420.12500 to 1971926.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1971926.8750\n",
      "Epoch 63/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1967612.7500\n",
      "Epoch 63: loss improved from 1971926.87500 to 1968455.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1968455.0000\n",
      "Epoch 64/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1968148.8750\n",
      "Epoch 64: loss improved from 1968455.00000 to 1967268.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1967268.2500\n",
      "Epoch 65/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1962776.3750\n",
      "Epoch 65: loss improved from 1967268.25000 to 1963791.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1963791.8750\n",
      "Epoch 66/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1960877.2500\n",
      "Epoch 66: loss improved from 1963791.87500 to 1960862.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1960862.1250\n",
      "Epoch 67/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1959269.1250\n",
      "Epoch 67: loss improved from 1960862.12500 to 1959173.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1959173.3750\n",
      "Epoch 68/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1956500.3750\n",
      "Epoch 68: loss improved from 1959173.37500 to 1957124.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1957124.7500\n",
      "Epoch 69/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1954648.7500\n",
      "Epoch 69: loss improved from 1957124.75000 to 1955483.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1955483.8750\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1953034.1250\n",
      "Epoch 70: loss improved from 1955483.87500 to 1953034.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1953034.1250\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1952503.8750\n",
      "Epoch 71: loss improved from 1953034.12500 to 1952503.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1952503.8750\n",
      "Epoch 72/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1949619.3750\n",
      "Epoch 72: loss improved from 1952503.87500 to 1949781.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1949781.2500\n",
      "Epoch 73/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1947514.7500\n",
      "Epoch 73: loss improved from 1949781.25000 to 1947778.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1947778.0000\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1945973.7500\n",
      "Epoch 74: loss improved from 1947778.00000 to 1945973.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1945973.7500\n",
      "Epoch 75/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1944573.5000\n",
      "Epoch 75: loss improved from 1945973.75000 to 1944628.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1944628.8750\n",
      "Epoch 76/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1942456.7500\n",
      "Epoch 76: loss improved from 1944628.87500 to 1942740.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1942740.3750\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 1940717.5000\n",
      "Epoch 77: loss improved from 1942740.37500 to 1940717.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1940717.5000\n",
      "Epoch 78/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1940652.2500\n",
      "Epoch 78: loss did not improve from 1940717.50000\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1940811.6250\n",
      "Epoch 79/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1935786.2500\n",
      "Epoch 79: loss improved from 1940717.50000 to 1935825.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1935825.7500\n",
      "Epoch 80/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1935998.7500\n",
      "Epoch 80: loss improved from 1935825.75000 to 1935439.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1935439.3750\n",
      "Epoch 81/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1933649.1250\n",
      "Epoch 81: loss improved from 1935439.37500 to 1933506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1933506.0000\n",
      "Epoch 82/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1933123.5000\n",
      "Epoch 82: loss improved from 1933506.00000 to 1932682.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 5ms/step - loss: 1932682.7500\n",
      "Epoch 83/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1930701.6250\n",
      "Epoch 83: loss improved from 1932682.75000 to 1931554.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1931554.1250\n",
      "Epoch 84/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1928319.7500\n",
      "Epoch 84: loss improved from 1931554.12500 to 1928150.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1928150.5000\n",
      "Epoch 85/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1926803.2500\n",
      "Epoch 85: loss improved from 1928150.50000 to 1926809.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1926809.5000\n",
      "Epoch 86/100\n",
      "3184/3189 [============================>.] - ETA: 0s - loss: 1926778.7500\n",
      "Epoch 86: loss improved from 1926809.50000 to 1926746.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1926746.7500\n",
      "Epoch 87/100\n",
      "3183/3189 [============================>.] - ETA: 0s - loss: 1923953.8750\n",
      "Epoch 87: loss improved from 1926746.75000 to 1923699.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1923699.3750\n",
      "Epoch 88/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 1922538.5000\n",
      "Epoch 88: loss improved from 1923699.37500 to 1922549.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 1922549.1250\n",
      "Epoch 89/100\n",
      "3177/3189 [============================>.] - ETA: 0s - loss: 1920721.6250\n",
      "Epoch 89: loss improved from 1922549.12500 to 1921228.62500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1921228.6250\n",
      "Epoch 90/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1919337.7500\n",
      "Epoch 90: loss improved from 1921228.62500 to 1918905.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 1918905.8750\n",
      "Epoch 91/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1917422.1250\n",
      "Epoch 91: loss improved from 1918905.87500 to 1916861.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1916861.3750\n",
      "Epoch 92/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 1917683.1250\n",
      "Epoch 92: loss did not improve from 1916861.37500\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1917110.7500\n",
      "Epoch 93/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1915662.6250\n",
      "Epoch 93: loss improved from 1916861.37500 to 1914861.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1914861.8750\n",
      "Epoch 94/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 1913519.6250\n",
      "Epoch 94: loss improved from 1914861.87500 to 1912834.12500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1912834.1250\n",
      "Epoch 95/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 1912545.1250\n",
      "Epoch 95: loss improved from 1912834.12500 to 1912676.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1912676.2500\n",
      "Epoch 96/100\n",
      "3186/3189 [============================>.] - ETA: 0s - loss: 1911290.5000\n",
      "Epoch 96: loss improved from 1912676.25000 to 1911151.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1911151.0000\n",
      "Epoch 97/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 1909618.3750\n",
      "Epoch 97: loss improved from 1911151.00000 to 1909335.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 1909335.3750\n",
      "Epoch 98/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 1908108.7500\n",
      "Epoch 98: loss improved from 1909335.37500 to 1908730.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1908730.0000\n",
      "Epoch 99/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 1907395.5000\n",
      "Epoch 99: loss improved from 1908730.00000 to 1907623.87500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1907623.8750\n",
      "Epoch 100/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 1907192.1250\n",
      "Epoch 100: loss improved from 1907623.87500 to 1906601.37500, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 1906601.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 09:57:35 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 09:57:35 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_8bgncsy\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_8bgncsy\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_167 (Dense)           (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "   1/3189 [..............................] - ETA: 2:12:45 - loss: 3587569.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0170s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0170s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2733883.5000\n",
      "Epoch 1: loss improved from inf to 2733883.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 22s 6ms/step - loss: 2733883.5000\n",
      "Epoch 2/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2718458.0000\n",
      "Epoch 2: loss improved from 2733883.50000 to 2721411.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2721411.7500\n",
      "Epoch 3/100\n",
      "3178/3189 [============================>.] - ETA: 0s - loss: 2702427.5000\n",
      "Epoch 3: loss improved from 2721411.75000 to 2703988.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 14s 4ms/step - loss: 2703988.5000\n",
      "Epoch 4/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2691040.5000\n",
      "Epoch 4: loss improved from 2703988.50000 to 2690043.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2690043.2500\n",
      "Epoch 5/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2678784.7500\n",
      "Epoch 5: loss improved from 2690043.25000 to 2678939.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2678939.0000\n",
      "Epoch 6/100\n",
      "3182/3189 [============================>.] - ETA: 0s - loss: 2670106.7500\n",
      "Epoch 6: loss improved from 2678939.00000 to 2669967.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2669967.7500\n",
      "Epoch 7/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2659933.0000\n",
      "Epoch 7: loss improved from 2669967.75000 to 2662699.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2662699.2500\n",
      "Epoch 8/100\n",
      "3176/3189 [============================>.] - ETA: 0s - loss: 2653547.5000\n",
      "Epoch 8: loss improved from 2662699.25000 to 2653711.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2653711.7500\n",
      "Epoch 9/100\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2645895.2500\n",
      "Epoch 9: loss improved from 2653711.75000 to 2645578.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2645578.0000\n",
      "Epoch 10/100\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2638647.7500\n",
      "Epoch 10: loss improved from 2645578.00000 to 2638712.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2638712.5000\n",
      "Epoch 11/100\n",
      "3180/3189 [============================>.] - ETA: 0s - loss: 2629644.7500\n",
      "Epoch 11: loss improved from 2638712.50000 to 2632204.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2632204.2500\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2627085.7500\n",
      "Epoch 12: loss improved from 2632204.25000 to 2627085.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2627085.7500\n",
      "Epoch 13/100\n",
      "3179/3189 [============================>.] - ETA: 0s - loss: 2621130.0000\n",
      "Epoch 13: loss improved from 2627085.75000 to 2621397.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2621397.0000\n",
      "Epoch 14/100\n",
      "3181/3189 [============================>.] - ETA: 0s - loss: 2615215.7500\n",
      "Epoch 14: loss improved from 2621397.00000 to 2615710.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 13s 4ms/step - loss: 2615710.7500\n",
      "Epoch 15/100\n",
      " 181/3189 [>.............................] - ETA: 13s - loss: 2580266.5000"
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "MODELS=[\"dnn3\",\"dnn4\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=3\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\"vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_73 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_31 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_45 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_52 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_24  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,804,690\n",
      "Trainable params: 3,090,002\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736509.5000\n",
      "Epoch 1: loss improved from inf to 2736509.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 253s 79ms/step - loss: 2736509.5000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 2: loss improved from 2736509.50000 to 2736506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736506.0000\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 3: loss improved from 2736506.00000 to 2736505.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.7500\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 4: loss did not improve from 2736505.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736506.7500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 5: loss improved from 2736505.75000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736503.7500\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 6: loss improved from 2736503.75000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736503.0000\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 7: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.0000\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 8: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.0000\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 9: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.0000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.5000\n",
      "Epoch 10: loss improved from 2736503.00000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736501.5000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 11: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.0000\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 12: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 13: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 14: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736506.2500\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 15: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.0000\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 16: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736502.5000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 17: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 18: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736502.7500\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 19: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.7500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 20: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 03:02:46 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 03:02:46 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0mm09674\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0mm09674\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_76 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_32 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_46 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_25  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,804,690\n",
      "Trainable params: 3,090,002\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 1: loss improved from inf to 2736506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 251s 79ms/step - loss: 2736506.0000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 2: loss improved from 2736506.00000 to 2736503.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736503.5000\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 3: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736505.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 4: loss improved from 2736503.50000 to 2736502.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736502.5000\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.0000\n",
      "Epoch 5: loss improved from 2736502.50000 to 2736500.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736500.0000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 6: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736503.7500\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 7: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736505.2500\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 8: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736504.7500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 9: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736504.7500\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.5000\n",
      "Epoch 10: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736500.5000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 11: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.7500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 12: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736503.0000\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 13: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736503.5000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 14: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.2500\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 15: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736504.5000\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 16: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.0000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 17: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736503.0000\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 18: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.7500\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 19: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736502.7500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 20: loss did not improve from 2736500.00000\n",
      "3189/3189 [==============================] - 249s 78ms/step - loss: 2736506.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 04:26:04 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 04:26:04 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_pgn4e53\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp_pgn4e53\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_79 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_33 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_47 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_26  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,804,690\n",
      "Trainable params: 3,090,002\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 1: loss improved from inf to 2736504.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 252s 79ms/step - loss: 2736504.7500\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 2: loss improved from 2736504.75000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736503.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 3: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 4: loss improved from 2736503.75000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736502.7500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 5: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 6: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.7500\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.2500\n",
      "Epoch 7: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736507.2500\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 8: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.2500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 9: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 10: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736506.2500\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.2500\n",
      "Epoch 11: loss improved from 2736502.75000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736502.2500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 12: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736504.5000\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 13: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 250s 78ms/step - loss: 2736505.0000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 14: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 250s 79ms/step - loss: 2736505.2500\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 15: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 250s 79ms/step - loss: 2736503.5000\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 16: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 253s 79ms/step - loss: 2736505.0000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 17: loss improved from 2736502.25000 to 2736501.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 250s 79ms/step - loss: 2736501.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.5000\n",
      "Epoch 18: loss improved from 2736501.75000 to 2736500.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 255s 80ms/step - loss: 2736500.5000\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 19: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 255s 80ms/step - loss: 2736505.2500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 20: loss did not improve from 2736500.50000\n",
      "3189/3189 [==============================] - 255s 80ms/step - loss: 2736504.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 05:50:02 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 05:50:02 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp792373wm\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp792373wm\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_82 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_34 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_48 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_55 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_27  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,804,690\n",
      "Trainable params: 3,090,002\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   6/3189 [..............................] - ETA: 4:47 - loss: 2911041.2500WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0489s vs `on_train_batch_end` time: 0.0504s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0489s vs `on_train_batch_end` time: 0.0504s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 1: loss improved from inf to 2736506.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 260s 81ms/step - loss: 2736506.0000\n",
      "Epoch 2/20\n",
      "1968/3189 [=================>............] - ETA: 1:37 - loss: 2699776.2500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m mlflow\u001b[39m.\u001b[39mtensorflow\u001b[39m.\u001b[39mautolog()\n\u001b[0;32m     13\u001b[0m model\u001b[39m=\u001b[39mselect_model(m,INPUTS,drop_out\u001b[39m=\u001b[39mdrop_out)\n\u001b[1;32m---> 14\u001b[0m model,history\u001b[39m=\u001b[39mtrain_model(model,EPOCHS)\n\u001b[0;32m     16\u001b[0m directory\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(directory):\n\u001b[0;32m     18\u001b[0m     \u001b[39m#print ('callbacks/model.'+str(file)+'.h5')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model,epochs):\n\u001b[0;32m      2\u001b[0m     my_callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         \u001b[39m#tf.keras.callbacks.ModelCheckpoint(filepath='callbacks/model.{epoch:02d}.h5')\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         ,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     ]\n\u001b[1;32m---> 14\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     15\u001b[0m                     train_gen\n\u001b[0;32m     16\u001b[0m                     ,epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m     17\u001b[0m                     ,callbacks\u001b[39m=\u001b[39;49mmy_callbacks\n\u001b[0;32m     18\u001b[0m                     )\n\u001b[0;32m     20\u001b[0m     plt\u001b[39m.\u001b[39mclf()\n\u001b[0;32m     21\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:553\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m try_log_autologging_event(\n\u001b[0;32m    544\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_start,\n\u001b[0;32m    545\u001b[0m     session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     kwargs,\n\u001b[0;32m    550\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[39mif\u001b[39;00m patch_is_class:\n\u001b[1;32m--> 553\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:170\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[1;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mcls\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:181\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_exception(e)\n\u001b[0;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[39m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[39m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:174\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    173\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_patch_implementation(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    175\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    176\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:232\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mlflow\u001b[39m.\u001b[39mactive_run():\n\u001b[0;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m--> 232\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_patch_implementation(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run:\n\u001b[0;32m    235\u001b[0m     mlflow\u001b[39m.\u001b[39mend_run(RunStatus\u001b[39m.\u001b[39mto_string(RunStatus\u001b[39m.\u001b[39mFINISHED))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\tensorflow\\__init__.py:1226\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[1;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1223\u001b[0m early_stop_callback \u001b[39m=\u001b[39m _get_early_stop_callback(callbacks)\n\u001b[0;32m   1224\u001b[0m _log_early_stop_callback_params(early_stop_callback)\n\u001b[1;32m-> 1226\u001b[0m history \u001b[39m=\u001b[39m original(inst, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1228\u001b[0m \u001b[39mif\u001b[39;00m log_models:\n\u001b[0;32m   1229\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:536\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[1;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[0;32m    534\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[1;32m--> 536\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:471\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[1;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    464\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[0;32m    465\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         og_kwargs,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 471\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39mog_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mog_kwargs)\n\u001b[0;32m    473\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    474\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[0;32m    475\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m         og_kwargs,\n\u001b[0;32m    480\u001b[0m     )\n\u001b[0;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:533\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[1;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[0;32m    530\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    531\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    532\u001b[0m ):\n\u001b[1;32m--> 533\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[0;32m    534\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "#MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "MODELS=[\"VGG16\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=2\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\" vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_61 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_27 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_41 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_48 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_20  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,843,154\n",
      "Trainable params: 3,614,290\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   6/3189 [..............................] - ETA: 2:22 - loss: 3688507.0000 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0235s vs `on_train_batch_end` time: 0.0238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0235s vs `on_train_batch_end` time: 0.0238s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736951.2500\n",
      "Epoch 1: loss improved from inf to 2736504.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 104s 32ms/step - loss: 2736504.5000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 2: loss did not improve from 2736504.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.2500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.2500\n",
      "Epoch 3: loss did not improve from 2736504.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736507.2500\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 4: loss did not improve from 2736504.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 5: loss improved from 2736504.50000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 6: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.2500\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 7: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.5000\n",
      "Epoch 8: loss improved from 2736502.75000 to 2736501.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736501.5000\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 9: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 10: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 11: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 12: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 13: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.5000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 14: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 15: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 16: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 17: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 18: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.0000\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 19: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 102s 32ms/step - loss: 2736502.5000\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.5000\n",
      "Epoch 20: loss did not improve from 2736501.50000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 23:56:42 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 23:56:42 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0ccs32ey\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp0ccs32ey\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_64 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_28 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_42 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_21  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,843,154\n",
      "Trainable params: 3,614,290\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   5/3189 [..............................] - ETA: 2:02 - loss: 2066955.3750  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0171s vs `on_train_batch_end` time: 0.0243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0171s vs `on_train_batch_end` time: 0.0243s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 1: loss improved from inf to 2736504.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 104s 32ms/step - loss: 2736504.0000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 2: loss did not improve from 2736504.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 3: loss did not improve from 2736504.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 4: loss improved from 2736504.00000 to 2736503.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.0000\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 5: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.5000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.2500\n",
      "Epoch 6: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736507.2500\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 7: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 8: loss did not improve from 2736503.00000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 9: loss improved from 2736503.00000 to 2736501.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736501.7500\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 10: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.2500\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 11: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 12: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 13: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 14: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 15: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736500.7500\n",
      "Epoch 16: loss improved from 2736501.75000 to 2736500.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736500.7500\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 17: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 18: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.7500\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 19: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 20: loss did not improve from 2736500.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 00:30:52 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 00:30:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpzx5b06d6\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpzx5b06d6\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_67 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_29 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_43 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_22  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,843,154\n",
      "Trainable params: 3,614,290\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   5/3189 [..............................] - ETA: 2:04 - loss: 2427656.7500  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0193s vs `on_train_batch_end` time: 0.0223s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0193s vs `on_train_batch_end` time: 0.0223s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.2500\n",
      "Epoch 1: loss improved from inf to 2736506.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 105s 32ms/step - loss: 2736506.2500\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 2: loss improved from 2736506.25000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 3: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 4: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 5: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.0000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 6: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.0000\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 7: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.0000\n",
      "Epoch 8/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736736.7500\n",
      "Epoch 8: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.0000\n",
      "Epoch 9: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.0000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 10: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.0000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 11: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.7500\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 12: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 13: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.2500\n",
      "Epoch 14: loss improved from 2736502.75000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.2500\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 15: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.5000\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 16: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 17: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 18: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.5000\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 19: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736506.7500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 20: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 01:05:03 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 01:05:03 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7ci9n146\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmp7ci9n146\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_70 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_30 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_44 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 224, 224, 3)       2190      \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_23  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,843,154\n",
      "Trainable params: 3,614,290\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   5/3189 [..............................] - ETA: 2:02 - loss: 2640719.5000  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0199s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0199s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.0000\n",
      "Epoch 1: loss improved from inf to 2736507.00000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 103s 32ms/step - loss: 2736507.0000\n",
      "Epoch 2/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 2: loss improved from 2736507.00000 to 2736501.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736501.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 3: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 4: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736507.5000\n",
      "Epoch 5: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736507.5000\n",
      "Epoch 6/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736508.5000\n",
      "Epoch 6: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736508.5000\n",
      "Epoch 7/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 7: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 8: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n",
      "Epoch 9/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736503.7500\n",
      "Epoch 9: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736503.7500\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 10: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 11/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.5000\n",
      "Epoch 11: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.5000\n",
      "Epoch 12/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 12: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.7500\n",
      "Epoch 13/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 13: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.5000\n",
      "Epoch 14/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 14: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.0000\n",
      "Epoch 15/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.7500\n",
      "Epoch 15: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.7500\n",
      "Epoch 16/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.5000\n",
      "Epoch 16: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.5000\n",
      "Epoch 17/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 17: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.7500\n",
      "Epoch 18/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736502.2500\n",
      "Epoch 18: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736502.2500\n",
      "Epoch 19/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 19: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736504.0000\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.2500\n",
      "Epoch 20: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 101s 32ms/step - loss: 2736505.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/28 01:39:08 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/28 01:39:08 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpu75eevox\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpu75eevox\\model\\data\\model\\assets\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "#MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "MODELS=[\"MobileNet\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=2\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\" vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet ajusted decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_49 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_23 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_37 (UpSamplin  (None, 150, 150, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 150, 150, 3)       30        \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_16  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,582,054\n",
      "Trainable params: 6,353,190\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.7500\n",
      "Epoch 1: loss improved from inf to 2736504.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 87s 26ms/step - loss: 2736504.7500\n",
      "Epoch 2/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736403.5000\n",
      "Epoch 2: loss did not improve from 2736504.75000\n",
      "3189/3189 [==============================] - 62s 19ms/step - loss: 2736506.0000\n",
      "Epoch 3/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736846.5000\n",
      "Epoch 3: loss improved from 2736504.75000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 61s 19ms/step - loss: 2736503.7500\n",
      "Epoch 4/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736735.0000\n",
      "Epoch 4: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736506.0000\n",
      "Epoch 5/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736095.2500\n",
      "Epoch 5: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 61s 19ms/step - loss: 2736504.5000\n",
      "Epoch 6/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737142.2500\n",
      "Epoch 6: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 60s 19ms/step - loss: 2736510.2500\n",
      "Epoch 7/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736783.7500\n",
      "Epoch 7: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736506.7500\n",
      "Epoch 8/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736857.5000\n",
      "Epoch 8: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 60s 19ms/step - loss: 2736506.0000\n",
      "Epoch 9/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737106.7500\n",
      "Epoch 9: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736504.7500\n",
      "Epoch 10/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736363.5000\n",
      "Epoch 10: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736505.2500\n",
      "Epoch 11/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737205.7500\n",
      "Epoch 11: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736506.2500\n",
      "Epoch 12/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2734771.2500\n",
      "Epoch 12: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736505.2500\n",
      "Epoch 13/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735813.7500\n",
      "Epoch 13: loss improved from 2736503.75000 to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736502.7500\n",
      "Epoch 14/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736631.2500\n",
      "Epoch 14: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736503.5000\n",
      "Epoch 15/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736583.2500\n",
      "Epoch 15: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736508.2500\n",
      "Epoch 16/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736253.0000\n",
      "Epoch 16: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736506.7500\n",
      "Epoch 17/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736360.5000\n",
      "Epoch 17: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.2500\n",
      "Epoch 18/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736832.2500\n",
      "Epoch 18: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736508.0000\n",
      "Epoch 19/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736094.5000\n",
      "Epoch 19: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736507.0000\n",
      "Epoch 20/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736482.0000\n",
      "Epoch 20: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 22:22:58 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 22:22:58 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpp45wf6qs\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpp45wf6qs\\model\\data\\model\\assets\n",
      "2023/05/27 22:24:09 INFO mlflow.tracking.fluent: Experiment with name 'MobileNet_2 vector_min_size 2500 n_samples 2000 dropout 0.5 epochs 20' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_52 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_24 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_38 (UpSamplin  (None, 150, 150, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 150, 150, 3)       30        \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_17  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,582,054\n",
      "Trainable params: 6,353,190\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "   5/3189 [..............................] - ETA: 2:53 - loss: 2107624.2500  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0262s vs `on_train_batch_end` time: 0.0289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0262s vs `on_train_batch_end` time: 0.0289s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735987.2500\n",
      "Epoch 1: loss improved from inf to 2736505.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 67s 21ms/step - loss: 2736505.7500\n",
      "Epoch 2/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736913.7500\n",
      "Epoch 2: loss improved from 2736505.75000 to 2736503.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.7500\n",
      "Epoch 3/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 3: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 4/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736504.0000\n",
      "Epoch 4: loss did not improve from 2736503.75000\n",
      "3189/3189 [==============================] - 59s 19ms/step - loss: 2736504.0000\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736501.7500\n",
      "Epoch 5: loss improved from 2736503.75000 to 2736501.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736501.7500\n",
      "Epoch 6/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736274.2500\n",
      "Epoch 6: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736503.0000\n",
      "Epoch 7/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736843.7500\n",
      "Epoch 7: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.0000\n",
      "Epoch 8/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736836.7500\n",
      "Epoch 8: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736507.2500\n",
      "Epoch 9/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2737105.5000\n",
      "Epoch 9: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 58s 18ms/step - loss: 2736507.0000\n",
      "Epoch 10/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736874.2500\n",
      "Epoch 10: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 11/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736455.5000\n",
      "Epoch 11: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 58s 18ms/step - loss: 2736506.2500\n",
      "Epoch 12/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735156.0000\n",
      "Epoch 12: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 58s 18ms/step - loss: 2736505.7500\n",
      "Epoch 13/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736617.5000\n",
      "Epoch 13: loss did not improve from 2736501.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.7500\n",
      "Epoch 14/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736192.2500\n",
      "Epoch 14: loss improved from 2736501.75000 to 2736501.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 58s 18ms/step - loss: 2736501.2500\n",
      "Epoch 15/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736930.5000\n",
      "Epoch 15: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736502.2500\n",
      "Epoch 16/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736718.5000\n",
      "Epoch 16: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 17/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736981.0000\n",
      "Epoch 17: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 18/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735648.0000\n",
      "Epoch 18: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.7500\n",
      "Epoch 19/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736965.2500\n",
      "Epoch 19: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.7500\n",
      "Epoch 20/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2737059.5000\n",
      "Epoch 20: loss did not improve from 2736501.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 22:43:39 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 22:43:39 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpj_uf3guk\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpj_uf3guk\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_55 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_25 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_39 (UpSamplin  (None, 150, 150, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 150, 150, 3)       30        \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_18  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,582,054\n",
      "Trainable params: 6,353,190\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736410.7500\n",
      "Epoch 1: loss improved from inf to 2736502.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736502.7500\n",
      "Epoch 2/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736360.5000\n",
      "Epoch 2: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 3/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736249.5000\n",
      "Epoch 3: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736508.2500\n",
      "Epoch 4/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736824.7500\n",
      "Epoch 4: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.2500\n",
      "Epoch 5/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 5: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.7500\n",
      "Epoch 6/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736958.7500\n",
      "Epoch 6: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 7/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736813.7500\n",
      "Epoch 7: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.5000\n",
      "Epoch 8/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.0000\n",
      "Epoch 8: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.0000\n",
      "Epoch 9/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737018.7500\n",
      "Epoch 9: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.5000\n",
      "Epoch 10/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.7500\n",
      "Epoch 10: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n",
      "Epoch 11/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736041.2500\n",
      "Epoch 11: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.7500\n",
      "Epoch 12/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2737322.5000\n",
      "Epoch 12: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736507.5000\n",
      "Epoch 13/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736946.5000\n",
      "Epoch 13: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 14/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736937.7500\n",
      "Epoch 14: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 15/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735879.7500\n",
      "Epoch 15: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.5000\n",
      "Epoch 16/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736520.5000\n",
      "Epoch 16: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.7500\n",
      "Epoch 17/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736618.0000\n",
      "Epoch 17: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n",
      "Epoch 18/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736152.7500\n",
      "Epoch 18: loss did not improve from 2736502.75000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 19/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736789.5000\n",
      "Epoch 19: loss improved from 2736502.75000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736502.2500\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736505.0000\n",
      "Epoch 20: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 23:03:09 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 23:03:09 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn5u4ewh9\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpn5u4ewh9\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_58 (InputLayer)       [(None, 2500)]            0         \n",
      "                                                                 \n",
      " reshape_26 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_40 (UpSamplin  (None, 150, 150, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 150, 150, 3)       30        \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_19  (None, 1024)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 2500)              2562500   \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,582,054\n",
      "Trainable params: 6,353,190\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736506.7500\n",
      "Epoch 1: loss improved from inf to 2736506.75000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 59s 18ms/step - loss: 2736506.7500\n",
      "Epoch 2/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736714.5000\n",
      "Epoch 2: loss improved from 2736506.75000 to 2736505.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.2500\n",
      "Epoch 3/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736042.7500\n",
      "Epoch 3: loss did not improve from 2736505.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736507.2500\n",
      "Epoch 4/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736010.7500\n",
      "Epoch 4: loss did not improve from 2736505.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736510.2500\n",
      "Epoch 5/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735576.5000\n",
      "Epoch 5: loss improved from 2736505.25000 to 2736504.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 6/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736393.0000\n",
      "Epoch 6: loss improved from 2736504.50000 to 2736503.50000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.5000\n",
      "Epoch 7/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2736698.5000\n",
      "Epoch 7: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736508.5000\n",
      "Epoch 8/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736787.7500\n",
      "Epoch 8: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736503.7500\n",
      "Epoch 9/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735867.0000\n",
      "Epoch 9: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 10/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736091.7500\n",
      "Epoch 10: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 11/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735885.7500\n",
      "Epoch 11: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.0000\n",
      "Epoch 12/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735742.7500\n",
      "Epoch 12: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n",
      "Epoch 13/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736645.7500\n",
      "Epoch 13: loss did not improve from 2736503.50000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 14/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736792.7500\n",
      "Epoch 14: loss improved from 2736503.50000 to 2736502.25000, saving model to callbacks\\model.h5\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736502.2500\n",
      "Epoch 15/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736041.2500\n",
      "Epoch 15: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736505.7500\n",
      "Epoch 16/20\n",
      "3187/3189 [============================>.] - ETA: 0s - loss: 2735013.0000\n",
      "Epoch 16: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.0000\n",
      "Epoch 17/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736426.0000\n",
      "Epoch 17: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.7500\n",
      "Epoch 18/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2735979.2500\n",
      "Epoch 18: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736506.7500\n",
      "Epoch 19/20\n",
      "3188/3189 [============================>.] - ETA: 0s - loss: 2736333.0000\n",
      "Epoch 19: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736504.5000\n",
      "Epoch 20/20\n",
      "3189/3189 [==============================] - ETA: 0s - loss: 2736508.2500\n",
      "Epoch 20: loss did not improve from 2736502.25000\n",
      "3189/3189 [==============================] - 57s 18ms/step - loss: 2736508.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/27 23:22:33 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: unsupported operand type(s) for *: 'slice' and 'int'\n",
      "2023/05/27 23:22:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpeibxb_st\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USUARIO\\AppData\\Local\\Temp\\tmpeibxb_st\\model\\data\\model\\assets\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "#MODELS=[\"naive\",\"dnn1\",\"dnn2\",\"cnn1\",\"cnn2\"]\n",
    "MODELS=[\"MobileNet_2\"]\n",
    "DROP_OUT=[0.1,0.5]\n",
    "RUNS=2\n",
    "INPUTS=clean_specs.shape[-1]\n",
    "for r in range(RUNS):\n",
    "    for m in MODELS:\n",
    "        for drop_out in DROP_OUT:\n",
    "            mlflow.set_experiment(m+\" vector_min_size \"+str(vector_min_size)+\" n_samples \"+str(n_samples)+ \" dropout \"+str(drop_out)+\" epochs \"+str(EPOCHS))\n",
    "            with mlflow.start_run():\n",
    "                mlflow.tensorflow.autolog()\n",
    "                model=select_model(m,INPUTS,drop_out=drop_out)\n",
    "                model,history=train_model(model,EPOCHS)\n",
    "                \n",
    "                directory='callbacks'\n",
    "                for filename in os.listdir(directory):\n",
    "                    #print ('callbacks/model.'+str(file)+'.h5')\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    mlflow.log_artifact(f)\n",
    "                    \n",
    "                \n",
    "                for filename in os.listdir(directory):\n",
    "                    f = os.path.join(directory, filename)\n",
    "                    os.remove(f)\n",
    "\n",
    "                mlflow.log_artifact(\"Training Plot.png\")\n",
    "            mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 64)                160064    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 2500)              162500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,724\n",
      "Trainable params: 326,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/26 03:38:12 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '3bc531b91b384e9ba6370e13439054da', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/5065 [..............................] - ETA: 4:48:18 - loss: 3486986.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_end` time: 0.0063s). Check your callbacks.\n",
      "5065/5065 [==============================] - 19s 3ms/step - loss: 3221340.7500\n",
      "Epoch 2/100\n",
      "2604/5065 [==============>...............] - ETA: 6s - loss: 3172217.2500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m auto_encoder0\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madamax\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m auto_encoder0\u001b[39m.\u001b[39msummary()\n\u001b[1;32m---> 11\u001b[0m auto_encoder0,history0\u001b[39m=\u001b[39mtrain_model(auto_encoder0,\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model,epochs):\n\u001b[0;32m      2\u001b[0m     my_callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         \u001b[39m#tf.keras.callbacks.ModelCheckpoint(filepath='callbacks/model.{epoch:02d}.h5')\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[0;32m      5\u001b[0m         filepath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcallbacks/model\u001b[39m\u001b[39m{epoch:02d}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{loss:2f}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[1;32m----> 8\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      9\u001b[0m                     train_gen\n\u001b[0;32m     10\u001b[0m                     ,epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m     11\u001b[0m                     ,callbacks\u001b[39m=\u001b[39;49mmy_callbacks\n\u001b[0;32m     12\u001b[0m                     )\n\u001b[0;32m     14\u001b[0m     plt\u001b[39m.\u001b[39mclf()\n\u001b[0;32m     15\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:553\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m try_log_autologging_event(\n\u001b[0;32m    544\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_start,\n\u001b[0;32m    545\u001b[0m     session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     kwargs,\n\u001b[0;32m    550\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[39mif\u001b[39;00m patch_is_class:\n\u001b[1;32m--> 553\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:170\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[1;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mcls\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:181\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_exception(e)\n\u001b[0;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[39m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[39m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:174\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    173\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_patch_implementation(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    175\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    176\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:232\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[1;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mlflow\u001b[39m.\u001b[39mactive_run():\n\u001b[0;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m--> 232\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_patch_implementation(original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run:\n\u001b[0;32m    235\u001b[0m     mlflow\u001b[39m.\u001b[39mend_run(RunStatus\u001b[39m.\u001b[39mto_string(RunStatus\u001b[39m.\u001b[39mFINISHED))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\tensorflow\\__init__.py:1226\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[1;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1223\u001b[0m early_stop_callback \u001b[39m=\u001b[39m _get_early_stop_callback(callbacks)\n\u001b[0;32m   1224\u001b[0m _log_early_stop_callback_params(early_stop_callback)\n\u001b[1;32m-> 1226\u001b[0m history \u001b[39m=\u001b[39m original(inst, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1228\u001b[0m \u001b[39mif\u001b[39;00m log_models:\n\u001b[0;32m   1229\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:536\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[1;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[0;32m    534\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[1;32m--> 536\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:471\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[1;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    464\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[0;32m    465\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         og_kwargs,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 471\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39mog_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mog_kwargs)\n\u001b[0;32m    473\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    474\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[0;32m    475\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m         og_kwargs,\n\u001b[0;32m    480\u001b[0m     )\n\u001b[0;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:533\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[1;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[0;32m    530\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    531\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    532\u001b[0m ):\n\u001b[1;32m--> 533\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[0;32m    534\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "auto_encoder0=models.Sequential()\n",
    "auto_encoder0.add(layers.Input(shape=(clean_specs.shape[-1],)))\n",
    "auto_encoder0.add(layers.Dense(64))\n",
    "auto_encoder0.add(layers.Dropout(drop_out))\n",
    "auto_encoder0.add(layers.Dense(64))\n",
    "auto_encoder0.add(layers.Dense(clean_specs.shape[-1]))\n",
    "auto_encoder0.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder0.summary()\n",
    "\n",
    "auto_encoder0,history0=train_model(auto_encoder0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 3000)              7503000   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1500)              4501500   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1500)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3000)              4503000   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3000)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2500)              7502500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,010,000\n",
      "Trainable params: 24,010,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 3082028.2500\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2403725.7500\n",
      "Epoch 3/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2352833.7500\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2338661.2500\n",
      "Epoch 5/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2315535.7500\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2301943.2500\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2304133.7500\n",
      "Epoch 8/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2279721.5000\n",
      "Epoch 9/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2283096.2500\n",
      "Epoch 10/100\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2268994.7500\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2260538.0000\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2257790.5000\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2251372.7500\n",
      "Epoch 14/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2256688.5000\n",
      "Epoch 15/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2237934.5000\n",
      "Epoch 16/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2228071.0000\n",
      "Epoch 17/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2220790.5000\n",
      "Epoch 18/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2226121.5000\n",
      "Epoch 19/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2232559.2500\n",
      "Epoch 20/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2199577.7500\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2219602.7500\n",
      "Epoch 22/100\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2211600.5000\n",
      "Epoch 23/100\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2216947.5000\n",
      "Epoch 24/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2213495.5000\n",
      "Epoch 25/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2210001.7500\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2209317.5000\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2210899.0000\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2206757.0000\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192273.7500\n",
      "Epoch 30/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2194371.5000\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2202483.5000\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2205742.0000\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2196406.5000\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2201251.0000\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2202107.7500\n",
      "Epoch 36/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2202255.2500\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2204272.2500\n",
      "Epoch 38/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2193600.0000\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2200365.5000\n",
      "Epoch 40/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186397.0000\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2194712.0000\n",
      "Epoch 42/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2189444.2500\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2189075.5000\n",
      "Epoch 44/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192632.0000\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186345.2500\n",
      "Epoch 46/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2188240.0000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192870.5000\n",
      "Epoch 48/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2194630.7500\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2193702.0000\n",
      "Epoch 50/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2183348.2500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181642.7500\n",
      "Epoch 52/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2192077.0000\n",
      "Epoch 53/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2195663.2500\n",
      "Epoch 54/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2191078.7500\n",
      "Epoch 55/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2188509.2500\n",
      "Epoch 56/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177257.7500\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2183153.7500\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2178220.2500\n",
      "Epoch 59/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180142.0000\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2168945.0000\n",
      "Epoch 61/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2185011.2500\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181841.0000\n",
      "Epoch 63/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177015.2500\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2184785.0000\n",
      "Epoch 65/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2183053.0000\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2187578.5000\n",
      "Epoch 67/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2172332.7500\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2188221.2500\n",
      "Epoch 69/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2186942.5000\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2181042.5000\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - 12s 4ms/step - loss: 2185855.7500\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2180934.2500\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177779.5000\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2172048.2500\n",
      "Epoch 75/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175971.5000\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2182807.2500\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182508.7500\n",
      "Epoch 78/100\n",
      "3189/3189 [==============================] - 11s 3ms/step - loss: 2177054.0000\n",
      "Epoch 79/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2175537.7500\n",
      "Epoch 80/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177472.2500\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2164963.2500\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2177569.0000\n",
      "Epoch 83/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2179048.5000\n",
      "Epoch 84/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2166931.5000\n",
      "Epoch 85/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2176817.0000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2187868.0000\n",
      "Epoch 87/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2161570.0000\n",
      "Epoch 88/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2182724.7500\n",
      "Epoch 89/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2172429.7500\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2169314.2500\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2161980.0000\n",
      "Epoch 92/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2180761.0000\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2176746.5000\n",
      "Epoch 94/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2172273.0000\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2173233.5000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2176973.2500\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2168021.2500\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2179147.0000\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2174971.2500\n",
      "Epoch 100/100\n",
      "3189/3189 [==============================] - 11s 4ms/step - loss: 2171384.5000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABVeklEQVR4nO3dd1hT9/4H8HcWYYPsISgo7oUbUatVW/eo2uvWVutVcXbcDtuqv9uq3eO2tbVDOxytWrV1762oOHHgQAERZMleIfn+/kCiKaCISQ6E9+t58txycs7JJ8fe8vY7ZUIIASIiIiILIZe6ACIiIiJjYrghIiIii8JwQ0RERBaF4YaIiIgsCsMNERERWRSGGyIiIrIoDDdERERkURhuiIiIyKIw3BAREZFFYbghIqORyWSYP3/+Y1938+ZNyGQyLF++3Og1EVHNw3BDZGGWL18OmUwGmUyGQ4cOlXpfCAE/Pz/IZDL0799fggqNY8uWLZDJZPDx8YFOp5O6HCKqQhhuiCyUtbU1Vq5cWer4/v37cevWLajVagmqMp4VK1agbt26SEhIwJ49e6Quh4iqEIYbIgvVt29frFmzBkVFRQbHV65ciTZt2sDLy0uiyp5cTk4ONm7ciJdffhnBwcFYsWKF1CWVKycnR+oSiGochhsiCzVy5EikpqZi586d+mOFhYVYu3YtRo0aVeY1OTk5eOWVV+Dn5we1Wo2GDRvi448/hhDC4LyCggLMmTMH7u7ucHBwwMCBA3Hr1q0y7xkfH48XX3wRnp6eUKvVaNq0KX766acn+m7r169HXl4ehg8fjhEjRuDPP/9Efn5+qfPy8/Mxf/58NGjQANbW1vD29sZzzz2H69ev68/R6XT44osv0Lx5c1hbW8Pd3R29e/fGyZMnATx8PNA/xxjNnz8fMpkMFy9exKhRo1CrVi107twZAHDu3DlMmDABgYGBsLa2hpeXF1588UWkpqaW+cwmTpwIHx8fqNVqBAQEYOrUqSgsLER0dDRkMhk+++yzUtcdOXIEMpkMq1atetxHSmRRlFIXQESmUbduXYSEhGDVqlXo06cPAGDr1q3IyMjAiBEj8OWXXxqcL4TAwIEDsXfvXkycOBGtWrXC9u3b8dprryE+Pt7gl+mkSZPw22+/YdSoUejUqRP27NmDfv36larhzp076NixI2QyGaZPnw53d3ds3boVEydORGZmJmbPnl2p77ZixQp0794dXl5eGDFiBN544w38/fffGD58uP4crVaL/v37Y/fu3RgxYgRmzZqFrKws7Ny5E5GRkahXrx4AYOLEiVi+fDn69OmDSZMmoaioCAcPHsSxY8fQtm3bStU3fPhwBAUFYeHChfpguHPnTkRHR+OFF16Al5cXLly4gKVLl+LChQs4duwYZDIZAOD27dto37490tPTMXnyZDRq1Ajx8fFYu3YtcnNzERgYiNDQUKxYsQJz5swp9VwcHBwwaNCgStVNZDEEEVmUZcuWCQDixIkT4quvvhIODg4iNzdXCCHE8OHDRffu3YUQQtSpU0f069dPf92GDRsEAPHee+8Z3G/YsGFCJpOJa9euCSGEOHPmjAAgpk2bZnDeqFGjBAAxb948/bGJEycKb29vkZKSYnDuiBEjhJOTk76uGzduCABi2bJlj/x+d+7cEUqlUnz//ff6Y506dRKDBg0yOO+nn34SAMSnn35a6h46nU4IIcSePXsEADFz5sxyz3lYbf/8vvPmzRMAxMiRI0udW/JdH7Rq1SoBQBw4cEB/bNy4cUIul4sTJ06UW9N3330nAIhLly7p3yssLBRubm5i/Pjxpa4jqmnYLUVkwZ5//nnk5eVh06ZNyMrKwqZNm8rtktqyZQsUCgVmzpxpcPyVV16BEAJbt27Vnweg1Hn/bIURQmDdunUYMGAAhBBISUnRv5599llkZGTg1KlTj/2dVq9eDblcjqFDh+qPjRw5Elu3bsXdu3f1x9atWwc3NzfMmDGj1D1KWknWrVsHmUyGefPmlXtOZUyZMqXUMRsbG/0/5+fnIyUlBR07dgQA/XPQ6XTYsGEDBgwYUGarUUlNzz//PKytrQ3GGm3fvh0pKSkYM2ZMpesmshQ1OtwcOHAAAwYMgI+PD2QyGTZs2PDY9xBC4OOPP0aDBg2gVqvh6+uL999/3/jFElWCu7s7evbsiZUrV+LPP/+EVqvFsGHDyjw3JiYGPj4+cHBwMDjeuHFj/fsl/yuXy/XdOiUaNmxo8HNycjLS09OxdOlSuLu7G7xeeOEFAEBSUtJjf6fffvsN7du3R2pqKq5du4Zr164hODgYhYWFWLNmjf6869evo2HDhlAqy+99v379Onx8fODi4vLYdTxMQEBAqWNpaWmYNWsWPD09YWNjA3d3d/15GRkZAIqfWWZmJpo1a/bQ+zs7O2PAgAEGs+FWrFgBX19fPP3000b8JkTVU40ec5OTk4OWLVvixRdfxHPPPVepe8yaNQs7duzAxx9/jObNmyMtLQ1paWlGrpSo8kaNGoWXXnoJiYmJ6NOnD5ydnc3yuSVrz4wZMwbjx48v85wWLVo81j2vXr2KEydOAACCgoJKvb9ixQpMnjz5MSt9uPJacLRabbnXPNhKU+L555/HkSNH8Nprr6FVq1awt7eHTqdD7969K7VOz7hx47BmzRocOXIEzZs3x19//YVp06ZBLq/Rf2clAlDDw02fPn30Ay3LUlBQgLlz52LVqlVIT09Hs2bN8MEHH6Bbt24AgEuXLmHJkiWIjIzU/621rL+xEUlpyJAh+Pe//41jx47h999/L/e8OnXqYNeuXcjKyjJovbl8+bL+/ZL/1el0+paRElFRUQb3K5lJpdVq0bNnT6N8lxUrVkClUuHXX3+FQqEweO/QoUP48ssvERsbC39/f9SrVw/h4eHQaDRQqVRl3q9evXrYvn070tLSym29qVWrFgAgPT3d4HhJS1ZF3L17F7t378aCBQvw7rvv6o9fvXrV4Dx3d3c4OjoiMjLykffs3bs33N3dsWLFCnTo0AG5ubkYO3ZshWsismSM+A8xffp0HD16FKtXr8a5c+cwfPhw9O7dW/8fpL///huBgYHYtGkTAgICULduXUyaNIktN1Sl2NvbY8mSJZg/fz4GDBhQ7nl9+/aFVqvFV199ZXD8s88+g0wm0/9FoOR//znb6vPPPzf4WaFQYOjQoVi3bl2Zv6yTk5Mf+7usWLECXbp0wb/+9S8MGzbM4PXaa68BgH4a9NChQ5GSklLq+wDQz2AaOnQohBBYsGBBuec4OjrCzc0NBw4cMHj/m2++qXDdJUFM/GNK/T+fmVwux+DBg/H333/rp6KXVRMAKJVKjBw5En/88QeWL1+O5s2bP3ZLGJGlqtEtNw8TGxuLZcuWITY2Fj4+PgCAV199Fdu2bcOyZcuwcOFCREdHIyYmBmvWrMEvv/wCrVaLOXPmYNiwYVwxlaqU8rqFHjRgwAB0794dc+fOxc2bN9GyZUvs2LEDGzduxOzZs/VjbFq1aoWRI0fim2++QUZGBjp16oTdu3fj2rVrpe65ePFi7N27Fx06dMBLL72EJk2aIC0tDadOncKuXbse6y8C4eHhuHbtGqZPn17m+76+vmjdujVWrFiB119/HePGjcMvv/yCl19+GcePH0eXLl2Qk5ODXbt2Ydq0aRg0aBC6d++OsWPH4ssvv8TVq1f1XUQHDx5E9+7d9Z81adIkLF68GJMmTULbtm1x4MABXLlypcK1Ozo6omvXrvjwww+h0Wjg6+uLHTt24MaNG6XOXbhwIXbs2IGnnnoKkydPRuPGjZGQkIA1a9bg0KFDBt2K48aNw5dffom9e/figw8+qHA9RBZPsnlaVQwAsX79ev3PmzZtEgCEnZ2dwUupVIrnn39eCCHESy+9JACIqKgo/XURERECgLh8+bK5vwKREMJwKvjD/HMquBBCZGVliTlz5ggfHx+hUqlEUFCQ+Oijj/RTkEvk5eWJmTNnCldXV2FnZycGDBgg4uLiSk2NFqJ46nZYWJjw8/MTKpVKeHl5iR49eoilS5fqz6nIVPAZM2YIAOL69evlnjN//nwBQJw9e1YIUTz9eu7cuSIgIED/2cOGDTO4R1FRkfjoo49Eo0aNhJWVlXB3dxd9+vQRERER+nNyc3PFxIkThZOTk3BwcBDPP/+8SEpKKncqeHJycqnabt26JYYMGSKcnZ2Fk5OTGD58uLh9+3aZzywmJkaMGzdOuLu7C7VaLQIDA0VYWJgoKCgodd+mTZsKuVwubt26Ve5zIappZEL8o520hpLJZFi/fj0GDx4MAPj9998xevRoXLhwoVTfvr29Pby8vDBv3jwsXLgQGo1G/15eXh5sbW2xY8cO9OrVy5xfgYhqoODgYLi4uGD37t1Sl0JUZbBbqhzBwcHQarVISkpCly5dyjwnNDQURUVFuH79ur7JvqSpumTwJRGRqZw8eRJnzpwpc2sIopqsRrfcZGdn68cJBAcH49NPP0X37t3h4uICf39/jBkzBocPH8Ynn3yC4OBgJCcnY/fu3WjRogX69esHnU6Hdu3awd7eHp9//jl0Oh3CwsLg6OiIHTt2SPztiMhSRUZGIiIiAp988glSUlIQHR0Na2trqcsiqjJq9GypkydPIjg4GMHBwQCg32G4ZKrmsmXLMG7cOLzyyito2LAhBg8ejBMnTsDf3x9A8cyGv//+G25ubujatSv69euHxo0bY/Xq1ZJ9JyKyfGvXrsULL7wAjUaDVatWMdgQ/UONbrkhIiIiy1OjW26IiIjI8jDcEBERkUWpcbOldDodbt++DQcHhyfa9ZeIiIjMRwiBrKws+Pj4PHIPtRoXbm7fvg0/Pz+pyyAiIqJKiIuLQ+3atR96To0LNyUbAsbFxcHR0VHiaoiIiKgiMjMz4efnZ7Cxb3lqXLgp6YpydHRkuCEiIqpmKjKkhAOKiYiIyKIw3BAREZFFYbghIiIii1LjxtxUlFarNdjtmx7NysrqkdPziIiITI3h5h+EEEhMTER6errUpVQ7crkcAQEBsLKykroUIiKqwRhu/qEk2Hh4eMDW1pYL/VVQyeKICQkJ8Pf353MjIiLJMNw8QKvV6oONq6ur1OVUO+7u7rh9+zaKioqgUqmkLoeIiGooDpB4QMkYG1tbW4krqZ5KuqO0Wq3ElRARUU3GcFMGdqlUDp8bERFVBQw3REREZFEYboiIiMiiMNxYiAkTJmDw4MFSl0FERCQ5zpYyEp0QKNIKAICVkpmRiIhIKvwtbCR5hVpcTszEjZRsqUspZf/+/Wjfvj3UajW8vb3xxhtvoKioSP/+2rVr0bx5c9jY2MDV1RU9e/ZETk4OAGDfvn1o37497Ozs4OzsjNDQUMTExEj1VYiIiB6JLTePIIRAnubRU5vzNEXI12ih0wnkFhY98vyKsFEpnngGUnx8PPr27YsJEybgl19+weXLl/HSSy/B2toa8+fPR0JCAkaOHIkPP/wQQ4YMQVZWFg4ePAghBIqKijB48GC89NJLWLVqFQoLC3H8+HHOiiIioiqN4eYR8jRaNHl3uySfffH/noWt1ZP9EX3zzTfw8/PDV199BZlMhkaNGuH27dt4/fXX8e677yIhIQFFRUV47rnnUKdOHQBA8+bNAQBpaWnIyMhA//79Ua9ePQBA48aNn+xLERERmRi7pSzcpUuXEBISYtDaEhoaiuzsbNy6dQstW7ZEjx490Lx5cwwfPhzff/897t69CwBwcXHBhAkT8Oyzz2LAgAH44osvkJCQINVXISIiqhC23DyCjUqBi//37CPPy9docS0pGwq5DI29HY322aamUCiwc+dOHDlyBDt27MD//vc/zJ07F+Hh4QgICMCyZcswc+ZMbNu2Db///jvefvtt7Ny5Ex07djR5bURERJXBlptHkMlksLVSPvJlp1bCWqWAtVJRofMr8jLG2JbGjRvj6NGjEELojx0+fBgODg6oXbu2/juGhoZiwYIFOH36NKysrLB+/Xr9+cHBwXjzzTdx5MgRNGvWDCtXrnziuoiIiEyFLTdGUhJDxEPPMq2MjAycOXPG4NjkyZPx+eefY8aMGZg+fTqioqIwb948vPzyy5DL5QgPD8fu3bvxzDPPwMPDA+Hh4UhOTkbjxo1x48YNLF26FAMHDoSPjw+ioqJw9epVjBs3TpovSEREVAEMN0ZS0soiJEw3+/btQ3BwsMGxiRMnYsuWLXjttdfQsmVLuLi4YOLEiXj77bcBAI6Ojjhw4AA+//xzZGZmok6dOvjkk0/Qp08f3LlzB5cvX8bPP/+M1NRUeHt7IywsDP/+97+l+HpEREQVIhNCyl/H5peZmQknJydkZGTA0dFwbEx+fj5u3LiBgIAAWFtbP9Z9i7Q6XEzIBAA093WqkdOln+T5ERERPczDfn//E8fcmECNSotERERVDMONkRi01DDdEBERSYbhxkgMsw3TDRERkVQYbozkwRE2NWsUExERUdXCcFOGyoyxfrBbqqZmmxo2Np2IiKoohpsHqFQqAEBubm6lrq8K08GlVFhYCKB41WMiIiKpcJ2bBygUCjg7OyMpKQkAYGtr+1hTukVRIYQQyM/Pg05Zs37B63Q6JCcnw9bWFkol/7UiIiLp8LfQP3h5eQGAPuA8juT0POgEIM9WQ6moeY1icrkc/v7+NXKNHyIiqjoYbv5BJpPB29sbHh4e0Gg0j3XtK98cRkaeBj+Mb4cANzsTVVh1WVlZQS6veaGOiIiqFoabcigUisceO5KSJ5CUpYVOruQKvURERBLhX7ONSCkv7o7R6mroiGIiIqIqgOHGiErG2Wi0DDdERERSYbgxIqWiuOWmSKuTuBIiIqKai+HGiFT3BtMWsVuKiIhIMgw3RqS4N+aG4YaIiEg6DDdGpGK3FBERkeQYboyIA4qJiIikx3BjRApOBSciIpKcpOFmyZIlaNGiBRwdHeHo6IiQkBBs3br1odesWbMGjRo1grW1NZo3b44tW7aYqdpH03dL6dgtRUREJBVJw03t2rWxePFiRERE4OTJk3j66acxaNAgXLhwoczzjxw5gpEjR2LixIk4ffo0Bg8ejMGDByMyMtLMlZdNKWe3FBERkdRkQogq9ZvYxcUFH330ESZOnFjqvX/961/IycnBpk2b9Mc6duyIVq1a4dtvv63Q/TMzM+Hk5ISMjAw4OjoarW4AmPTzCey6lIQPhjbHv9r5G/XeRERENdnj/P6uMmNutFotVq9ejZycHISEhJR5ztGjR9GzZ0+DY88++yyOHj1a7n0LCgqQmZlp8DKVkjE3bLkhIiKSjuTh5vz587C3t4darcaUKVOwfv16NGnSpMxzExMT4enpaXDM09MTiYmJ5d5/0aJFcHJy0r/8/PyMWv+DSmZLcSo4ERGRdCQPNw0bNsSZM2cQHh6OqVOnYvz48bh48aLR7v/mm28iIyND/4qLizPavf9JxUX8iIiIJKeUugArKyvUr18fANCmTRucOHECX3zxBb777rtS53p5eeHOnTsGx+7cuQMvL69y769Wq6FWq41bdDkU3H6BiIhIcpK33PyTTqdDQUFBme+FhIRg9+7dBsd27txZ7hgdc+MKxURERNKTtOXmzTffRJ8+feDv74+srCysXLkS+/btw/bt2wEA48aNg6+vLxYtWgQAmDVrFp566il88skn6NevH1avXo2TJ09i6dKlUn4NvZJdwTmgmIiISDqShpukpCSMGzcOCQkJcHJyQosWLbB9+3b06tULABAbGwu5/H7jUqdOnbBy5Uq8/fbbeOuttxAUFIQNGzagWbNmUn0FAyXr3HCFYiIiIulIGm5+/PHHh76/b9++UseGDx+O4cOHm6iiJ6MsmQrOFYqJiIgkU+XG3FRn96eCs+WGiIhIKgw3RsQBxURERNJjuDEiBde5ISIikhzDjRGp2C1FREQkOYYbI+KAYiIiIukx3BhRSbcUp4ITERFJh+HGiNgtRUREJD2GGyO6v0Ixu6WIiIikwnBjRCpunElERCQ5hhsj4lRwIiIi6THcGJGSi/gRERFJjuHGiDigmIiISHoMN0Z0v1uKLTdERERSYbgxIv3eUhxzQ0REJBmGGyNS3pstpWG3FBERkWQYboyIA4qJiIikx3BjRCUtN9x+gYiISDoMN0akX6GYA4qJiIgkw3BjRPoVijnmhoiISDIMN0bEFYqJiIikx3BjRCoOKCYiIpIcw40RKblCMRERkeQYboxIKeeAYiIiIqkx3BhRyWwpTgUnIiKSDsONET24QrEQDDhERERSYLgxopIBxQBbb4iIiKTCcGNEJVPBAU4HJyIikgrDjRGpFPcfJ8MNERGRNBhujEj5YMsN17ohIiKSBMONET3YLaXhWjdERESSYLgxIplMpm+94YBiIiIiaTDcGJl+Z3B2SxEREUmC4cbI9DuDs+WGiIhIEgw3RqbQr1LMlhsiIiIpMNwY2YOrFBMREZH5MdwYWckqxdwZnIiISBoMN0am4M7gREREkmK4MbKSVYo5FZyIiEgaDDdGVrLODaeCExERSYPhxsiU91puOOaGiIhIGgw3RsYViomIiKTFcGNkXKGYiIhIWgw3RsYViomIiKTFcGNkCg4oJiIikhTDjZEpFRxzQ0REJCWGGyNTcbYUERGRpBhujEzJFYqJiIgkxXBjZOyWIiIikhbDjZFxV3AiIiJpMdwYmVK/Kzi7pYiIiKTAcGNkJWNuuM4NERGRNBhujIx7SxEREUmL4cbIVPqWG3ZLERERSYHhxshKWm44oJiIiEgaDDdGdn9XcLbcEBERSYHhxsju7wrOlhsiIiIpMNwYmVK/KzhbboiIiKTAcGNk97ul2HJDREQkBYYbI+OAYiIiImkx3BiZiisUExERSYrhxsju7wrOlhsiIiIpMNwYmeJet5SW3VJERESSkDTcLFq0CO3atYODgwM8PDwwePBgREVFPfK6zz//HA0bNoSNjQ38/PwwZ84c5Ofnm6HiR+MKxURERNKSNNzs378fYWFhOHbsGHbu3AmNRoNnnnkGOTk55V6zcuVKvPHGG5g3bx4uXbqEH3/8Eb///jveeustM1ZePg4oJiIikpZSyg/ftm2bwc/Lly+Hh4cHIiIi0LVr1zKvOXLkCEJDQzFq1CgAQN26dTFy5EiEh4ebvN6K4FRwIiIiaVWpMTcZGRkAABcXl3LP6dSpEyIiInD8+HEAQHR0NLZs2YK+ffuWeX5BQQEyMzMNXqZ0f4VidksRERFJQdKWmwfpdDrMnj0boaGhaNasWbnnjRo1CikpKejcuTOEECgqKsKUKVPK7ZZatGgRFixYYKqyS7m/QjFbboiIiKRQZVpuwsLCEBkZidWrVz/0vH379mHhwoX45ptvcOrUKfz555/YvHkz/vvf/5Z5/ptvvomMjAz9Ky4uzhTl65V0S3GdGyIiImlUiZab6dOnY9OmTThw4ABq16790HPfeecdjB07FpMmTQIANG/eHDk5OZg8eTLmzp0Ludwwr6nVaqjVapPV/k8l3VJsuSEiIpKGpOFGCIEZM2Zg/fr12LdvHwICAh55TW5ubqkAo1Ao9PeTmurebKkizpYiIiKShKThJiwsDCtXrsTGjRvh4OCAxMREAICTkxNsbGwAAOPGjYOvry8WLVoEABgwYAA+/fRTBAcHo0OHDrh27RreeecdDBgwQB9ypHR/hWJ2SxEREUlB0nCzZMkSAEC3bt0Mji9btgwTJkwAAMTGxhq01Lz99tuQyWR4++23ER8fD3d3dwwYMADvv/++ucp+qJJuKU4FJyIikobk3VKPsm/fPoOflUol5s2bh3nz5pmoqiejny3FbikiIiJJVJnZUpaC69wQERFJi+HGyLjODRERkbQYboxMPxWcLTdERESSYLgxMhVbboiIiCTFcGNk91tuGG6IiIikwHBjZPrtF7jODRERkSQYboxMeW+FYp0AdOyaIiIiMjuGGyMr6ZYCuEoxERGRFBhujKykWwrguBsiIiIpMNwYmfKBrSI4Y4qIiMj8GG6MTKV4sOWG3VJERETmxnBjZDKZDAr9jCm23BAREZkbw40JMNwQERFJh+HGBFRybsFAREQkFYYbEyhZ60bD2VJERERmx3BjAlylmIiISDoMNybA/aWIiIikw3BjAkruDE5ERCQZhhsTuN9yw24pIiIic2O4MQElp4ITERFJhuHGBFT3ZktxzA0REZH5MdyYQEm3FHcFJyIiMj+GGxNQyNlyQ0REJBWGGxMoWaFYy5YbIiIis2O4MQF9txRbboiIiMyO4cYE7q9zw5YbIiIic2O4MQGuUExERCQdhhsT4ArFRERE0mG4MQEVVygmIiKSDMONCSjkHFBMREQkFYYbEyhZoVjLbikiIiKzY7gxgZK9pbhCMRERkfkx3JgAZ0sRERFJh+HGBDhbioiISDoMNyag5GwpIiIiyTDcmEDJgGK23BAREZkfw40JlEwF55gbIiIi82O4MYGSXcG5txQREZH5MdyYgPJetxQX8SMiIjI/hhsTuN8txZYbIiIic2O4MYGSvaW4QjEREZH5MdyYQMk6NxqGGyIiIrNjuDEB7gpOREQkHYYbE1BwhWIiIiLJMNyYAFcoJiIikg7DjQnou6XYckNERGR2DDcmUNItpWHLDRERkdkx3JhAyQrFnApORERkfpUKNz///DM2b96s//k///kPnJ2d0alTJ8TExBituOqKKxQTERFJp1LhZuHChbCxsQEAHD16FF9//TU+/PBDuLm5Yc6cOUYtsDpScm8pIiIiySgrc1FcXBzq168PANiwYQOGDh2KyZMnIzQ0FN26dTNmfdXS/dlSbLkhIiIyt0q13Njb2yM1NRUAsGPHDvTq1QsAYG1tjby8PONVV00puc4NERGRZCrVctOrVy9MmjQJwcHBuHLlCvr27QsAuHDhAurWrWvM+qolrlBMREQknUq13Hz99dcICQlBcnIy1q1bB1dXVwBAREQERo4cadQCq6OSXcE5oJiIiMj8KtVy4+zsjK+++qrU8QULFjxxQZZAdW+2FKeCExERmV+lWm62bduGQ4cO6X/++uuv0apVK4waNQp37941WnHVlX5AMWdLERERmV2lws1rr72GzMxMAMD58+fxyiuvoG/fvrhx4wZefvlloxZYHSnZLUVERCSZSnVL3bhxA02aNAEArFu3Dv3798fChQtx6tQp/eDimqxkthS7pYiIiMyvUi03VlZWyM3NBQDs2rULzzzzDADAxcVF36JTk5V0S3FvKSIiIvOrVMtN586d8fLLLyM0NBTHjx/H77//DgC4cuUKateubdQCq6OSAcVc54aIiMj8KtVy89VXX0GpVGLt2rVYsmQJfH19AQBbt25F7969jVpgdaR4YONMIRhwiIiIzKlSLTf+/v7YtGlTqeOfffbZExdkCVTy+5mxSCf0i/oRERGR6VWq5QYAtFot1q1bh/feew/vvfce1q9fD61W+1j3WLRoEdq1awcHBwd4eHhg8ODBiIqKeuR16enpCAsLg7e3N9RqNRo0aIAtW7ZU9qsYnfKBMMP9pYiIiMyrUi03165dQ9++fREfH4+GDRsCKA4qfn5+2Lx5M+rVq1eh++zfvx9hYWFo164dioqK8NZbb+GZZ57BxYsXYWdnV+Y1hYWF6NWrFzw8PLB27Vr4+voiJiYGzs7OlfkqJlHSLQUAGp0ONlBIWA0REVHNUqlwM3PmTNSrVw/Hjh2Di4sLACA1NRVjxozBzJkzsXnz5grdZ9u2bQY/L1++HB4eHoiIiEDXrl3LvOann35CWloajhw5ApVKBQBVbj+rkgHFAKBlyw0REZFZVapbav/+/fjwww/1wQYAXF1dsXjxYuzfv7/SxWRkZACAwX3/6a+//kJISAjCwsLg6emJZs2aYeHCheV2iRUUFCAzM9PgZWoKuQyye403Gq5STEREZFaVCjdqtRpZWVmljmdnZ8PKyqpSheh0OsyePRuhoaFo1qxZuedFR0dj7dq10Gq12LJlC9555x188skneO+998o8f9GiRXByctK//Pz8KlXf4yoZVMwxN0REROZVqXDTv39/TJ48GeHh4RCieLrzsWPHMGXKFAwcOLBShYSFhSEyMhKrV69+6Hk6nQ4eHh5YunQp2rRpg3/961+YO3cuvv322zLPf/PNN5GRkaF/xcXFVaq+x1Uy7obhhoiIyLwqNebmyy+/xPjx4xESEqIf96LRaDBo0CB8/vnnj32/6dOnY9OmTThw4MAjFwH09vaGSqWCQnF/kG7jxo2RmJiIwsLCUi1HarUaarX6sWt6UkqFDNBw80wiIiJzq1S4cXZ2xsaNG3Ht2jVcunQJQHHAqF+//mPdRwiBGTNmYP369di3bx8CAgIeeU1oaChWrlwJnU4H+b2unytXrsDb27vSXWKmwFWKiYiIpFHhcPOo3b737t2r/+dPP/20QvcMCwvDypUrsXHjRjg4OCAxMREA4OTkBBsbGwDAuHHj4Ovri0WLFgEApk6diq+++gqzZs3CjBkzcPXqVSxcuBAzZ86s6FcxC4Wc+0sRERFJocLh5vTp0xU6Tyar+Gq8S5YsAQB069bN4PiyZcswYcIEAEBsbKy+hQYA/Pz8sH37dsyZMwctWrSAr68vZs2ahddff73Cn2sOqge2YCAiIiLzqXC4ebBlxlgqsu/Svn37Sh0LCQnBsWPHjF6PMSnvdUtpOKCYiIjIrCq9/QI9XMkWDEXsliIiIjIrhhsTUZZMBWe3FBERkVkx3JiIUs7ZUkRERFJguDERFbuliIiIJMFwYyL3p4Kz5YaIiMicGG5MpGS2FKeCExERmRfDjYnou6W4/QIREZFZMdyYiELOdW6IiIikwHBjIio5BxQTERFJgeHGRPSL+HHMDRERkVkx3JhIyYBittwQERGZF8ONiXCFYiIiImkw3JgIVygmIiKSBsONiXCFYiIiImkw3JgIVygmIiKSBsONiai4QjEREZEkGG5MpGRAsYYrFBMREZkVw42J3J8KzpYbIiIic2K4MRElVygmIiKSBMONiXCFYiIiImkw3JiIit1SREREkmC4MREFBxQTERFJguHGRErG3HAqOBERkXkx3JgIu6WIiIikwXBjIiUDijWcLUVERGRWDDcmwl3BiYiIpMFwYyLcFZyIiEgaDDcmouSu4ERERJJguDERfcsNBxQTERGZFcONidxfoZgtN0RERObEcGMiKm6/QEREJAmGGxMp6ZbSsFuKiIjIrBhuTIS7ghMREUmD4cZElPdWKOb2C0RERObFcGMi+hWKOaCYiIjIrBhuTOR+txRbboiIiMyJ4cZEuEIxERGRNBhuTETFFYqJiIgkwXBjIgp2SxEREUmC4cZEVPdmS3FAMRERkXkx3JhIyWwpTgUnIiIyL4YbE3lwhWIhGHCIiIjMheHGREqmggNsvSEiIjInhhsTKemWAjgdnIiIyJwYbkykZEAxwHBDRERkTgw3JqJ4oFuKa90QERGZD8ONiTw45kbDtW6IiIjMhuHGRGQymT7gcEAxERGR+TDcmJB+Z3B2SxEREZkNw40JcfNMIiIi82O4MaH7qxSz5YaIiMhcGG5M6MFViomIiMg8GG5MSMmdwYmIiMyO4caE9AOK2S1FRERkNgw3JlSySjGnghMREZkPw40JlXRLcSo4ERGR+TDcmJCCY26IiIjMjuHGhNgtRUREZH4MNybEFYqJiIjMj+HGhPRTwdlyQ0REZDYMNyZ0fxE/ttwQERGZC8ONCd3ffoEtN0REROYiabhZtGgR2rVrBwcHB3h4eGDw4MGIioqq8PWrV6+GTCbD4MGDTVfkEygZUMzZUkREROYjabjZv38/wsLCcOzYMezcuRMajQbPPPMMcnJyHnntzZs38eqrr6JLly5mqLRySqaCc4ViIiIi81FK+eHbtm0z+Hn58uXw8PBAREQEunbtWu51Wq0Wo0ePxoIFC3Dw4EGkp6ebuNLKsVYpAACZeUUSV0JERFRzVKkxNxkZGQAAFxeXh573f//3f/Dw8MDEiRMfec+CggJkZmYavMylmY8jAOBU7F2zfSYREVFNV2XCjU6nw+zZsxEaGopmzZqVe96hQ4fw448/4vvvv6/QfRctWgQnJyf9y8/Pz1glP1L7gOKQduJmGnQcVExERGQWVSbchIWFITIyEqtXry73nKysLIwdOxbff/893NzcKnTfN998ExkZGfpXXFycsUp+pGa+TrC1UiA9V4OoO1lm+1wiIqKaTNIxNyWmT5+OTZs24cCBA6hdu3a5512/fh03b97EgAED9Md09wbrKpVKREVFoV69egbXqNVqqNVq0xT+CCqFHG3q1MLBqykIj05FY29HSeogIiKqSSRtuRFCYPr06Vi/fj327NmDgICAh57fqFEjnD9/HmfOnNG/Bg4ciO7du+PMmTNm7XKqqI6BrgCA4zfTJK6EiIioZpC05SYsLAwrV67Exo0b4eDggMTERACAk5MTbGxsAADjxo2Dr68vFi1aBGtr61LjcZydnQHgoeN0pNTh3rib4zfSIISATCaTuCIiIiLLJmnLzZIlS5CRkYFu3brB29tb//r999/158TGxiIhIUHCKp9M89pOUCvlSMkuxPXkbKnLISIisniSttwI8egZRPv27Xvo+8uXLzdOMSaiVirQ2r8Wjkan4lh0Gup7OEhdEhERkUWrMrOlLFmHwPtdU0RERGRaDDdm0CGgeFBx+I3UCrVWERERUeUx3JhBsL8zrBRy3MksQExqrtTlEBERWTSGGzOwVinQ0s8JQHHrDREREZkOw42Z3O+a4rgbIiIiU2K4MZOSQcXh0Qw3REREpsRwYyat/WtBIZchPj0Pt+5y3A0REZGpMNyYiZ1aiea+98bdsPWGiIjIZBhuzIjr3RAREZkew40Zdbw3qPjQtRTka7QSV0NERGSZGG7MqF2AC5xsVIhPz8Mra85Cp+OCfkRERMbGcGNG9molloxpDZVChs3nErBo6yWpSyIiIrI4DDdm1qmeGz4e3hIA8P3BG1h2+IbEFREREVkWhhsJDGrli//0bggA+L9NF7EtMkHiioiIiCwHw41Epj5VD2M6+kMIYNbqMzgbly51SURERBaB4UYiMpkM8wc0RY9GHigo0uHdjZHcMZyIiMgIGG4kpFTIsXhoC9haKXD2Vga2RiZKXRIREVG1x3AjMXcHNV7qEggA+Gh7FDRancQVERERVW8MN1XAS10D4WpnhRspOVh9Ik7qcoiIiKo1hpsqwF6txMweQQCAL3ZdRU5BkcQVERERVV8MN1XEyPb+8HexRUp2AX48xLVviIiIKovhpoqwUsrx6rPFa998t/86UrMLJK6IiIioemK4qUL6N/dGc18n5BRq8b8916Quh4iIqFpiuKlC5HIZ3ujTCADw27EY7ItKkrgiIiKi6ofhpooJre+GIcG+KNIJTP3tFCJi7kpdEhERUbXCcFMFfTC0Bbo2cEeeRosXl5/AlTtZUpdERERUbTDcVEFWSjm+HdMarfyckZGnwbgfj+PW3VyDczJyNcjI00hUIRERUdUlEzVsQ6PMzEw4OTkhIyMDjo6OUpfzUHdzCjH8u6O4lpSNQDc79GriicuJWYhKzEJiZj7srBTYPLML6rrZSV0qERGRST3O72+23FRhteys8OvE9vB1tkF0Sg6+OxCN/VeSkZiZDwDIKdTii91XJa6SiIioalFKXQA9nLeTDX6d2B6f7LwCVzsrNPRyQCMvB2i0AiOWHsOGM/EI614P9T0cpC6ViIioSmC4qQYC3e3x9ajWpY4/08QTOy7ewWe7rpb5PhERUU3EbqlqbE6vBgCAzecScCkhU+JqiIiIqgaGm2qssbcj+rXwBgB8tvOKxNUQERFVDQw31dycnkGQyYAdF+/g/K0MqcshIiKSHMNNNVffwwGDWvoAAD7bdb/1JiNPg22RifjjZByKtDqpyiMiIjI7Dii2ALN6NsDf5xKw53IS5m2MxLn4DJyNS4fu3gpGp2PTsXBIM8hkMmkLJSIiMgO23FiAADc7PBfsCwD4+WgMTscWB5sANzvIZcCq47H4ei93GSciopqBLTcW4tVnGyIxMx9ONip0CXJD5yB3+Drb4JejN/Huxgv4eMcVeDnZYFib2lKXSkREZFIMNxbC09Eav07sUOr4uJC6uJ2ej2/3X8cb687Bw0GNrg3cJaiQiIjIPNgtVQP859mGGNzKB0U6gam/RSAynrOqiIjIcjHc1AByuQwfDmuJTvVckVOoxfPfHcWvx2JQw/ZMJSKiGoLhpoawUsrx7dg2CAl0RW6hFu9siMTYH48jPj1P6tKIiIiMiuGmBnG0VmHFpA6YN6AJrFVyHLqWgt6fHcDvJ2KRW1gkdXlERERGIRM1rG8iMzMTTk5OyMjIgKOjo9TlSCY6ORuvrDmL07HpAACFXIZGXg4I9ndGa/9a6FzfDR6O1tIWSUREdM/j/P5muKnBtDqBHw5GY9nhm0jMzDd4z0ohx6gO/pjWvR48HO6HHJ1O4MDVZKw6Hgs7tRIfDG0BlYINgEREZFoMNw/BcFO2hIw8nI5Nx6mYuzh2IxWR8cW7jFur5BjfqS7GdKiDXZfu4JejMbiRkqO/7pVeDTCjR5BUZRMRUQ3BcPMQDDePJoTA4Wup+HhHFM7EpZd630GtREg9V+y4eAcqhQx/z+iMRl58lkREZDqP8/ub/QlUikwmQ+cgN6yf1gk/TWiLpj7F/xIFedjjv4Ob4dhbPfDd2Dbo2dgTGq3Aq2vOQsPNOYmIqIrgCsVULplMhqcbeaJbAw/czS2Ei52VweabC4c0w4mbaYiMz8TSA9EI615fwmqJiIiKseWGHkkul8HVXl1qV3EPR2vMH9gEAPD5riuISsySojwiIiIDDDf0RAa38kXPxh767qkidk8REZHEGG7oichkMiwc0hyO1kqcj8/Au39d4PgbIiKSFMMNPTEPR2v8d3AzAMDK8FiM/iEcyVkFEldFREQ1FcMNGcWgVr74bmwb2KuVOH4jDf3/dxCnYu8+1j2y8jX4YtdVtH1vF97887yJKiUiIkvHcENG82xTL2wIC0U9dzvcySzAiO+O4bcK7D6eW1iEb/dfR5cP9+KzXVeQkl2A1SdiuaknERFVCsMNGVV9D3tsnN4ZvZt6oVCrw9sbIjH826OIjM8odW56biG+238dXT/ch8VbLyM9V4NAdzsEedhDCODPiFsSfAMiIqruuEIxmYQQAj8cvIFPd15BnkYLmQwY0c4Prz7TEMnZBfj5yE2sPx2PfE3x4GM/FxvM7tEAg1r5YOOZ23hlzVnUcbXFvle7lZqCTkRENQ+3X3gIhhvzSsjIw+Ktl7HxzG0AgFopR0HR/dlUjb0d8UKnuhjS2le/AWduYRHav78b2QVF+H1yR3QIdC1135jUHFirFPDkzuVERDUCt1+gKsPbyQZfjAjGmikhaOrjiIIiHRRyGfo198Yf/w7Blpmd8Xw7P4OdxW2tlOjX3BsAsLaMrqmLtzPR69MD6PLBXny15yoKi8qeen45MRMnb6aV+z4REVkmttyQ2Wh1AmdvpcPbyRreTjYPPffEzTQM//YobK0UODG3J+zUSv09hi45YrChZyMvBywe2gKt/JxRWKTD1sgELD9yE6dji8+xs1IgpJ4bnmrghq4N3FHH1c5UX5GIiEzkcX5/c28pMhuFXIbW/rUqdG7bOrUQ4GaHGyk52HI+AcPb+gEAVobH4ExcOuzVSrzeuyE+23UVlxOzMOSbw+jb3BsnbqQh6d4aOyqFDA7WKqTlFGLXpTvYdekOAKCOqy2eauCOpxq4o2Ogqz44lSe3sAiaIgEnW9UTfHsiIjIXttxQlfX13mv4aHsU2ge44I9/hyApMx89PtmPrIIiLBjYFOM71UVaTiHe23QRf56O11/n7qDGmA51MLKDH9zs1LiYkIn9V5Jx4EoyImLuokh3/195K4Ucjbwd4GSjgr1aCXu1ErZWCqTkFOJWWi5u3c1Dak4hAKBNnVro29wbfZt7PbLliYiIjKvaDChetGgR/vzzT1y+fBk2Njbo1KkTPvjgAzRs2LDca77//nv88ssviIyMBAC0adMGCxcuRPv27Sv0mQw31UdCRh5CF++BTgD7X+uGD7dHYfO5BLSs7YQ/p4VCIb8/i+rAlWT8eeoWujfyQJ9m3rBSlj2cLLugCEeupeDA1WTsv5KMuLTKraXT2t8ZM54OQvdGHpW6HiieURaXlgcnGxVbhYiIHqHahJvevXtjxIgRaNeuHYqKivDWW28hMjISFy9ehJ1d2eMiRo8ejdDQUHTq1AnW1tb44IMPsH79ely4cAG+vr6P/EyGm+pl3E/HceBKMjoEuCD8RhrkMuCv6Z3RzNfpie8thMDN1FxcS8pGdoEG2flFyCooQk5BEWrZWsHPxRa1a9mgdi1b5BVqsTUyAVvOJ+BkzF0IASjlMvzyYnt0qu9W4c+8k5mPw9dScPhaKo5cT0FCRj7USjnmD2yKEe38nmjae16hFteTs3EtqfgVm5aLpxq4Y2ib2pW+JxFRVVFtws0/JScnw8PDA/v370fXrl0rdI1Wq0WtWrXw1VdfYdy4cY88n+Gmevnr7G3MXHVa//OkzgF4u38TCSsqDigL/r6ALecT4WCtxPppnVDfw+Gh19zNKcR//9F9BgAyGVDy/8BBrXzw/pDmsH9gDFBaTiE2nI5HZHwG0vM0SM8tRHqeBpl5RdBoddDpBLRCQKsTBlPsHzQ+pA7e6d8ESkXp1qzo5Gw42qjgZq9+zKdARGRe1XZAcUZG8Sq2Li4uFb4mNzcXGo2m3GsKCgpQUHB/E8fMzMwnK5LM6pkmnnC0ViIzvwg+TtaY06uB1CXB09Eanz7fCkmZ4TgZcxcTlp3A+mmhcHcoHRCEENh8PgHzNl5Aak4hZDKgua8TOtVzQ2h9V7SpUws/H4nBxzuisPHMbZy/lYEvRwYjLacQv5+Iw46LidBoK/73j1q2KgR5OKCehz2sFDL8fDQGPx+NQWxaLv43qrU+OEUnZ+Oj7VHYGpkIFzsrrJ0SgkB3e6M9IyIiKVWZlhudToeBAwciPT0dhw4dqvB106ZNw/bt23HhwgVYW5de0G3+/PlYsGBBqeNsuak+vth1FV/vu4alY9ugW8PKj3ExtrScQjz3zWHcTM1FSz9nrH6pI2ysFPr3EzPy8e7GSOy4WDxLK8jDHh8Ma1HmjLGTN9MwY9VpJGTkl3qvua8Tejfzgpu9FZxsrOBsq4KjtQpWSjkUchkUMhnkcsDOSoladlYG1249n4DZv59BQZEOjbwc8OGwFvjjZBxWHY+D9oGB1X4uNlg3tRM8HEr/fyg1uwBaIcp8z9Q0Wh3i7+ZBJwRq2VrB0UZlMNaqsvI1Why/kYaImLto6OWApxt5wFqlePSFRCSZatktNXXqVGzduhWHDh1C7doVGyOwePFifPjhh9i3bx9atGhR5jlltdz4+fkx3FQzRVpdmd0qUruRkoMh3xxGeq4GTzVwRyMvB1y5k4Urd7L1G38q5TJM614fYd3rQa0s/xfo3ZxCvLLmLPZcToKTjQpDgn3xfFs/NPF5sn9Pz8SlY9LPJ5GSXWBw/OlGHpjUJQBv/nkeMam5aObriNWTQ/StOzqdwC9Hb+KDbVEoKNKiVxNPjA+pi5B6rg8dGySEwNWkbJyJTUdLP2c09Hp4l12JwiId9kUl4dC1FNxIyUFMai7i0/MMQphMBjjZqODtZIPXnm2Apxt5Vvg5JGcVYFtkAvZFJePI9VTkabT69xytlejf0gdDW/uitX8tbvlBVAVVu3Azffp0bNy4EQcOHEBAQECFrvn444/x3nvvYdeuXWjbtm2FP4tjbsjYjt9Iw5gfwlGoLT3mJdjfGQuHNEdj74r9uyaEwOXELAS42Rm1JeHW3Vy8uPwErtzJRks/Z7zZpxE63tvW4mZKDoYuOYLUnEJ0CXLDj+PbISEjD6+tPYfjN9JK3au+hz3GdqyDQHc7WCnkUKsUUCvliE3Lxb6oZOyPSsLtB1qg+jX3xqyeQWjgWTrkCCFw4XYm1kbcwl9nbyPt3rT7B1mr5FDK5cguKDI4LpMBb/RuhMldAx8ZRo5eT8XUFRFIz9Xoj3k5WqNdgAtO3kwzaDHzdbZB6zq10MLXCS1qO6GZr9Mj10IiItOrNuFGCIEZM2Zg/fr12LdvH4KCgip03Ycffoj3338f27dvR8eOHR/rMxluyBR2XEjEyuOx8HexRZCnAxp42KOBp0OpbiIp5Wu0uJaUjaY+jqXCwNm4dIxYegx5Gi3a13XB+fgM5Gm0sLVS4M0+jdAh0BW/Ho3BulO3kFuoLecT7lMr5Wjk5YCzt4rH0clkxSHn+bZ+uJOZj+iUHNxIzkHUnSzcSMnRX+dmr0b/Ft5o4u2IOq62qOtmBw8HNWQyGQqLdEjPK0R6rgbLDt/EquOxAIChrWtj4XPNym0VW3U8Fu9siESRTqCBpz0GB/uie0MPNPJygEwmg1YncCw6FetO3cK2yMRS308uK/6M94aU/xlVmRAClxKysC0yAY29HdHn3tYm/1RYpMOra87ienI2PB2t773U8HGyQY/GHnDloPMKKSjSYsm+63C2UWFCaMX+sk4VU23CzbRp07By5Ups3LjRYG0bJycn2NgUL5I2btw4+Pr6YtGiRQCADz74AO+++y5WrlyJ0NBQ/TX29vawt3/0gEiGG6Ky7Y1KwqSfT+q7gToGuuCjYS3h52KrPyczX4M/I25h8/kEZOUXoVCrQ4FGh0KtDg7WSnQNcsdTDd0REugKa5UClxMz8cWuq9gamVju51op5ejVxBPDWtdGlyC3CnU/CiHwy9EY/N+mi9DqBFr7O+O7sW0NBnVrdQLvb76Enw7fAAAMaOmDj4a1eGiLWE5BESJi7uJ8fAbOxqXj3K0MJGYWt+qE1nfFt2PawMHadGsSrQyPReTtDLSrWwshgW7wcnr0OKe0nEL8diwGRTqBOi62qOtmC38XOxTpdNh45jbWn4pH1J0sAMUh86fx7cpcn+ntDefx27HYMj/DWiXHiHb+eKlrIHydi//bLITAyZi7+P1EHPZfSUZDTweMaO+HZ5p4lbvOlKXLyNNg8i8nEX6vxXNjWCha+jlLW5QFqTbhprym5GXLlmHChAkAgG7duqFu3bpYvnw5AKBu3bqIiYkpdc28efMwf/78R34mww1R+Taeice3+6Mxqr0fRneoA7kRBu8CxZud/m/PVZyPz4C/iy0C3e0Q6GaPQHc7BPvXgpNN5QLDwavJCFtxCpn5RZDJAB8nG/i72KKOqy1i03Jx5HoqAODlXg0w4+n6lRpLc+BKMqb+FoGcQi2a+jhi2Qvt9IOrs/I1WBEei5XhsbC1UmBYm9oY2rp2pVrsNp9LQNjKUwbHAtzs0DHQFX2beyG0npvBn4cQAutOxeP9zRdx94HutrJYKeQIcLND1J0sOKiVWB9muHzB2ohbeHXNWQDAu/2bwMZKgcSMfCRl5eNsXAYuJhTPMlXKZRjUyheB7nZYF3EL0Q+0upVwsbPC0Na+GN7WD0Ee9pUev5SVr8HSA9HYfD4BEIBKIYdKKYNKIUd9d3uM71TXKOtdGcvt9DxMWHYcV+5k648929QT340tPWxCCIEPtkXhdnoe3u7f+IkG6+t0AjmFRSYN3VVFtQk3UmC4IbIs0cnZmLbiFC4nZpV6z1olx6fPt0LfcrpiKur8rQxMWHYcqTmF8HMp3ul+z6Uk/HL0JjLzDccCWSnk6N3MCwNa+iA9txDXk3MQnZyNGyk5aODlgPcHN4OzrWH4uZaUjUFfHUJOoRZdgtyQnqvBhdsZeGAsNXydbTC8bW0Mb+uHvEIt3t5wHseii1sIGno6oHUdZ8Sk5iImNRe3M/IgBNC+rguGtPZF32besLFSYMwP4Th+Mw0BbnbYMC0UTrYqRMZnYOiSIygo0mFWj6BSyy0IIXDoWgqW7LuuD4slbK0U6NfcG/1aeCMi5i7+OBmHO5n3B657OqrRMdAVIYGuCKnnCn8X20eGHY1Wh1XHY/HFrqv6rU/K076uC17sXBe9mngZZRYdUNw6eTs9D/F383A7PQ9qlQIDW/o8tMXvcmImJvx0AomZ+fB0VGNuvyaYtfo0hAB2zumKoH+MN9t+IRH//jUCAODtZI3vx7WtVFATQmDW6jP46+xttKhdPKuyd1Mvky3rkJGnwcrwWAR52KNLAzezd9My3DwEww2R5RFCICW7EDGpxbOsYtJykZmnwfC2tdHUxzh/u7+ZkoNxPx1HbFquwfF67nb491P1UFCkw+rjsbhw++FradX3sMfPL7bXd+/kFBRh0NeHcS0pGx0CXLBiUgcoFXJk5Glw4kYa9l1Jwl9nbutDlEwGKGQyFOkErFVyzO7ZABM7B0D1QHdevkaLAo2u1LYeKdkFGPTVYcSn56FLkBs++1crDP76MG7dzUP3hu74cXy7h7bWnY69ix8P3UBGngb9W3ijXwsfg0Uni7Q67I1KxurjsTh4NaXUIPtmvo6Y3LUe+jbzKtX9mFeoxY6Lifh811X9OKxANzvM7tUAXo7W0GiLuz/zC7XYdiERm88l6PeJ83W2QSs/Z/jWsoGvc/FLJgOuJ2cjOjkH15OzEX83D2NC6mBat/plfrezcemYvupUmVuydAlyw9KxbQ2Weihx8Goypv12ClkFRQjysMfye3+2//71JLZfuIPnWvvi0+dbGfzZ9Px0P27dzYONSoE8jRbWKjk+Gd4K/Vo8XgjfeCYes1afKXW8gac9+jX3weBgH9RxLXu1/xJCCFxMyMTuS0m4npyN51rXxlMN3Eud98+WKQe1Er2aeKJfC290DjJP0GG4eQiGGyKqrOSsAryw/Dgi4zPRorYTpnWrh2eaeBkEgvO3MrDqRCyORafC28ka9dztEehmB3cHa7y3+SISMvLh5WiN5S+2Q0NPB8xYdRqbziXAw0GNTTM7l9lFka/RYvuFRPx+Ik7fetK9oTv+b1AzgzFRFXHhdgaGLTmKPI0WLnZWSMsphL+LLf6e3tmoe5zla7Q4FXMXx6JTcTQ6FWfi0vULUtauZYOXugSiVxNPHLmeih0XEnHgajLyNcVhyM3eCrN7NsC/2vkZhLYHJWbk49djN7EyPPaR3XIPmtOzAWb1NJy8cu5WOkb/EI6sewGylq0KPs428HayweFrKcjTaBES6IofJ7SFrVVxmBNC4LsD0fhw22Xo7rWSfT+urf4Zno1Lx6CvD0Mpl2Hfa91Qu1bxn9MXu67is11X4O1kjQ1hofjP2nPYfyUZADCzRxBm9wiqUHdwclYBen22H+m5Gvy7ayDqutlha2QijlxLMdgcONjfGUOCffFMEy9ohcDdnELczS1EanYhTtxMw57LSaXW13oxNACv92moDywPtky52VtBIZcZtNDZq5XoXN8N3Rq6o1tDjwqNFasMhpuHYLghoidRWKRDbFoO6rk//niS2+l5GP/TcVxNyoaDtRIDWvpgZXgslHIZVk/uiLZ1H706e1xaLrILivSzvSpj6/kETF1RPL7HWiXHn1NDn3g9pUdJyynEr0dj8PPRm2VO+QeKQ8+wNrUxqUugQYvQw+RrtDh4NQWxabmIv5uH+PRc3Lpb3C0X6G5XHC7d7XAjJQef77oKoHgM1swexQEnMj4Do74/hsz8IrStUws/jG9r0G144mYaXlh2AtkFRWhXtxZ+mtAOcpkM/1l7rng8EIDhbWrjv4Obleq6GvNDOA5dS8H4kDpYMKgZbt3NRY9P9qOgSIf/jQzGgJY+0OoEFm25hB8OFQ98d3co7srrGOiCjoGuCHSzK/XnLITAlN8isP3CHTTxdsTG6aH6EJiRq8GuS3ew8extHLqabNC1WR5rlRyd67vDyUaFdaduAQAaezviyxGtkJJdiMm/nkRWfhHqe9hj+Qvt4ONkg4jYu9h8LgFbIxMMgk7JtT0be+DlXg2MumYUw81DMNwQkZQycjWY9MsJnLh5V3/s3f5N8GJn804bXnrgOpYeiMb8gU3Rv4WP2T43r1CLtadu4YeD0YhJzUVjb0c808QTzzb1QmPvyge2ivhu/3Us2noZQHHA6dHYA6O+D0dGnqZ4K5QX25cZqk7H3sW4n44jK78ILWs7IU+jxZU72VApZJg3oClGd/Avs+4j11Iw6odwqJVyHHr9aby7MRJbIxPRMdAFq17qaHDNHyfjMP+vC6WWIvBzscF/nm2E/i289ef/ffY2Zqw6DaVcho3TQ8vtek3KysffZxOw4XQ8zsdnQKWQoZatFVzsrFDL1gqB7nbo0dgDneq56YPZ7kt38Nrac0jLKYS1Sg6dDijU6soMfkDxgObI2xnYezkZe6OScPZWOoQA2tSphXVTOz3Gn86jMdw8BMMNEUktX6PFrNWnsf3CHQxs6YMvRrSSZFVkIYRkqzHrdALZhUVwNPMsn2/3X8fiewHHWiVHvkaHYH9n/PJi+4fOOIqMz8CYH8P1C0F6OKixZEwbtKlTejuVEkIIDPnmCM7EpaNTPVccuZ4KhVyGzTM7o5FX6d8/+RotzsSl41h0Ko5Fp+JUbDoK722I2yHABfMHNoW7gxq9Pt2Pu7maMgeAlydfo4VaKa/Qn3dSVj5e+eMsDl5NAQD0buqFz0e0qtDCoqnZBTh4NQW2Vgo809SrQrVVFMPNQzDcEFFVoNMJXE/ORj13e6NNuaeKeTDgtPRzxq8T21coZF1OzMSUXyPg52KLT4a3hIfjo8eW7LiQiMn3ZkYBwIROdTF/YNMK1ZlXqMX3B6Pxzb5ryNfoIJcBdVyLu9gaeTngr+mdTbamkE4nsDbiFrILijC+U12jzUZ7Egw3D8FwQ0REf5yMw/lbGXj12YaPtc7S47Z26XQCvb84gCt3suFiZ4W9r3R77IHbt+7mYtGWy/oxPgq5DBvDQqvUOj/mwHDzEAw3RERkTgeuJOONdefw7oAm6N2s8msuHbmWgu8PRqNPM288387PiBVWDww3D8FwQ0REVP08zu/vmrkBCBEREVkshhsiIiKyKAw3REREZFEYboiIiMiiMNwQERGRRWG4ISIiIovCcENEREQWheGGiIiILArDDREREVkUhhsiIiKyKAw3REREZFEYboiIiMiiMNwQERGRRWG4ISIiIouilLoAcxNCACjeOp2IiIiqh5Lf2yW/xx+mxoWbrKwsAICfn5/ElRAREdHjysrKgpOT00PPkYmKRCALotPpcPv2bTg4OEAmkxn13pmZmfDz80NcXBwcHR2Nem8yxGdtPnzW5sNnbT581uZjrGcthEBWVhZ8fHwglz98VE2Na7mRy+WoXbu2ST/D0dGR/2cxEz5r8+GzNh8+a/PhszYfYzzrR7XYlOCAYiIiIrIoDDdERERkURhujEitVmPevHlQq9VSl2Lx+KzNh8/afPiszYfP2nykeNY1bkAxERERWTa23BAREZFFYbghIiIii8JwQ0RERBaF4YaIiIgsCsONkXz99deoW7curK2t0aFDBxw/flzqkqq9RYsWoV27dnBwcICHhwcGDx6MqKgog3Py8/MRFhYGV1dX2NvbY+jQobhz545EFVuOxYsXQyaTYfbs2fpjfNbGEx8fjzFjxsDV1RU2NjZo3rw5Tp48qX9fCIF3330X3t7esLGxQc+ePXH16lUJK66etFot3nnnHQQEBMDGxgb16tXDf//7X4O9ifisK+/AgQMYMGAAfHx8IJPJsGHDBoP3K/Js09LSMHr0aDg6OsLZ2RkTJ05Ednb2kxcn6ImtXr1aWFlZiZ9++klcuHBBvPTSS8LZ2VncuXNH6tKqtWeffVYsW7ZMREZGijNnzoi+ffsKf39/kZ2drT9nypQpws/PT+zevVucPHlSdOzYUXTq1EnCqqu/48ePi7p164oWLVqIWbNm6Y/zWRtHWlqaqFOnjpgwYYIIDw8X0dHRYvv27eLatWv6cxYvXiycnJzEhg0bxNmzZ8XAgQNFQECAyMvLk7Dy6uf9998Xrq6uYtOmTeLGjRtizZo1wt7eXnzxxRf6c/isK2/Lli1i7ty54s8//xQAxPr16w3er8iz7d27t2jZsqU4duyYOHjwoKhfv74YOXLkE9fGcGME7du3F2FhYfqftVqt8PHxEYsWLZKwKsuTlJQkAIj9+/cLIYRIT08XKpVKrFmzRn/OpUuXBABx9OhRqcqs1rKyskRQUJDYuXOneOqpp/Thhs/aeF5//XXRuXPnct/X6XTCy8tLfPTRR/pj6enpQq1Wi1WrVpmjRIvRr18/8eKLLxoce+6558To0aOFEHzWxvTPcFORZ3vx4kUBQJw4cUJ/ztatW4VMJhPx8fFPVA+7pZ5QYWEhIiIi0LNnT/0xuVyOnj174ujRoxJWZnkyMjIAAC4uLgCAiIgIaDQag2ffqFEj+Pv789lXUlhYGPr162fwTAE+a2P666+/0LZtWwwfPhweHh4IDg7G999/r3//xo0bSExMNHjWTk5O6NChA5/1Y+rUqRN2796NK1euAADOnj2LQ4cOoU+fPgD4rE2pIs/26NGjcHZ2Rtu2bfXn9OzZE3K5HOHh4U/0+TVu40xjS0lJgVarhaenp8FxT09PXL58WaKqLI9Op8Ps2bMRGhqKZs2aAQASExNhZWUFZ2dng3M9PT2RmJgoQZXV2+rVq3Hq1CmcOHGi1Ht81sYTHR2NJUuW4OWXX8Zbb72FEydOYObMmbCyssL48eP1z7Os/6bwWT+eN954A5mZmWjUqBEUCgW0Wi3ef/99jB49GgD4rE2oIs82MTERHh4eBu8rlUq4uLg88fNnuKFqISwsDJGRkTh06JDUpVikuLg4zJo1Czt37oS1tbXU5Vg0nU6Htm3bYuHChQCA4OBgREZG4ttvv8X48eMlrs6y/PHHH1ixYgVWrlyJpk2b4syZM5g9ezZ8fHz4rC0cu6WekJubGxQKRalZI3fu3IGXl5dEVVmW6dOnY9OmTdi7dy9q166tP+7l5YXCwkKkp6cbnM9n//giIiKQlJSE1q1bQ6lUQqlUYv/+/fjyyy+hVCrh6enJZ20k3t7eaNKkicGxxo0bIzY2FgD0z5P/TXlyr732Gt544w2MGDECzZs3x9ixYzFnzhwsWrQIAJ+1KVXk2Xp5eSEpKcng/aKiIqSlpT3x82e4eUJWVlZo06YNdu/erT+m0+mwe/duhISESFhZ9SeEwPTp07F+/Xrs2bMHAQEBBu+3adMGKpXK4NlHRUUhNjaWz/4x9ejRA+fPn8eZM2f0r7Zt22L06NH6f+azNo7Q0NBSSxpcuXIFderUAQAEBATAy8vL4FlnZmYiPDycz/ox5ebmQi43/DWnUCig0+kA8FmbUkWebUhICNLT0xEREaE/Z8+ePdDpdOjQocOTFfBEw5FJCFE8FVytVovly5eLixcvismTJwtnZ2eRmJgodWnV2tSpU4WTk5PYt2+fSEhI0L9yc3P150yZMkX4+/uLPXv2iJMnT4qQkBAREhIiYdWW48HZUkLwWRvL8ePHhVKpFO+//764evWqWLFihbC1tRW//fab/pzFixcLZ2dnsXHjRnHu3DkxaNAgTk+uhPHjxwtfX1/9VPA///xTuLm5if/85z/6c/isKy8rK0ucPn1anD59WgAQn376qTh9+rSIiYkRQlTs2fbu3VsEBweL8PBwcejQIREUFMSp4FXJ//73P+Hv7y+srKxE+/btxbFjx6QuqdoDUOZr2bJl+nPy8vLEtGnTRK1atYStra0YMmSISEhIkK5oC/LPcMNnbTx///23aNasmVCr1aJRo0Zi6dKlBu/rdDrxzjvvCE9PT6FWq0WPHj1EVFSURNVWX5mZmWLWrFnC399fWFtbi8DAQDF37lxRUFCgP4fPuvL27t1b5n+jx48fL4So2LNNTU0VI0eOFPb29sLR0VG88MILIisr64lrkwnxwFKNRERERNUcx9wQERGRRWG4ISIiIovCcENEREQWheGGiIiILArDDREREVkUhhsiIiKyKAw3REREZFEYboioRpLJZNiwYYPUZRCRCTDcEJHZTZgwATKZrNSrd+/eUpdGRBZAKXUBRFQz9e7dG8uWLTM4plarJaqGiCwJW26ISBJqtRpeXl4Gr1q1agEo7jJasmQJ+vTpAxsbGwQGBmLt2rUG158/fx5PP/00bGxs4OrqismTJyM7O9vgnJ9++glNmzaFWq2Gt7c3pk+fbvB+SkoKhgwZAltbWwQFBeGvv/7Sv3f37l2MHj0a7u7usLGxQVBQUKkwRkRVE8MNEVVJ77zzDoYOHYqzZ89i9OjRGDFiBC5dugQAyMnJwbPPPotatWrhxIkTWLNmDXbt2mUQXpYsWYKwsDBMnjwZ58+fx19//YX69esbfMaCBQvw/PPP49y5c+jbty9Gjx6NtLQ0/edfvHgRW7duxaVLl7BkyRK4ubmZ7wEQUeU98dabRESPafz48UKhUAg7OzuD1/vvvy+EKN4RfsqUKQbXdOjQQUydOlUIIcTSpUtFrVq1RHZ2tv79zZs3C7lcLhITE4UQQvj4+Ii5c+eWWwMA8fbbb+t/zs7OFgDE1q1bhRBCDBgwQLzwwgvG+cJEZFYcc0NEkujevTuWLFlicMzFxUX/zyEhIQbvhYSE4MyZMwCAS5cuoWXLlrCzs9O/HxoaCp1Oh6ioKMhkMty+fRs9evR4aA0tWrTQ/7OdnR0cHR2RlJQEAJg6dSqGDh2KU6dO4ZlnnsHgwYPRqVOnSn1XIjIvhhsikoSdnV2pbiJjsbGxqdB5KpXK4GeZTAadTgcA6NOnD2JiYrBlyxbs3LkTPXr0QFhYGD7++GOj10tExsUxN0RUJR07dqzUz40bNwYANG7cGGfPnkVOTo7+/cOHD0Mul6Nhw4ZwcHBA3bp1sXv37ieqwd3dHePHj8dvv/2Gzz//HEuXLn2i+xGRebDlhogkUVBQgMTERINjSqVSP2h3zZo1aNu2LTp37owVK1bg+PHj+PHHHwEAo0ePxrx58zB+/HjMnz8fycnJmDFjBsaOHQtPT08AwPz58zFlyhR4eHigT58+yMrKwuHDhzFjxowK1ffuu++iTZs2aNq0KQoKCrBp0yZ9uCKiqo3hhogksW3bNnh7exsca9iwIS5fvgygeCbT6tWrMW3aNHh7e2PVqlVo0qQJAMDW1hbbt2/HrFmz0K5dO9ja2mLo0KH49NNP9fcaP3488vPz8dlnn+HVV1+Fm5sbhg0bVuH6rKys8Oabb+LmzZuwsbFBly5dsHr1aiN8cyIyNZkQQkhdBBHRg2QyGdavX4/BgwdLXQoRVUMcc0NEREQWheGGiIiILArH3BBRlcPeciJ6Emy5ISIiIovCcENEREQWheGGiIiILArDDREREVkUhhsiIiKyKAw3REREZFEYboiIiMiiMNwQERGRRWG4ISIiIovy/6rNYifN15vzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "auto_encoder1=models.Sequential()\n",
    "auto_encoder1.add(layers.Input(shape=(clean_specs.shape[-1],)))\n",
    "auto_encoder1.add(layers.Dense(3000))\n",
    "auto_encoder1.add(layers.Dropout(drop_out))\n",
    "auto_encoder1.add(layers.Dense(1500))\n",
    "auto_encoder1.add(layers.Dropout(drop_out))\n",
    "auto_encoder1.add(layers.Dense(3000))\n",
    "auto_encoder1.add(layers.Dropout(drop_out))\n",
    "auto_encoder1.add(layers.Dense(clean_specs.shape[-1]))\n",
    "auto_encoder1.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder1.summary()\n",
    "\n",
    "auto_encoder1,history1=train_model(auto_encoder1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 1024)              2561024   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 320)               82240     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 2500)              802500    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,790,660\n",
      "Trainable params: 3,790,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2549545.5000\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2383092.0000\n",
      "Epoch 3/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2366617.7500\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2352778.5000\n",
      "Epoch 5/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2341553.5000\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2336948.7500\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2331921.0000\n",
      "Epoch 8/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2328187.0000\n",
      "Epoch 9/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2322924.5000\n",
      "Epoch 10/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2321229.2500\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2317581.2500\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2314522.0000\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2311618.0000\n",
      "Epoch 14/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2310217.0000\n",
      "Epoch 15/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2309257.5000\n",
      "Epoch 16/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2307174.5000\n",
      "Epoch 17/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2305198.5000\n",
      "Epoch 18/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2301796.0000\n",
      "Epoch 19/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2302849.5000\n",
      "Epoch 20/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2300759.5000\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2299491.0000\n",
      "Epoch 22/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2299322.7500\n",
      "Epoch 23/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2296476.2500\n",
      "Epoch 24/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2294738.2500\n",
      "Epoch 25/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2294961.0000\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2294320.0000\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2293461.7500\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2292042.0000\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2290691.2500\n",
      "Epoch 30/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2290427.7500\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2289230.2500\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2288170.0000\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2288084.2500\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2287234.2500\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2287541.5000\n",
      "Epoch 36/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2285627.5000\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2285647.7500\n",
      "Epoch 38/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2285303.2500\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2286502.2500\n",
      "Epoch 40/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2284256.2500\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2284009.5000\n",
      "Epoch 42/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2281134.2500\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2283160.5000\n",
      "Epoch 44/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2281278.7500\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2281317.0000\n",
      "Epoch 46/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2279581.5000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2280661.5000\n",
      "Epoch 48/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2279852.7500\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2278709.0000\n",
      "Epoch 50/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2279259.0000\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2278175.2500\n",
      "Epoch 52/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2278619.2500\n",
      "Epoch 53/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2277796.2500\n",
      "Epoch 54/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2278855.2500\n",
      "Epoch 55/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276757.2500\n",
      "Epoch 56/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276115.2500\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276033.7500\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2276114.5000\n",
      "Epoch 59/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275756.7500\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275523.2500\n",
      "Epoch 61/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274586.0000\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2275508.0000\n",
      "Epoch 63/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274740.7500\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274444.5000\n",
      "Epoch 65/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2274908.5000\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272724.2500\n",
      "Epoch 67/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273698.2500\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273445.0000\n",
      "Epoch 69/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272407.2500\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2273775.2500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271438.2500\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272034.2500\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272556.7500\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271094.7500\n",
      "Epoch 75/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271076.5000\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271713.2500\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2272359.0000\n",
      "Epoch 78/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270441.7500\n",
      "Epoch 79/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2271375.5000\n",
      "Epoch 80/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2269336.7500\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2271028.2500\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270370.2500\n",
      "Epoch 83/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2270738.0000\n",
      "Epoch 84/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2269310.7500\n",
      "Epoch 85/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268819.0000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2269207.2500\n",
      "Epoch 87/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268051.2500\n",
      "Epoch 88/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2270018.2500\n",
      "Epoch 89/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2268426.0000\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267538.7500\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267914.5000\n",
      "Epoch 92/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268084.7500\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268298.0000\n",
      "Epoch 94/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268171.7500\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2267609.0000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - 9s 3ms/step - loss: 2268215.5000\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2266053.7500\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2266667.2500\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2267202.2500\n",
      "Epoch 100/100\n",
      "3189/3189 [==============================] - 10s 3ms/step - loss: 2266912.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "auto_encoder2=models.Sequential()\n",
    "auto_encoder2.add(layers.Input(shape=(clean_specs.shape[-1],)))\n",
    "auto_encoder2.add(layers.Dense(1024))\n",
    "auto_encoder2.add(layers.Dense(256))\n",
    "auto_encoder2.add(layers.Dense(128))\n",
    "auto_encoder2.add(layers.Dense(64))\n",
    "auto_encoder2.add(layers.Dense(128))\n",
    "auto_encoder2.add(layers.Dense(256))\n",
    "auto_encoder2.add(layers.Dense(320))\n",
    "auto_encoder2.add(layers.Dense(clean_specs.shape[-1]))\n",
    "auto_encoder2.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder2.summary()\n",
    "\n",
    "auto_encoder2,history2=train_model(auto_encoder2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 25, 25, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 12, 12, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,095,045\n",
      "Trainable params: 6,095,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - 20s 5ms/step - loss: 2736520.7500\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736506.2500\n",
      "Epoch 3/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736505.0000\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736502.5000\n",
      "Epoch 5/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736506.0000\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736507.5000\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736506.0000\n",
      "Epoch 8/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736504.5000\n",
      "Epoch 9/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736505.7500\n",
      "Epoch 10/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736503.5000\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736502.2500\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736502.2500\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - 18s 5ms/step - loss: 2736504.5000\n",
      "Epoch 14/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736501.2500\n",
      "Epoch 15/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736503.7500\n",
      "Epoch 16/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736504.0000\n",
      "Epoch 17/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736501.7500\n",
      "Epoch 18/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736505.7500\n",
      "Epoch 19/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.2500\n",
      "Epoch 20/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736502.2500\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736502.5000\n",
      "Epoch 22/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736503.0000\n",
      "Epoch 23/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736500.7500\n",
      "Epoch 24/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736500.2500\n",
      "Epoch 25/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.5000\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.7500\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736499.5000\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736502.7500\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.0000\n",
      "Epoch 30/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736501.2500\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736500.7500\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736499.0000\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736500.2500\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736500.5000\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736499.2500\n",
      "Epoch 36/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.2500\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736499.0000\n",
      "Epoch 38/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736499.0000\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736499.0000\n",
      "Epoch 40/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736501.5000\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736498.2500\n",
      "Epoch 42/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736500.5000\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736496.2500\n",
      "Epoch 44/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736499.5000\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736495.0000\n",
      "Epoch 46/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736497.7500\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736498.0000\n",
      "Epoch 48/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736496.2500\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736498.2500\n",
      "Epoch 50/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736500.7500\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736498.5000\n",
      "Epoch 52/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736500.7500\n",
      "Epoch 53/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736496.0000\n",
      "Epoch 54/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736496.2500\n",
      "Epoch 55/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736495.7500\n",
      "Epoch 56/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736495.7500\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736496.0000\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736497.2500\n",
      "Epoch 59/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736493.5000\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736495.7500\n",
      "Epoch 61/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736496.7500\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736494.0000\n",
      "Epoch 63/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736495.5000\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736496.2500\n",
      "Epoch 65/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736497.7500\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736494.0000\n",
      "Epoch 67/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736491.7500\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736495.7500\n",
      "Epoch 69/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736493.7500\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736494.7500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736494.7500\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736496.2500\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736495.0000\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736494.5000\n",
      "Epoch 75/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736494.5000\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736493.2500\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736492.7500\n",
      "Epoch 78/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.0000\n",
      "Epoch 79/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736493.5000\n",
      "Epoch 80/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736494.0000\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736494.5000\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736494.7500\n",
      "Epoch 83/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736495.0000\n",
      "Epoch 84/100\n",
      "3189/3189 [==============================] - 16s 5ms/step - loss: 2736492.7500\n",
      "Epoch 85/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736490.0000\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736493.2500\n",
      "Epoch 87/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736490.5000\n",
      "Epoch 88/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736496.0000\n",
      "Epoch 89/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736494.0000\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736491.2500\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736489.2500\n",
      "Epoch 92/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736490.5000\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736490.5000\n",
      "Epoch 94/100\n",
      "3189/3189 [==============================] - 18s 6ms/step - loss: 2736487.7500\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736491.0000\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736490.2500\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736490.2500\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736494.0000\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736488.7500\n",
      "Epoch 100/100\n",
      "3189/3189 [==============================] - 17s 5ms/step - loss: 2736490.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "image_side=int(sqrt(clean_specs.shape[-1]))\n",
    "image_dims=(image_side,image_side,1)\n",
    "\n",
    "\n",
    "auto_encoder3=models.Sequential()\n",
    "auto_encoder3.add(layers.Input(shape=(clean_specs.shape[-1],)))\n",
    "auto_encoder3.add(layers.Reshape(image_dims))\n",
    "auto_encoder3.add(layers.Conv2D(filters = 32, kernel_size = (1,1),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder3.add(layers.MaxPooling2D(2,strides=2))\n",
    "auto_encoder3.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder3.add(layers.MaxPooling2D(2,strides=2))\n",
    "auto_encoder3.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder3.add(layers.MaxPooling2D(2,strides=2))\n",
    "auto_encoder3.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder3.add(layers.UpSampling2D(2))\n",
    "auto_encoder3.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder3.add(layers.UpSampling2D(2))\n",
    "auto_encoder3.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder3.add(layers.UpSampling2D(2))\n",
    "auto_encoder3.add(layers.Conv2D(filters = 1, kernel_size = (3,3), padding = 'Same',\n",
    "                activation ='relu'))\n",
    "auto_encoder3.add(layers.Flatten())\n",
    "auto_encoder3.add(layers.Dense(clean_specs.shape[-1]))\n",
    "auto_encoder3.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder3.summary()\n",
    "\n",
    "auto_encoder3,history2=train_model(auto_encoder3,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_4 (Reshape)         (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 50, 50, 32)        64        \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 25, 25, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 25, 25, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 12, 12, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 6, 6, 32)          18464     \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSamplin  (None, 12, 12, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSamplin  (None, 24, 24, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 24, 24, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_14 (UpSamplin  (None, 48, 48, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 48, 48, 1)         289       \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 2500)              5762500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,132,005\n",
      "Trainable params: 6,132,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 2736507.2500\n",
      "Epoch 2/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736503.0000\n",
      "Epoch 3/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736506.7500\n",
      "Epoch 4/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736506.7500\n",
      "Epoch 5/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2736435.2500\n",
      "Epoch 6/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2626703.7500\n",
      "Epoch 7/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2330380.0000\n",
      "Epoch 8/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 2127828.2500\n",
      "Epoch 9/100\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 1994834.5000\n",
      "Epoch 10/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1876327.5000\n",
      "Epoch 11/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1785333.5000\n",
      "Epoch 12/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1700446.3750\n",
      "Epoch 13/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1635548.7500\n",
      "Epoch 14/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1581869.5000\n",
      "Epoch 15/100\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 1535986.6250\n",
      "Epoch 16/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1503053.0000\n",
      "Epoch 17/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1465612.2500\n",
      "Epoch 18/100\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 1441460.7500\n",
      "Epoch 19/100\n",
      "3189/3189 [==============================] - 30s 9ms/step - loss: 1415766.2500\n",
      "Epoch 20/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 1394984.6250\n",
      "Epoch 21/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1380896.1250\n",
      "Epoch 22/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1362982.7500\n",
      "Epoch 23/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1343423.8750\n",
      "Epoch 24/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1334050.5000\n",
      "Epoch 25/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1317409.0000\n",
      "Epoch 26/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1303359.6250\n",
      "Epoch 27/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1288352.7500\n",
      "Epoch 28/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1282040.8750\n",
      "Epoch 29/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1270075.5000\n",
      "Epoch 30/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1258375.2500\n",
      "Epoch 31/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1248352.1250\n",
      "Epoch 32/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1238652.8750\n",
      "Epoch 33/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1222388.7500\n",
      "Epoch 34/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1219606.0000\n",
      "Epoch 35/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1212257.8750\n",
      "Epoch 36/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1202684.0000\n",
      "Epoch 37/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1195566.1250\n",
      "Epoch 38/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1193057.8750\n",
      "Epoch 39/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1181710.1250\n",
      "Epoch 40/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1176868.5000\n",
      "Epoch 41/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1170382.2500\n",
      "Epoch 42/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1160867.5000\n",
      "Epoch 43/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1161863.2500\n",
      "Epoch 44/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1155252.5000\n",
      "Epoch 45/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1146256.2500\n",
      "Epoch 46/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1141015.5000\n",
      "Epoch 47/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1137642.8750\n",
      "Epoch 48/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1130027.7500\n",
      "Epoch 49/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1130664.0000\n",
      "Epoch 50/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1121837.5000\n",
      "Epoch 51/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1121285.1250\n",
      "Epoch 52/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1112772.1250\n",
      "Epoch 53/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1113241.6250\n",
      "Epoch 54/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1107209.3750\n",
      "Epoch 55/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1104566.5000\n",
      "Epoch 56/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1102881.7500\n",
      "Epoch 57/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1097782.8750\n",
      "Epoch 58/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1090331.7500\n",
      "Epoch 59/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1091779.5000\n",
      "Epoch 60/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1084100.3750\n",
      "Epoch 61/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1078852.2500\n",
      "Epoch 62/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1074270.5000\n",
      "Epoch 63/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1078262.7500\n",
      "Epoch 64/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1073578.1250\n",
      "Epoch 65/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1069950.8750\n",
      "Epoch 66/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1071288.8750\n",
      "Epoch 67/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1065295.2500\n",
      "Epoch 68/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1058978.3750\n",
      "Epoch 69/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1061027.2500\n",
      "Epoch 70/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1057619.2500\n",
      "Epoch 71/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1055100.1250\n",
      "Epoch 72/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1049590.5000\n",
      "Epoch 73/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1050545.7500\n",
      "Epoch 74/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1045436.1875\n",
      "Epoch 75/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1044627.0625\n",
      "Epoch 76/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1042617.6875\n",
      "Epoch 77/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1039094.6875\n",
      "Epoch 78/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1038949.8750\n",
      "Epoch 79/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1032395.8750\n",
      "Epoch 80/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1034956.3750\n",
      "Epoch 81/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1029414.7500\n",
      "Epoch 82/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1027420.3125\n",
      "Epoch 83/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1024843.6250\n",
      "Epoch 84/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1025434.6250\n",
      "Epoch 85/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1022465.6875\n",
      "Epoch 86/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1019375.9375\n",
      "Epoch 87/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1019060.0625\n",
      "Epoch 88/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1014254.8125\n",
      "Epoch 89/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1013474.3750\n",
      "Epoch 90/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1008398.9375\n",
      "Epoch 91/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1011378.0625\n",
      "Epoch 92/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1010317.5000\n",
      "Epoch 93/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1009597.9375\n",
      "Epoch 94/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1006414.6875\n",
      "Epoch 95/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1007835.1250\n",
      "Epoch 96/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1004379.8750\n",
      "Epoch 97/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 1002977.5000\n",
      "Epoch 98/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 997566.0625\n",
      "Epoch 99/100\n",
      "3189/3189 [==============================] - 29s 9ms/step - loss: 999996.6875\n",
      "Epoch 100/100\n",
      "3189/3189 [==============================] - 28s 9ms/step - loss: 996075.3750\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "image_side=int(sqrt(clean_specs.shape[-1]))\n",
    "image_dims=(image_side,image_side,1)\n",
    "\n",
    "\n",
    "auto_encoder4=models.Sequential()\n",
    "auto_encoder4.add(layers.Input(shape=(clean_specs.shape[-1],)))\n",
    "auto_encoder4.add(layers.Reshape(image_dims))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 32, kernel_size = (1,1),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.MaxPooling2D(2,strides=2))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.MaxPooling2D(2,strides=2))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.MaxPooling2D(2,strides=2))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.UpSampling2D(2))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.UpSampling2D(2))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.UpSampling2D(2))\n",
    "auto_encoder4.add(layers.Conv2D(filters = 1, kernel_size = (3,3), padding = 'Same',\n",
    "                activation ='relu'))\n",
    "auto_encoder4.add(layers.Flatten())\n",
    "auto_encoder4.add(layers.Dense(clean_specs.shape[-1]))\n",
    "auto_encoder4.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder4.summary()\n",
    "\n",
    "auto_encoder4,history2=train_model(auto_encoder4,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_20 (Reshape)        (None, 50, 50, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_20 (UpSamplin  (None, 250, 250, 1)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 250, 250, 3)       2190      \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, None, None, 512)   14714688  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              25691136  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2500)              2562500   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,970,514\n",
      "Trainable params: 42,970,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "inputs=2500\n",
    "image_side=50\n",
    "image_dims=(image_side,image_side,1)\n",
    "\n",
    "\n",
    "auto_encoder5=models.Sequential()\n",
    "auto_encoder5.add(layers.Input(shape=(inputs,)))\n",
    "auto_encoder5.add(layers.Reshape(image_dims))\n",
    "auto_encoder5.add(layers.UpSampling2D(5))\n",
    "auto_encoder5.add(layers.Conv2D(filters = 3, kernel_size = (27,27),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder5.add(VGG16(weights='imagenet',include_top= False))\n",
    "auto_encoder5.add(layers.Flatten())\n",
    "auto_encoder5.add(layers.Dense(1024))\n",
    "auto_encoder5.add(layers.Dense(inputs))\n",
    "auto_encoder5.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataGenerator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m auto_encoder5,history5\u001b[39m=\u001b[39mtrain_gen(auto_encoder5,\u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataGenerator' object is not callable"
     ]
    }
   ],
   "source": [
    "auto_encoder5,history5=train_gen(auto_encoder5,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 134ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-99.54379  , -87.35634  , -85.9384   , ...,  -2.3729415,\n",
       "        -6.250412 ,  -2.07368  ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_audio(audio_path=r'C:\\Users\\USUARIO\\Desktop\\DeepLearning\\proyectoFinalGit\\Music_noise_reduction\\wavs\\noisy\\clnsp37.wav',model=auto_encoder4,file_name=\"auto_encoder4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 50, 50, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_56 (Conv2D)          (None, 50, 50, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 50, 50, 128)       295040    \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 25, 25, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_58 (Conv2D)          (None, 25, 25, 64)        73792     \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 25, 25, 64)        36928     \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 50, 50, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 50, 50, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 50, 50, 3)         6915      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 784,259\n",
      "Trainable params: 784,259\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs=clean_specs.shape[-1]\n",
    "image_side=int(sqrt(inputs))\n",
    "image_dims=(image_side,image_side,1)\n",
    "#transform vector to square matrix\n",
    "\n",
    "\n",
    "\n",
    "#Input_img = keras.Input(shape=image_dims) \n",
    "\n",
    "Input_vector=keras.Input(shape=(inputs,))\n",
    "Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "#encoding architecture\n",
    "x1 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(Input_img)\n",
    "x2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "x2 = keras.layers.MaxPool2D( (2, 2))(x2)\n",
    "encoded = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x2)\n",
    "\n",
    "# decoding architecture\n",
    "x3 =keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x3 = keras.layers.UpSampling2D((2, 2))(x3)\n",
    "x2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x3)\n",
    "x1 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n",
    "decoded = keras.layers.Conv2D(3, (3, 3), padding='same')(x1)\n",
    "\n",
    "autoencoder = keras.Model(Input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=clean_specs.shape[-1]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 2500) dtype=float32 (created by layer 'input_35')>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 17 layers, found 16 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m upscale\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mUpSampling2D(\u001b[39m5\u001b[39m)(Input_img)\n\u001b[0;32m      7\u001b[0m x1 \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m3\u001b[39m, (\u001b[39m27\u001b[39m, \u001b[39m27\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m)(upscale)\n\u001b[1;32m----> 9\u001b[0m VGG_model\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39;49mapplications\u001b[39m.\u001b[39;49mvgg16\u001b[39m.\u001b[39;49mVGG16(weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m,include_top\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,input_tensor\u001b[39m=\u001b[39;49mx1)\n\u001b[0;32m     10\u001b[0m poppedModel \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mModel(VGG_model\u001b[39m.\u001b[39minput,VGG_model\u001b[39m.\u001b[39moutput)\n\u001b[0;32m     11\u001b[0m poppedModel\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\applications\\vgg16.py:223\u001b[0m, in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    217\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     weights_path \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mget_file(\n\u001b[0;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         WEIGHTS_PATH_NO_TOP,\n\u001b[0;32m    221\u001b[0m         cache_subdir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    222\u001b[0m         file_hash\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m6d6bbae143d832006294945121d1f1fc\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m   model\u001b[39m.\u001b[39;49mload_weights(weights_path)\n\u001b[0;32m    224\u001b[0m \u001b[39melif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m   model\u001b[39m.\u001b[39mload_weights(weights)\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\saving\\hdf5_format.py:728\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    726\u001b[0m layer_names \u001b[39m=\u001b[39m filtered_layer_names\n\u001b[0;32m    727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(layer_names) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(filtered_layers):\n\u001b[1;32m--> 728\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    729\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLayer count mismatch when loading weights from file. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    730\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mModel expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(filtered_layers)\u001b[39m}\u001b[39;00m\u001b[39m layers, found \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    731\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(layer_names)\u001b[39m}\u001b[39;00m\u001b[39m saved layers.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m \u001b[39m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[39m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[0;32m    735\u001b[0m weight_value_tuples \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 17 layers, found 16 saved layers."
     ]
    }
   ],
   "source": [
    "inputs=2500\n",
    "image_side=int(sqrt(inputs))\n",
    "image_dims=(image_side,image_side,1)\n",
    "Input_vector=keras.Input(shape=(inputs,))\n",
    "Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "upscale=keras.layers.UpSampling2D(5)(Input_img)\n",
    "x1 = keras.layers.Conv2D(3, (27, 27), activation='relu', padding='valid')(upscale)\n",
    "\n",
    "VGG_model=keras.applications.vgg16.VGG16(weights='imagenet',include_top= True,input_tensor=x1)\n",
    "poppedModel = keras.models.Model(VGG_model.input,VGG_model.output)\n",
    "poppedModel.trainable = False\n",
    "\n",
    "output_layer = Dense(inputs, activation='softmax')(poppedModel.output)\n",
    "model = keras.models.Model(inputs=Input_vector, outputs=output_layer)\n",
    "model.compile(optimizer='adam', \n",
    "                loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Input_vector\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(inputs,))\n\u001b[0;32m      2\u001b[0m Input_img\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mReshape(image_dims)(Input_vector)\n\u001b[0;32m      3\u001b[0m x1 \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m3\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(Input_img)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "Input_vector=keras.Input(shape=(inputs,))\n",
    "Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "x1 = keras.layers.Conv2D(3, (3, 3), activation='relu', padding='same')(Input_img)\n",
    "upscale=keras.layers.UpSampling2D(5)(x1)\n",
    "\n",
    "VGG_model=keras.applications.vgg16.VGG16(weights='imagenet',include_top= True,input_tensor=upscale)\n",
    "poppedModel = keras.models.Model(VGG_model.input,VGG_model.layers[-2].output)\n",
    "poppedModel.trainable = False\n",
    "# x3=keras.layers.Flatten()(x2)\n",
    "output_layer = Dense(inputs, activation='softmax')(poppedModel.output)\n",
    "model = keras.models.Model(inputs=Input_vector, outputs=output_layer)\n",
    "model.compile(optimizer='adam', \n",
    "                loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = keras.Input(shape=(224,224,3))\n",
    "VGG_model=keras.applications.vgg16.VGG16(weights='imagenet',include_top= True,input_tensor=input_tensor)\n",
    "poppedModel = keras.models.Model(VGG_model.input,VGG_model.layers[-2].output)\n",
    "poppedModel.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_58 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2500)              10242500  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 144,503,044\n",
      "Trainable params: 10,242,500\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input_vector=keras.Input(shape=(inputs,))\n",
    "# Input_img=keras.layers.Reshape(image_dims)(Input_vector)\n",
    "# upscale=keras.layers.UpSampling2D(5)(Input_vector)\n",
    "\n",
    "input_tensor = keras.Input(shape=(224,224,3))\n",
    "VGG_model=keras.applications.vgg16.VGG16(weights='imagenet',include_top= True,input_tensor=input_tensor)\n",
    "poppedModel = keras.models.Model(VGG_model.input,VGG_model.layers[-2].output)\n",
    "poppedModel.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "# top_model = layers.Flatten(name=\"flatten\")(poppedModel.output)\n",
    "# # top_model = layers.Dense(4096, activation='relu')(top_model)\n",
    "# # top_model = layers.Dense(1072, activation='relu')(top_model)\n",
    "# # top_model = layers.Dropout(0.2)(top_model)\n",
    "output_layer = Dense(inputs, activation='softmax')(poppedModel.output)\n",
    "\n",
    "model = keras.models.Model(inputs=input_tensor, outputs=output_layer)\n",
    "\n",
    "# Compiles the model for training.\n",
    "model.compile(optimizer='adam', \n",
    "                loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_42'), name='input_42', description=\"created by layer 'input_42'\"), but it was called on an input with incompatible shape (None, None).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model_20\" (type Functional).\n    \n    Input 0 of layer \"block1_conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received by layer \"model_20\" (type Functional):\n       inputs=tf.Tensor(shape=(None, None), dtype=int16)\n       training=True\n       mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cnn_transfer_1,history2\u001b[39m=\u001b[39mtrain_model(model,\u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model,epochs):\n\u001b[1;32m----> 2\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      3\u001b[0m                     train_gen\n\u001b[0;32m      4\u001b[0m                     ,epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m      5\u001b[0m                     )\n\u001b[0;32m      7\u001b[0m     plt\u001b[39m.\u001b[39mclf()\n\u001b[0;32m      8\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filephpnzf50.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model_20\" (type Functional).\n    \n    Input 0 of layer \"block1_conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received by layer \"model_20\" (type Functional):\n       inputs=tf.Tensor(shape=(None, None), dtype=int16)\n       training=True\n       mask=None\n"
     ]
    }
   ],
   "source": [
    "cnn_transfer_1,history2=train_model(model,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 300, 300, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (None, 2500).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"vgg16\" (type Functional).\n\nInput 0 of layer \"block1_conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 2500)\n\nCall arguments received by layer \"vgg16\" (type Functional):\n   inputs=tf.Tensor(shape=(None, 2500), dtype=float32)\n   training=False\n   mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m inputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(inputs,))\n\u001b[0;32m     12\u001b[0m x\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mReshape(image_dims)\n\u001b[1;32m---> 13\u001b[0m x \u001b[39m=\u001b[39m model(inputs, training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     14\u001b[0m \u001b[39m#pooling layer \u001b[39;00m\n\u001b[0;32m     15\u001b[0m x \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mGlobalAveragePooling2D()(x)\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\input_spec.py:228\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    226\u001b[0m   ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    227\u001b[0m   \u001b[39mif\u001b[39;00m ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ndim \u001b[39m<\u001b[39m spec\u001b[39m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 228\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    230\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexpected min_ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mmin_ndim\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    231\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfound ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    232\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    233\u001b[0m \u001b[39m# Check dtype.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"vgg16\" (type Functional).\n\nInput 0 of layer \"block1_conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (None, 2500)\n\nCall arguments received by layer \"vgg16\" (type Functional):\n   inputs=tf.Tensor(shape=(None, 2500), dtype=float32)\n   training=False\n   mask=None"
     ]
    }
   ],
   "source": [
    "n_neurons=32\n",
    "inputs=clean_specs.shape[-1]\n",
    "image_side=int(sqrt(clean_specs.shape[-1]))\n",
    "image_dims=(image_side,image_side,1)\n",
    "model = tf.keras.applications.VGG16(\n",
    "weights='imagenet',\n",
    "input_shape=(300, 300, 3),\n",
    "include_top=False\n",
    ")\n",
    "model.trainable = False\n",
    "inputs = keras.Input(shape=(inputs,))\n",
    "x = model(inputs, training=False)\n",
    "#pooling layer \n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dense(n_neurons, activation = 'softmax')(x)\n",
    "x = keras.layers.Dropout(drop_out, input_shape=(n_neurons,))(x)\n",
    "#final dense layer\n",
    "outputs = keras.layers.Dense((inputs), activation = 'softmax')(x)\n",
    "model = keras.Model(inputs,outputs)\n",
    "\n",
    "autoenconder_transfer=keras.Model()\n",
    "autoenconder_transfer.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall()])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
