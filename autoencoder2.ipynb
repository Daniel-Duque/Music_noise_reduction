{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(file_path):\n",
    "    audio_array, sample_rate= librosa.load(file_path)\n",
    "    spec = librosa.feature.melspectrogram(y=audio_array,\n",
    "                                    sr=sample_rate, \n",
    "                                        n_fft=2048, \n",
    "                                        hop_length=512, \n",
    "                                        win_length=None, \n",
    "                                        window='hann', \n",
    "                                        center=True, \n",
    "                                        pad_mode='reflect', \n",
    "                                        power=2.0,\n",
    "                                    n_mels=128)\n",
    "    log_spec = librosa.power_to_db(spec, ref=np.max)\n",
    "    return spec,sample_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando con un archivo limpio y uno con overlay de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file=r'wavs\\clean\\clnsp0.wav'\n",
    "noise_file=r'wavs\\noisy\\output1.wav'\n",
    "test_clean_spec,test_clean_sr =create_spectrogram(clean_file)\n",
    "test_noisy_spec,test_noisy_sr=create_spectrogram(noise_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 475), (128, 475))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_spec.shape,test_noisy_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_noisy_sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador de archivos en masa usando batcheador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spec_from_dir(dir_path,top_x=200):\n",
    "    #dir_path directorio o folder donde estan los wavs\n",
    "    # top_x opcional cuantos archivos maximo desea usar, dejar vacio para usarlos todos\n",
    "    dir = os.listdir(dir_path)\n",
    "    spec_list=[]\n",
    "    s_rates=[]\n",
    "    for i, file in enumerate(dir):\n",
    "        try:\n",
    "            if i<=top_x:\n",
    "                input_file = os.path.join(dir_path, file)\n",
    "                ms,sr=create_spectrogram(input_file)\n",
    "                spec_list.append(ms)\n",
    "                s_rates.append(sr)\n",
    "        except:\n",
    "            print(file,\" file skipped\")\n",
    "    \n",
    "    return spec_list,s_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_specs,clean_s_rates=create_spec_from_dir(r'wavs\\clean',10000)\n",
    "noisy_specs,noisy_s_rates=create_spec_from_dir(r'wavs\\noisy',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22050, 22050)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(clean_s_rates),max(clean_s_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22050, 22050)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(noisy_s_rates),max(noisy_s_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_specs(clean_specs,noisy_specs):\n",
    "    \n",
    "    #getting max lenght of all audios\n",
    "    max_y=0\n",
    "    \n",
    "    for i,j in zip(clean_specs,noisy_specs):\n",
    "        if i.shape[1]>max_y: max_y=i.shape[1]\n",
    "        if j.shape[1]>max_y: max_y=j.shape[1]\n",
    "    print(max_y)\n",
    "    # reshapping all spectrogram\n",
    "\n",
    "    for index,s in enumerate(clean_specs):\n",
    "        try:\n",
    "            clean_specs[index]=np.resize(s,(s.shape[0],max_y))\n",
    "        except Exception as e:\n",
    "            print(f\"skipping clean {index} {e}\")\n",
    "    \n",
    "    clean_specs=np.array(clean_specs)\n",
    "    clean_specs=clean_specs.reshape(-1,s.shape[0],max_y,1)\n",
    "\n",
    "    for index,s in enumerate(noisy_specs):\n",
    "        try:\n",
    "            noisy_specs[index]=np.resize(s,(s.shape[0],max_y))\n",
    "        except Exception as e:\n",
    "            print(f\"skipping noise {index} {e}\")\n",
    "            \n",
    "    noisy_specs=np.array(noisy_specs)\n",
    "    noisy_specs=noisy_specs.reshape(-1,s.shape[0],max_y,1)\n",
    "\n",
    "    return clean_specs,noisy_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n"
     ]
    }
   ],
   "source": [
    "s_clean_specs,s_noisy_specs=standardize_specs(clean_specs,noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10001, 128, 702, 1), (10001, 128, 702, 1))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_clean_specs.shape,s_noisy_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np   \n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(s_noisy_specs, s_clean_specs, 32)\n",
    "#test_gen = DataGenerator(X_test, y_test, 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Dense,Flatten,Reshape,InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "#import mlflow\n",
    "#import mlflow.tensorflow\n",
    "from PIL import Image\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 0\n",
    "\n",
    "solo 2 capas densas y regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_21 (Flatten)        (None, 89856)             0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                5750848   \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 89856)             5840640   \n",
      "                                                                 \n",
      " reshape_19 (Reshape)        (None, 128, 702)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,595,648\n",
      "Trainable params: 11,595,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 702) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 702), dtype=tf.float32, name='input_13'), name='input_13', description=\"created by layer 'input_13'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 702) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 702), dtype=tf.float32, name='input_13'), name='input_13', description=\"created by layer 'input_13'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 88.2829\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 82.8643\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 82.0721\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.9028\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.7747\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.6994\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.5737\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 81.3918\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 81.2537\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.0218\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.8503\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.6660\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 80.4784\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 80.2789\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.1188\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 79.9823\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 79.8346\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 79.6739\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 79.5535\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 79.4080\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder0=models.Sequential()\n",
    "auto_encoder0.add(layers.Input(shape=img_shape))\n",
    "auto_encoder0.add(layers.Flatten())\n",
    "auto_encoder0.add(layers.Dense(64))\n",
    "auto_encoder0.add(layers.Dropout(drop_out))\n",
    "auto_encoder0.add(layers.Dense(64))\n",
    "auto_encoder0.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder0.add(layers.Reshape(img_shape))\n",
    "auto_encoder0.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder0.summary()\n",
    "\n",
    "history0 = auto_encoder0.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 1\n",
    "\n",
    "capas más complejas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_22 (Flatten)        (None, 89856)             0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 128)               11501696  \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 89856)             11591424  \n",
      "                                                                 \n",
      " reshape_20 (Reshape)        (None, 128, 702)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,421,952\n",
      "Trainable params: 23,421,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 702) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 702), dtype=tf.float32, name='input_14'), name='input_14', description=\"created by layer 'input_14'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 702) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 702), dtype=tf.float32, name='input_14'), name='input_14', description=\"created by layer 'input_14'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 92.1561\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 83.0667\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 82.2496\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 82.0119\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.8365\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.8118\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.7414\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 81.6410\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.5158\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 81.4222\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 81.3030\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 81.1725\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 81.0134\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 80.9013\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 80.7124\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 80.5872\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 80.4831\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.2953\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.1515\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.0121\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder=models.Sequential()\n",
    "auto_encoder.add(layers.Input(shape=img_shape))\n",
    "auto_encoder.add(layers.Flatten())\n",
    "auto_encoder.add(layers.Dense(128))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(256))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(512))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(256))\n",
    "auto_encoder.add(layers.Dense(128))\n",
    "auto_encoder.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder.add(layers.Reshape(img_shape))\n",
    "\n",
    "\n",
    "auto_encoder.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder.summary()\n",
    "\n",
    "history1 = auto_encoder.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_25 (Flatten)        (None, 89856)             0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 64)                5750848   \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 89856)             5840640   \n",
      "                                                                 \n",
      " reshape_23 (Reshape)        (None, 128, 702)          0         \n",
      "                                                                 \n",
      " flatten_26 (Flatten)        (None, 89856)             0         \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 128)               11501696  \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 89856)             11591424  \n",
      "                                                                 \n",
      " reshape_24 (Reshape)        (None, 128, 702)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,705,280\n",
      "Trainable params: 34,705,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 702) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 702), dtype=tf.float32, name='input_16'), name='input_16', description=\"created by layer 'input_16'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 702) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 702), dtype=tf.float32, name='input_16'), name='input_16', description=\"created by layer 'input_16'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "313/313 [==============================] - 4s 8ms/step - loss: 96.9001\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 82.1958\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.9068\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.9080\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 81.9663\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.9168\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.8818\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 82.0160\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 81.9179\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 81.9232\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 81.8768\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 81.8469\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 81.7589\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 81.6930\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 81.6217\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.5401\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.4940\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.4251\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.4080\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 81.3373\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder2=models.Sequential()\n",
    "auto_encoder2.add(layers.Input(shape=img_shape))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(64))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(64))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(128))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(128))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "auto_encoder2.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder2.summary()\n",
    "\n",
    "history2 = auto_encoder2.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_31 (Flatten)        (None, 89856)             0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 32)                2875424   \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 89856)             2965248   \n",
      "                                                                 \n",
      " reshape_28 (Reshape)        (None, 128, 702)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,841,728\n",
      "Trainable params: 5,841,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 702) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 702), dtype=tf.float32, name='input_21'), name='input_21', description=\"created by layer 'input_21'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 702) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 702), dtype=tf.float32, name='input_21'), name='input_21', description=\"created by layer 'input_21'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 84.1688\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 82.0928\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.8457\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 81.7173\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.6092\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.5115\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 81.3926\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 81.2451\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 81.1007\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.8988\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.8061\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 80.6421\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 80.5226\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 80.3863\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.2887\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.1839\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.0858\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 80.0109\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 79.9079\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 79.8183\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder3=models.Sequential()\n",
    "auto_encoder3.add(layers.Input(shape=img_shape))\n",
    "auto_encoder3.add(layers.Flatten())\n",
    "auto_encoder3.add(layers.Dense(32))\n",
    "auto_encoder3.add(layers.Dropout(drop_out))\n",
    "auto_encoder3.add(layers.Dense(32))\n",
    "auto_encoder3.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder3.add(layers.Reshape(img_shape))\n",
    "auto_encoder3.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder3.summary()\n",
    "\n",
    "history3 = auto_encoder3.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    batch_size=32\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 702, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_shape_cnn=(s_clean_specs.shape[1],s_clean_specs.shape[2],1)\n",
    "img_shape_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89856"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod((128, 702))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "failed to allocate memory [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mConv2D(filters \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, kernel_size \u001b[39m=\u001b[39m (\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m),padding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSame\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      5\u001b[0m                 activation \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape \u001b[39m=\u001b[39m img_shape_cnn))\n\u001b[0;32m      6\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mFlatten())\n\u001b[1;32m----> 7\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39;49madd(layers\u001b[39m.\u001b[39;49mDense(\u001b[39m128\u001b[39;49m)) \u001b[39m# np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\u001b[39;00m\n\u001b[0;32m      8\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mReshape(img_shape_cnn))\n\u001b[0;32m      9\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    588\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\audio_dl\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\audio_dl\\lib\\site-packages\\keras\\backend.py:1920\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generator:\n\u001b[0;32m   1918\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generator\u001b[39m.\u001b[39muniform(\n\u001b[0;32m   1919\u001b[0m       shape\u001b[39m=\u001b[39mshape, minval\u001b[39m=\u001b[39mminval, maxval\u001b[39m=\u001b[39mmaxval, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m-> 1920\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49muniform(\n\u001b[0;32m   1921\u001b[0m     shape\u001b[39m=\u001b[39;49mshape, minval\u001b[39m=\u001b[39;49mminval, maxval\u001b[39m=\u001b[39;49mmaxval, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1922\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_legacy_seed())\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: failed to allocate memory [Op:Mul]"
     ]
    }
   ],
   "source": [
    "img_shape_cnn=(s_clean_specs.shape[1],s_clean_specs.shape[2],1)\n",
    "\n",
    "cnn_auto_encoder0=models.Sequential()\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 3, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu', input_shape = img_shape_cnn))\n",
    "\n",
    "cnn_auto_encoder0.add(layers.Flatten())\n",
    "cnn_auto_encoder0.add(layers.Dense(128)) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "cnn_auto_encoder0.add(layers.Reshape(img_shape_cnn))\n",
    "cnn_auto_encoder0.summary()\n",
    "\n",
    "history_cnn0 = cnn_auto_encoder0.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    batch_size=8\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 128, 702, 32)      832       \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 128, 702, 32)      25632     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 64, 351, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 64, 351, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64, 351, 32)       0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 718848)            0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 512)               368050688 \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 89856)             5840640   \n",
      "                                                                 \n",
      " reshape_9 (Reshape)         (None, 128, 702, 1)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 373,950,752\n",
      "Trainable params: 373,950,688\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2],1)\n",
    "auto_encoder2=models.Sequential()\n",
    "auto_encoder2.add(layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                activation ='relu', input_shape = img_shape))\n",
    "auto_encoder2.add(layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder2.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "auto_encoder2.add(layers.BatchNormalization())\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(512, activation = \"relu\"))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(64, activation = \"softmax\"))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "\n",
    "auto_encoder2.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 82.3141\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 82.2238\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 82.1419\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 82.0619\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 25s 78ms/step - loss: 81.9885\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.9242\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.8656\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.8094\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.7610\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.7171\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.6769\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.6403\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.6069\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.5766\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.5484\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.5227\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.4990\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.4774\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.4560\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.4370\n"
     ]
    }
   ],
   "source": [
    "#history2 = auto_encoder2.fit(x=s_noisy_specs, y=s_clean_specs, epochs=100)\n",
    "history2 = auto_encoder2.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "preds=auto_encoder.predict(s_noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 128, 702)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 702)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.88221908e-04, 2.46463984e-04, 2.61851761e-04, ...,\n",
       "        2.57317000e-03, 8.99837411e-04, 2.44650152e-03],\n",
       "       [1.21181156e-03, 9.17276135e-04, 9.90588916e-04, ...,\n",
       "        5.54336328e-03, 2.85331719e-02, 2.60114968e-01],\n",
       "       [1.14793324e+00, 1.52739763e+00, 1.82428646e+00, ...,\n",
       "        2.50480145e-01, 8.19973946e-02, 4.72248858e-03],\n",
       "       ...,\n",
       "       [9.29996677e-05, 9.84362341e-05, 1.06841406e-04, ...,\n",
       "        2.54283252e-04, 6.28159833e-05, 1.36140082e-03],\n",
       "       [2.84687756e-03, 9.32404399e-03, 1.19536798e-02, ...,\n",
       "        1.18909981e-02, 1.26775932e-02, 8.28386191e-03],\n",
       "       [6.48943149e-03, 3.44618829e-03, 1.06449577e-03, ...,\n",
       "        2.61219740e-02, 3.30388226e-04, 1.67555991e-04]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_specs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_log=librosa.db_to_power(preds[0])\n",
    "# step3 converting mel-spectrogrma back to wav file\n",
    "res = librosa.feature.inverse.mel_to_audio(reversed_log, \n",
    "                                           sr=noisy_s_rates[0], \n",
    "                                           n_fft=2048, \n",
    "                                           hop_length=512, \n",
    "                                           win_length=None, \n",
    "                                           window='hann', \n",
    "                                           center=True, \n",
    "                                           pad_mode='reflect', \n",
    "                                           power=2.0, \n",
    "                                           n_iter=32)\n",
    "\n",
    "import soundfile as sf\n",
    "sf.write(\"test2.wav\", res, noisy_s_rates[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertidor de espectrograma a audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 - converting a wav file to numpy array and then converting that to mel-spectrogram\n",
    "scale_file=r'wavs\\clean\\clnsp0.wav'\n",
    "my_audio_as_np_array, my_sample_rate= librosa.load(scale_file)\n",
    "\n",
    "# step2 - converting audio np array to spectrogram\n",
    "spec = librosa.feature.melspectrogram(y=my_audio_as_np_array,\n",
    "                                        sr=my_sample_rate, \n",
    "                                            n_fft=2048, \n",
    "                                            hop_length=512, \n",
    "                                            win_length=None, \n",
    "                                            window='hann', \n",
    "                                            center=True, \n",
    "                                            pad_mode='reflect', \n",
    "                                            power=2.0,\n",
    "                                     n_mels=128)\n",
    "log_spec = librosa.power_to_db(spec)\n",
    "reversed_log=librosa.db_to_power(log_spec)\n",
    "# step3 converting mel-spectrogrma back to wav file\n",
    "res = librosa.feature.inverse.mel_to_audio(reversed_log, \n",
    "                                           sr=my_sample_rate, \n",
    "                                           n_fft=2048, \n",
    "                                           hop_length=512, \n",
    "                                           win_length=None, \n",
    "                                           window='hann', \n",
    "                                           center=True, \n",
    "                                           pad_mode='reflect', \n",
    "                                           power=2.0, \n",
    "                                           n_iter=32)\n",
    "\n",
    "# step4 - save it as a wav file\n",
    "import soundfile as sf\n",
    "sf.write(\"test1.wav\", res, my_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('scale.wav', scale, sr, format='ogg', subtype='vorbis')\n",
    "sf.write('audio.wav', audio, sr, format='ogg', subtype='vorbis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
