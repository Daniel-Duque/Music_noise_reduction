{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(file_path):\n",
    "    audio_array, sample_rate= librosa.load(file_path)\n",
    "    spec = librosa.feature.melspectrogram(y=audio_array,\n",
    "                                    sr=sample_rate, \n",
    "                                        n_fft=2048, \n",
    "                                        hop_length=512, \n",
    "                                        win_length=None, \n",
    "                                        window='hann', \n",
    "                                        center=True, \n",
    "                                        pad_mode='reflect', \n",
    "                                        power=2.0,\n",
    "                                    n_mels=128)\n",
    "    log_spec = librosa.power_to_db(spec, ref=np.max)\n",
    "    return log_spec,sample_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando con un archivo limpio y uno con overlay de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file=r'wavs\\clean\\clnsp0.wav'\n",
    "noise_file=r'wavs\\noisy\\clnsp0.wav'\n",
    "test_clean_spec,test_clean_sr =create_spectrogram(clean_file)\n",
    "test_noisy_spec,test_noisy_sr=create_spectrogram(noise_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 475), (128, 475))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_spec.shape,test_noisy_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(test_clean_spec.shape[1]/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 95)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hsplit(test_clean_spec,5)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_noisy_sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador de archivos en masa usando batcheador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def create_spec_from_dir(dir_path,top_x=200):\n",
    "    #dir_path directorio o folder donde estan los wavs\n",
    "    # top_x opcional cuantos archivos maximo desea usar, dejar vacio para usarlos todos\n",
    "    dir = os.listdir(dir_path)\n",
    "    spec_list=[]\n",
    "    s_rates=[]\n",
    "    for i, file in enumerate(dir):\n",
    "        try:\n",
    "            if i<=top_x:\n",
    "                input_file = os.path.join(dir_path, file)\n",
    "                ms,sr=create_spectrogram(input_file)\n",
    "                num_batches=ceil(ms.shape[1]/128) if ms.shape[1]>128 else 1\n",
    "                #print(ms.shape)\n",
    "                ms=np.resize(ms,(ms.shape[0],128*num_batches))\n",
    "                batches=np.hsplit(ms,num_batches)\n",
    "                for batch in batches:\n",
    "                    spec_list.append(batch)\n",
    "                    s_rates.append(sr)\n",
    "        except:\n",
    "            print(file,\" file skipped\")\n",
    "    \n",
    "    return spec_list,s_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_specs,clean_s_rates=create_spec_from_dir(r'wavs\\clean',24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_specs,noisy_s_rates=create_spec_from_dir(r'wavs\\noisy',24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37301, 37297)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_specs),len(noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_specs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22050, 22050)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(clean_s_rates),max(clean_s_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22050, 22050)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(noisy_s_rates),max(noisy_s_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_specs(clean_specs,noisy_specs):\n",
    "    \n",
    "    #getting max lenght of all audios\n",
    "    max_y=0\n",
    "    \n",
    "    for i,j in zip(clean_specs,noisy_specs):\n",
    "        if i.shape[1]>max_y: max_y=i.shape[1]\n",
    "        if j.shape[1]>max_y: max_y=j.shape[1]\n",
    "    print(max_y)\n",
    "    # reshapping all spectrogram\n",
    "\n",
    "    for index,s in enumerate(clean_specs):\n",
    "        try:\n",
    "            clean_specs[index]=np.resize(s,(s.shape[0],max_y))\n",
    "        except Exception as e:\n",
    "            print(f\"skipping clean {index} {e}\")\n",
    "    \n",
    "    clean_specs=np.array(clean_specs)\n",
    "    clean_specs=clean_specs.reshape(-1,s.shape[0],max_y,1)\n",
    "\n",
    "    for index,s in enumerate(noisy_specs):\n",
    "        try:\n",
    "            noisy_specs[index]=np.resize(s,(s.shape[0],max_y))\n",
    "        except Exception as e:\n",
    "            print(f\"skipping noise {index} {e}\")\n",
    "            \n",
    "    noisy_specs=np.array(noisy_specs)\n",
    "    noisy_specs=noisy_specs.reshape(-1,s.shape[0],max_y,1)\n",
    "\n",
    "    return clean_specs,noisy_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "s_clean_specs,s_noisy_specs=standardize_specs(clean_specs,noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s_clean_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37301, 128, 128, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_clean_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=min(s_clean_specs.shape[0],s_noisy_specs.shape[0])\n",
    "s_clean_specs=s_clean_specs[:samples]\n",
    "s_noisy_specs=s_noisy_specs[:samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np   \n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(s_noisy_specs, s_clean_specs, 32)\n",
    "#test_gen = DataGenerator(X_test, y_test, 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Dense,Flatten,Reshape,InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "#import mlflow\n",
    "#import mlflow.tensorflow\n",
    "from PIL import Image\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 0\n",
    "\n",
    "solo 2 capas densas y regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1048640   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16384)             1064960   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 128, 128)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,117,760\n",
      "Trainable params: 2,117,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "1166/1166 [==============================] - 19s 14ms/step - loss: 480.4779\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 420.4550\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 410.3920\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 398.5192\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 398.9811\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 395.5131\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 393.3949\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 395.0036\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 396.2625\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 391.7239\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 389.5161\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 389.2381\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 388.6719\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 388.9037\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 386.8879\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 384.7669\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 382.9645\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 384.7155\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 383.4988\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 382.0653\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder0=models.Sequential()\n",
    "auto_encoder0.add(layers.Input(shape=img_shape))\n",
    "auto_encoder0.add(layers.Flatten())\n",
    "auto_encoder0.add(layers.Dense(64))\n",
    "auto_encoder0.add(layers.Dropout(drop_out))\n",
    "auto_encoder0.add(layers.Dense(64))\n",
    "auto_encoder0.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder0.add(layers.Reshape(img_shape))\n",
    "auto_encoder0.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder0.summary()\n",
    "\n",
    "history0 = auto_encoder0.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 1\n",
    "\n",
    "capas más complejas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               2097280   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16384)             2113536   \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,539,648\n",
      "Trainable params: 4,539,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 516.4131\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 435.4940\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 427.0521\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 416.7943\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 407.9789\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 400.4070\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 405.0248\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 399.6114\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 401.8953\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 397.3332\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 398.2874\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 393.1780\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 393.4762\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 392.6712\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 394.1548\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 390.7886\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 388.8821\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 388.8377\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 387.6641\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 389.3995\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder=models.Sequential()\n",
    "auto_encoder.add(layers.Input(shape=img_shape))\n",
    "auto_encoder.add(layers.Flatten())\n",
    "auto_encoder.add(layers.Dense(128))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(256))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(512))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(256))\n",
    "auto_encoder.add(layers.Dense(128))\n",
    "auto_encoder.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder.add(layers.Reshape(img_shape))\n",
    "\n",
    "\n",
    "auto_encoder.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder.summary()\n",
    "\n",
    "history1 = auto_encoder.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                1048640   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 16384)             1064960   \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               2097280   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16384)             2113536   \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               4194560   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16384)             4210688   \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,816,128\n",
      "Trainable params: 14,816,128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 953.3074\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 433.6781\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 431.3289\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 434.0010\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 443.6713\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 428.3086\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 420.0246\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 418.7983\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 418.7912\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 414.2997\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 411.2219\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 408.3740\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 404.2964\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 403.5695\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 402.7091\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 400.7959\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 399.3134\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 398.7466\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 396.9754\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 394.9631\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder2=models.Sequential()\n",
    "auto_encoder2.add(layers.Input(shape=img_shape))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(64))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(64))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(128))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(128))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(256))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(256))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "auto_encoder2.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder2.summary()\n",
    "\n",
    "history2 = auto_encoder2.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                524320    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 16384)             540672    \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,066,048\n",
      "Trainable params: 1,066,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 464.8795\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 418.8796\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 404.9266\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 402.2472\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 401.2303\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 398.0476\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 398.8533\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 396.4258\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 391.8134\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 391.1881\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 390.8354\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 390.1514\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 389.9757\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 390.8962\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 3s 2ms/step - loss: 388.9807\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 3s 2ms/step - loss: 388.6565\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 3s 2ms/step - loss: 386.6643\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 387.0480\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 3s 2ms/step - loss: 385.9772\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 3s 2ms/step - loss: 386.6845\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder3=models.Sequential()\n",
    "auto_encoder3.add(layers.Input(shape=img_shape))\n",
    "auto_encoder3.add(layers.Flatten())\n",
    "auto_encoder3.add(layers.Dense(32))\n",
    "auto_encoder3.add(layers.Dropout(drop_out))\n",
    "auto_encoder3.add(layers.Dense(32))\n",
    "auto_encoder3.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder3.add(layers.Reshape(img_shape))\n",
    "auto_encoder3.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder3.summary()\n",
    "\n",
    "history3 = auto_encoder3.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    batch_size=32\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_shape_cnn=(s_clean_specs.shape[1],s_clean_specs.shape[2],1)\n",
    "img_shape_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89856"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod((128, 702))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37297, 128, 128, 1), (37297, 128, 128, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_clean_specs.shape,s_noisy_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[19554369536,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:RandomUniform]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDropout(drop_out))\n\u001b[0;32m      9\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mFlatten())\n\u001b[1;32m---> 10\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39;49madd(layers\u001b[39m.\u001b[39;49mDense(\u001b[39m512\u001b[39;49m, activation \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     11\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDropout(drop_out))\n\u001b[0;32m     12\u001b[0m cnn_auto_encoder0\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDense(\u001b[39m64\u001b[39m, activation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    588\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\audio_dl\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\USUARIO\\anaconda3\\envs\\audio_dl\\lib\\site-packages\\keras\\backend.py:1920\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generator:\n\u001b[0;32m   1918\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generator\u001b[39m.\u001b[39muniform(\n\u001b[0;32m   1919\u001b[0m       shape\u001b[39m=\u001b[39mshape, minval\u001b[39m=\u001b[39mminval, maxval\u001b[39m=\u001b[39mmaxval, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m-> 1920\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49muniform(\n\u001b[0;32m   1921\u001b[0m     shape\u001b[39m=\u001b[39;49mshape, minval\u001b[39m=\u001b[39;49mminval, maxval\u001b[39m=\u001b[39;49mmaxval, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1922\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_legacy_seed())\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[19554369536,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:RandomUniform]"
     ]
    }
   ],
   "source": [
    "cnn_auto_encoder0=models.Sequential()\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu', input_shape = s_noisy_specs.shape))\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "#cnn_auto_encoder0.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "cnn_auto_encoder0.add(layers.BatchNormalization())\n",
    "cnn_auto_encoder0.add(layers.Dropout(drop_out))\n",
    "cnn_auto_encoder0.add(layers.Flatten())\n",
    "cnn_auto_encoder0.add(layers.Dense(512, activation = \"relu\"))\n",
    "cnn_auto_encoder0.add(layers.Dropout(drop_out))\n",
    "cnn_auto_encoder0.add(layers.Dense(64, activation = \"softmax\"))\n",
    "cnn_auto_encoder0.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "cnn_auto_encoder0.add(layers.Reshape(img_shape))\n",
    "\n",
    "cnn_auto_encoder0.compile(optimizer='adamax', loss='mse')\n",
    "cnn_auto_encoder0.summary()\n",
    "history_cnn0 = cnn_auto_encoder0.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    batch_size=8\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 128, 702, 32)      832       \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 128, 702, 32)      25632     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 64, 351, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 64, 351, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64, 351, 32)       0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 718848)            0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 512)               368050688 \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 89856)             5840640   \n",
      "                                                                 \n",
      " reshape_9 (Reshape)         (None, 128, 702, 1)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 373,950,752\n",
      "Trainable params: 373,950,688\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2],1)\n",
    "auto_encoder2=models.Sequential()\n",
    "auto_encoder2.add(layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                activation ='relu', input_shape = img_shape))\n",
    "auto_encoder2.add(layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "auto_encoder2.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "auto_encoder2.add(layers.BatchNormalization())\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(512, activation = \"relu\"))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(64, activation = \"softmax\"))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "\n",
    "auto_encoder2.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 82.3141\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 82.2238\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 82.1419\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 82.0619\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 25s 78ms/step - loss: 81.9885\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.9242\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.8656\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.8094\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.7610\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.7171\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.6769\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.6403\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.6069\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.5766\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 81.5484\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.5227\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.4990\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.4774\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.4560\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 81.4370\n"
     ]
    }
   ],
   "source": [
    "#history2 = auto_encoder2.fit(x=s_noisy_specs, y=s_clean_specs, epochs=100)\n",
    "history2 = auto_encoder2.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "preds=auto_encoder.predict(s_noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 128, 702)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 702)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.88221908e-04, 2.46463984e-04, 2.61851761e-04, ...,\n",
       "        2.57317000e-03, 8.99837411e-04, 2.44650152e-03],\n",
       "       [1.21181156e-03, 9.17276135e-04, 9.90588916e-04, ...,\n",
       "        5.54336328e-03, 2.85331719e-02, 2.60114968e-01],\n",
       "       [1.14793324e+00, 1.52739763e+00, 1.82428646e+00, ...,\n",
       "        2.50480145e-01, 8.19973946e-02, 4.72248858e-03],\n",
       "       ...,\n",
       "       [9.29996677e-05, 9.84362341e-05, 1.06841406e-04, ...,\n",
       "        2.54283252e-04, 6.28159833e-05, 1.36140082e-03],\n",
       "       [2.84687756e-03, 9.32404399e-03, 1.19536798e-02, ...,\n",
       "        1.18909981e-02, 1.26775932e-02, 8.28386191e-03],\n",
       "       [6.48943149e-03, 3.44618829e-03, 1.06449577e-03, ...,\n",
       "        2.61219740e-02, 3.30388226e-04, 1.67555991e-04]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_specs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_log=librosa.db_to_power(preds[0])\n",
    "# step3 converting mel-spectrogrma back to wav file\n",
    "res = librosa.feature.inverse.mel_to_audio(reversed_log, \n",
    "                                           sr=noisy_s_rates[0], \n",
    "                                           n_fft=2048, \n",
    "                                           hop_length=512, \n",
    "                                           win_length=None, \n",
    "                                           window='hann', \n",
    "                                           center=True, \n",
    "                                           pad_mode='reflect', \n",
    "                                           power=2.0, \n",
    "                                           n_iter=32)\n",
    "\n",
    "import soundfile as sf\n",
    "sf.write(\"test2.wav\", res, noisy_s_rates[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertidor de espectrograma a audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 - converting a wav file to numpy array and then converting that to mel-spectrogram\n",
    "scale_file=r'wavs\\clean\\clnsp0.wav'\n",
    "my_audio_as_np_array, my_sample_rate= librosa.load(scale_file)\n",
    "\n",
    "# step2 - converting audio np array to spectrogram\n",
    "spec = librosa.feature.melspectrogram(y=my_audio_as_np_array,\n",
    "                                        sr=my_sample_rate, \n",
    "                                            n_fft=2048, \n",
    "                                            hop_length=512, \n",
    "                                            win_length=None, \n",
    "                                            window='hann', \n",
    "                                            center=True, \n",
    "                                            pad_mode='reflect', \n",
    "                                            power=2.0,\n",
    "                                     n_mels=128)\n",
    "log_spec = librosa.power_to_db(spec)\n",
    "reversed_log=librosa.db_to_power(log_spec)\n",
    "# step3 converting mel-spectrogrma back to wav file\n",
    "res = librosa.feature.inverse.mel_to_audio(reversed_log, \n",
    "                                           sr=my_sample_rate, \n",
    "                                           n_fft=2048, \n",
    "                                           hop_length=512, \n",
    "                                           win_length=None, \n",
    "                                           window='hann', \n",
    "                                           center=True, \n",
    "                                           pad_mode='reflect', \n",
    "                                           power=2.0, \n",
    "                                           n_iter=32)\n",
    "\n",
    "# step4 - save it as a wav file\n",
    "import soundfile as sf\n",
    "sf.write(\"test1.wav\", res, my_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('scale.wav', scale, sr, format='ogg', subtype='vorbis')\n",
    "sf.write('audio.wav', audio, sr, format='ogg', subtype='vorbis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
