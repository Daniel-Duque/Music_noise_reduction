{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(file_path):\n",
    "    audio_array, sample_rate= librosa.load(file_path)\n",
    "    spec = librosa.feature.melspectrogram(y=audio_array,\n",
    "                                    sr=sample_rate, \n",
    "                                        n_fft=2048, \n",
    "                                        hop_length=512, \n",
    "                                        win_length=None, \n",
    "                                        window='hann', \n",
    "                                        center=True, \n",
    "                                        pad_mode='reflect', \n",
    "                                        power=2.0,\n",
    "                                    n_mels=128)\n",
    "    log_spec = librosa.power_to_db(spec, ref=np.max)\n",
    "    return log_spec,sample_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando con un archivo limpio y uno con overlay de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file=r'wavs\\clean\\clnsp0.wav'\n",
    "noise_file=r'wavs\\noisy\\clnsp0.wav'\n",
    "test_clean_spec,test_clean_sr =create_spectrogram(clean_file)\n",
    "test_noisy_spec,test_noisy_sr=create_spectrogram(noise_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 475), (128, 475))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_spec.shape,test_noisy_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(test_clean_spec.shape[1]/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 95)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hsplit(test_clean_spec,5)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_noisy_sr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador de archivos en masa usando batcheador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def create_spec_from_dir(dir_path,top_x=200):\n",
    "    #dir_path directorio o folder donde estan los wavs\n",
    "    # top_x opcional cuantos archivos maximo desea usar, dejar vacio para usarlos todos\n",
    "    dir = os.listdir(dir_path)\n",
    "    spec_list=[]\n",
    "    s_rates=[]\n",
    "    for i, file in enumerate(dir):\n",
    "        try:\n",
    "            if i<=top_x:\n",
    "                input_file = os.path.join(dir_path, file)\n",
    "                ms,sr=create_spectrogram(input_file)\n",
    "                num_batches=ceil(ms.shape[1]/128) if ms.shape[1]>128 else 1\n",
    "                #print(ms.shape)\n",
    "                ms=np.resize(ms,(ms.shape[0],128*num_batches))\n",
    "                batches=np.hsplit(ms,num_batches)\n",
    "                for batch in batches:\n",
    "                    spec_list.append(batch)\n",
    "                    s_rates.append(sr)\n",
    "        except:\n",
    "            print(file,\" file skipped\")\n",
    "    \n",
    "    return spec_list,s_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_specs,clean_s_rates=create_spec_from_dir(r'wavs\\clean',24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_specs,noisy_s_rates=create_spec_from_dir(r'wavs\\noisy',24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37301, 37297)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_specs),len(noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_specs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22050, 22050)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(clean_s_rates),max(clean_s_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22050, 22050)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(noisy_s_rates),max(noisy_s_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_specs(clean_specs,noisy_specs):\n",
    "    \n",
    "    #getting max lenght of all audios\n",
    "    max_y=0\n",
    "    \n",
    "    for i,j in zip(clean_specs,noisy_specs):\n",
    "        if i.shape[1]>max_y: max_y=i.shape[1]\n",
    "        if j.shape[1]>max_y: max_y=j.shape[1]\n",
    "    print(max_y)\n",
    "    # reshapping all spectrogram\n",
    "\n",
    "    for index,s in enumerate(clean_specs):\n",
    "        try:\n",
    "            clean_specs[index]=np.resize(s,(s.shape[0],max_y))\n",
    "        except Exception as e:\n",
    "            print(f\"skipping clean {index} {e}\")\n",
    "    \n",
    "    clean_specs=np.array(clean_specs)\n",
    "    clean_specs=clean_specs.reshape(-1,s.shape[0],max_y,1)\n",
    "\n",
    "    for index,s in enumerate(noisy_specs):\n",
    "        try:\n",
    "            noisy_specs[index]=np.resize(s,(s.shape[0],max_y))\n",
    "        except Exception as e:\n",
    "            print(f\"skipping noise {index} {e}\")\n",
    "            \n",
    "    noisy_specs=np.array(noisy_specs)\n",
    "    noisy_specs=noisy_specs.reshape(-1,s.shape[0],max_y,1)\n",
    "\n",
    "    return clean_specs,noisy_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "s_clean_specs,s_noisy_specs=standardize_specs(clean_specs,noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s_clean_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37301, 128, 128, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_clean_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=min(s_clean_specs.shape[0],s_noisy_specs.shape[0])\n",
    "s_clean_specs=s_clean_specs[:samples]\n",
    "s_noisy_specs=s_noisy_specs[:samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np   \n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(s_clean_specs, s_clean_specs, 32)\n",
    "#test_gen = DataGenerator(X_test, y_test, 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Dense,Flatten,Reshape,InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "#import mlflow\n",
    "#import mlflow.tensorflow\n",
    "from PIL import Image\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 0\n",
    "\n",
    "solo 2 capas densas y regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                1048640   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 16384)             1064960   \n",
      "                                                                 \n",
      " reshape_6 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,117,760\n",
      "Trainable params: 2,117,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 411.3357\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 308.6693\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 284.3328\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 262.6251\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 251.8824\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 246.2486\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 242.9985\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 239.7418\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 236.6167\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 234.8797\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 235.0647\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 232.9137\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 230.5360\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 229.6917\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 229.9365\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 228.0366\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 229.3527\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 227.2462\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 226.1429\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 225.5143\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder0=models.Sequential()\n",
    "auto_encoder0.add(layers.Input(shape=img_shape))\n",
    "auto_encoder0.add(layers.Flatten())\n",
    "auto_encoder0.add(layers.Dense(64))\n",
    "auto_encoder0.add(layers.Dropout(drop_out))\n",
    "auto_encoder0.add(layers.Dense(64))\n",
    "auto_encoder0.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder0.add(layers.Reshape(img_shape))\n",
    "auto_encoder0.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder0.summary()\n",
    "\n",
    "history0 = auto_encoder0.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 1\n",
    "\n",
    "capas más complejas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_7 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               2097280   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 16384)             2113536   \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,539,648\n",
      "Trainable params: 4,539,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 438.3586\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 324.0573\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 299.8000\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 276.3824\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 263.1703\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 252.8285\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 245.7531\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 239.9238\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 237.2476\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 233.5604\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 233.7694\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 231.6747\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 228.0791\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 227.4977\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 225.8920\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 226.0113\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 225.0326\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 223.2435\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 222.9795\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 4s 4ms/step - loss: 222.1582\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder=models.Sequential()\n",
    "auto_encoder.add(layers.Input(shape=img_shape))\n",
    "auto_encoder.add(layers.Flatten())\n",
    "auto_encoder.add(layers.Dense(128))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(256))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(512))\n",
    "auto_encoder.add(layers.Dropout(drop_out))\n",
    "auto_encoder.add(layers.Dense(256))\n",
    "auto_encoder.add(layers.Dense(128))\n",
    "auto_encoder.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder.add(layers.Reshape(img_shape))\n",
    "\n",
    "\n",
    "auto_encoder.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder.summary()\n",
    "\n",
    "history1 = auto_encoder.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                1048640   \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 16384)             1064960   \n",
      "                                                                 \n",
      " reshape_8 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               2097280   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 16384)             2113536   \n",
      "                                                                 \n",
      " reshape_9 (Reshape)         (None, 128, 128)          0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               4194560   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16384)             4210688   \n",
      "                                                                 \n",
      " reshape_10 (Reshape)        (None, 128, 128)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,816,128\n",
      "Trainable params: 14,816,128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_7'), name='input_7', description=\"created by layer 'input_7'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_7'), name='input_7', description=\"created by layer 'input_7'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 1021.8936\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 6s 5ms/step - loss: 368.3226\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 359.0207\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 353.9155\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 336.6148\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 6s 5ms/step - loss: 329.5722\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 313.2242\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 5s 5ms/step - loss: 310.8134\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 308.0753\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 5s 5ms/step - loss: 302.9866\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 302.5406\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 298.3562\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 298.6867\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 292.7549\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 291.5242\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 289.5596\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 290.7021\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 288.8949\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 287.7854\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 5s 4ms/step - loss: 287.2764\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder2=models.Sequential()\n",
    "auto_encoder2.add(layers.Input(shape=img_shape))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(64))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(64))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(128))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(128))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "auto_encoder2.add(layers.Flatten())\n",
    "auto_encoder2.add(layers.Dense(256))\n",
    "auto_encoder2.add(layers.Dropout(drop_out))\n",
    "auto_encoder2.add(layers.Dense(256))\n",
    "auto_encoder2.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder2.add(layers.Reshape(img_shape))\n",
    "auto_encoder2.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder2.summary()\n",
    "\n",
    "history2 = auto_encoder2.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_11 (Flatten)        (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 32)                524320    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 16384)             540672    \n",
      "                                                                 \n",
      " reshape_11 (Reshape)        (None, 128, 128)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,066,048\n",
      "Trainable params: 1,066,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_8'), name='input_8', description=\"created by layer 'input_8'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128), dtype=tf.float32, name='input_8'), name='input_8', description=\"created by layer 'input_8'\"), but it was called on an input with incompatible shape (None, None, None, None).\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 398.5534\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 310.7562\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 297.0556\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 289.3695\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 282.7855\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 279.4191\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 276.5448\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 273.6980\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 274.1023\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 270.7324\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 268.3403\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 267.3389\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 266.1602\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 268.3925\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 267.5554\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 3s 3ms/step - loss: 267.4737\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 266.3415\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 265.7846\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 265.0420\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 4s 3ms/step - loss: 263.5863\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.1\n",
    "\n",
    "img_shape=(s_clean_specs.shape[1],s_clean_specs.shape[2])\n",
    "\n",
    "auto_encoder3=models.Sequential()\n",
    "auto_encoder3.add(layers.Input(shape=img_shape))\n",
    "auto_encoder3.add(layers.Flatten())\n",
    "auto_encoder3.add(layers.Dense(32))\n",
    "auto_encoder3.add(layers.Dropout(drop_out))\n",
    "auto_encoder3.add(layers.Dense(32))\n",
    "auto_encoder3.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "auto_encoder3.add(layers.Reshape(img_shape))\n",
    "auto_encoder3.compile(optimizer='adamax', loss='mse')\n",
    "auto_encoder3.summary()\n",
    "\n",
    "history3 = auto_encoder3.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    batch_size=32\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_shape_cnn=(s_clean_specs.shape[1],s_clean_specs.shape[2],1)\n",
    "img_shape_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89856"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod((128, 702))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37297, 128, 128, 1), (37297, 128, 128, 1))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_clean_specs.shape,s_noisy_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37297, 128, 128, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_noisy_specs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_132 (Conv2D)         (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_59 (MaxPoolin  (None, 64, 64, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_133 (Conv2D)         (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_60 (MaxPoolin  (None, 32, 32, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_134 (Conv2D)         (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_61 (MaxPoolin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_135 (Conv2D)         (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " up_sampling2d_54 (UpSamplin  (None, 32, 32, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_136 (Conv2D)         (None, 32, 32, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_55 (UpSamplin  (None, 64, 64, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_137 (Conv2D)         (None, 64, 64, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_56 (UpSamplin  (None, 128, 128, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_138 (Conv2D)         (None, 128, 128, 1)       289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 332,801\n",
      "Trainable params: 332,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3508.3508\n",
      "Epoch 2/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9565\n",
      "Epoch 3/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9558\n",
      "Epoch 4/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9570\n",
      "Epoch 5/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 6/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9551\n",
      "Epoch 7/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9580\n",
      "Epoch 8/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 9/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9585\n",
      "Epoch 10/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9551\n",
      "Epoch 11/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9585\n",
      "Epoch 12/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9563\n",
      "Epoch 13/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9565\n",
      "Epoch 14/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9563\n",
      "Epoch 15/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9558\n",
      "Epoch 16/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9558\n",
      "Epoch 17/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9558\n",
      "Epoch 18/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 19/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9570\n",
      "Epoch 20/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9580\n",
      "Epoch 21/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9556\n",
      "Epoch 22/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9565\n",
      "Epoch 23/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9551\n",
      "Epoch 24/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9563\n",
      "Epoch 25/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9553\n",
      "Epoch 26/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 27/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9556\n",
      "Epoch 28/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 29/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9541\n",
      "Epoch 30/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9573\n",
      "Epoch 31/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9578\n",
      "Epoch 32/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9558\n",
      "Epoch 33/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9565\n",
      "Epoch 34/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9561\n",
      "Epoch 35/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9543\n",
      "Epoch 36/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 37/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 38/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 39/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9573\n",
      "Epoch 40/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9561\n",
      "Epoch 41/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9561\n",
      "Epoch 42/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9556\n",
      "Epoch 43/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9580\n",
      "Epoch 44/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9583\n",
      "Epoch 45/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9570\n",
      "Epoch 46/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9558\n",
      "Epoch 47/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9575\n",
      "Epoch 48/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9568\n",
      "Epoch 49/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9556\n",
      "Epoch 50/50\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9548\n"
     ]
    }
   ],
   "source": [
    "img_shape_cnn=(s_noisy_specs.shape[1],s_noisy_specs.shape[2],s_noisy_specs.shape[3])\n",
    "\n",
    "cnn_auto_encoder0=models.Sequential()\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu', input_shape = img_shape_cnn))\n",
    "cnn_auto_encoder0.add(layers.MaxPooling2D(2,strides=2))\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder0.add(layers.MaxPooling2D(2,strides=2))\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder0.add(layers.MaxPooling2D(2,strides=2))\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder0.add(layers.UpSampling2D(2))\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder0.add(layers.UpSampling2D(2))\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder0.add(layers.UpSampling2D(2))\n",
    "cnn_auto_encoder0.add(layers.Conv2D(filters = 1, kernel_size = (3,3), padding = 'Same',\n",
    "                activation ='relu'))\n",
    "#cnn_auto_encoder0.add(layers.BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #cnn_auto_encoder0.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "# cnn_auto_encoder0.add(layers.BatchNormalization())\n",
    "# cnn_auto_encoder0.add(layers.Dropout(drop_out))\n",
    "# cnn_auto_encoder0.add(layers.Flatten())\n",
    "# cnn_auto_encoder0.add(layers.Dense(512, activation = \"relu\"))\n",
    "# cnn_auto_encoder0.add(layers.Dropout(drop_out))\n",
    "# cnn_auto_encoder0.add(layers.Dense(64, activation = \"softmax\"))\n",
    "# cnn_auto_encoder0.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "# cnn_auto_encoder0.add(layers.Reshape(img_shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cnn_auto_encoder0.compile(optimizer='adam', loss='mse')\n",
    "cnn_auto_encoder0.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history_cnn0 = cnn_auto_encoder0.fit(train_gen,\n",
    "                    epochs=50,\n",
    "                    batch_size=32\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_111 (Conv2D)         (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_50 (MaxPoolin  (None, 64, 64, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_112 (Conv2D)         (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_51 (MaxPoolin  (None, 32, 32, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32, 32, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_113 (Conv2D)         (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_52 (MaxPoolin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_114 (Conv2D)         (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " up_sampling2d_45 (UpSamplin  (None, 32, 32, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_115 (Conv2D)         (None, 32, 32, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_46 (UpSamplin  (None, 64, 64, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64, 64, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_116 (Conv2D)         (None, 64, 64, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_47 (UpSamplin  (None, 128, 128, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_117 (Conv2D)         (None, 128, 128, 1)       289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,313\n",
      "Trainable params: 333,057\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1166/1166 [==============================] - 15s 12ms/step - loss: 3508.0417\n",
      "Epoch 2/20\n",
      "1166/1166 [==============================] - 16s 13ms/step - loss: 3507.9565\n",
      "Epoch 3/20\n",
      "1166/1166 [==============================] - 15s 12ms/step - loss: 3507.9575\n",
      "Epoch 4/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9568\n",
      "Epoch 5/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9565\n",
      "Epoch 6/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9553\n",
      "Epoch 7/20\n",
      "1166/1166 [==============================] - 14s 12ms/step - loss: 3507.9580\n",
      "Epoch 8/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9558\n",
      "Epoch 9/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9561\n",
      "Epoch 10/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9558\n",
      "Epoch 11/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9563\n",
      "Epoch 12/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9570\n",
      "Epoch 13/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9558\n",
      "Epoch 14/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9575\n",
      "Epoch 15/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9565\n",
      "Epoch 16/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9563\n",
      "Epoch 17/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9568\n",
      "Epoch 18/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9563\n",
      "Epoch 19/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9551\n",
      "Epoch 20/20\n",
      "1166/1166 [==============================] - 15s 13ms/step - loss: 3507.9580\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.5\n",
    "\n",
    "cnn_auto_encoder1=models.Sequential()\n",
    "cnn_auto_encoder1.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu', input_shape = img_shape_cnn))\n",
    "cnn_auto_encoder1.add(layers.MaxPooling2D(2,strides=2))\n",
    "cnn_auto_encoder1.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder1.add(layers.MaxPooling2D(2,strides=2))\n",
    "cnn_auto_encoder1.add(layers.BatchNormalization())\n",
    "cnn_auto_encoder0.add(layers.Dropout(drop_out))\n",
    "cnn_auto_encoder1.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder1.add(layers.MaxPooling2D(2,strides=2))\n",
    "cnn_auto_encoder1.add(layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder1.add(layers.UpSampling2D(2))\n",
    "cnn_auto_encoder1.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder1.add(layers.UpSampling2D(2))\n",
    "cnn_auto_encoder1.add(layers.BatchNormalization())\n",
    "cnn_auto_encoder0.add(layers.Dropout(drop_out))\n",
    "cnn_auto_encoder1.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder1.add(layers.UpSampling2D(2))\n",
    "cnn_auto_encoder1.add(layers.Conv2D(filters = 1, kernel_size = (3,3), padding = 'Same',\n",
    "                activation ='relu'))\n",
    "cnn_auto_encoder1.compile(optimizer='adamax', loss='mse')\n",
    "cnn_auto_encoder1.summary()\n",
    "\n",
    "\n",
    "\n",
    "history_cnn1 = cnn_auto_encoder1.fit(train_gen,\n",
    "                    epochs=20,\n",
    "                    batch_size=8\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "preds=auto_encoder.predict(s_noisy_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 128, 702)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 702)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.88221908e-04, 2.46463984e-04, 2.61851761e-04, ...,\n",
       "        2.57317000e-03, 8.99837411e-04, 2.44650152e-03],\n",
       "       [1.21181156e-03, 9.17276135e-04, 9.90588916e-04, ...,\n",
       "        5.54336328e-03, 2.85331719e-02, 2.60114968e-01],\n",
       "       [1.14793324e+00, 1.52739763e+00, 1.82428646e+00, ...,\n",
       "        2.50480145e-01, 8.19973946e-02, 4.72248858e-03],\n",
       "       ...,\n",
       "       [9.29996677e-05, 9.84362341e-05, 1.06841406e-04, ...,\n",
       "        2.54283252e-04, 6.28159833e-05, 1.36140082e-03],\n",
       "       [2.84687756e-03, 9.32404399e-03, 1.19536798e-02, ...,\n",
       "        1.18909981e-02, 1.26775932e-02, 8.28386191e-03],\n",
       "       [6.48943149e-03, 3.44618829e-03, 1.06449577e-03, ...,\n",
       "        2.61219740e-02, 3.30388226e-04, 1.67555991e-04]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_specs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_log=librosa.db_to_power(preds[0])\n",
    "# step3 converting mel-spectrogrma back to wav file\n",
    "res = librosa.feature.inverse.mel_to_audio(reversed_log, \n",
    "                                           sr=noisy_s_rates[0], \n",
    "                                           n_fft=2048, \n",
    "                                           hop_length=512, \n",
    "                                           win_length=None, \n",
    "                                           window='hann', \n",
    "                                           center=True, \n",
    "                                           pad_mode='reflect', \n",
    "                                           power=2.0, \n",
    "                                           n_iter=32)\n",
    "\n",
    "import soundfile as sf\n",
    "sf.write(\"test2.wav\", res, noisy_s_rates[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertidor de espectrograma a audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 - converting a wav file to numpy array and then converting that to mel-spectrogram\n",
    "scale_file=r'wavs\\clean\\clnsp0.wav'\n",
    "my_audio_as_np_array, my_sample_rate= librosa.load(scale_file)\n",
    "\n",
    "# step2 - converting audio np array to spectrogram\n",
    "spec = librosa.feature.melspectrogram(y=my_audio_as_np_array,\n",
    "                                        sr=my_sample_rate, \n",
    "                                            n_fft=2048, \n",
    "                                            hop_length=512, \n",
    "                                            win_length=None, \n",
    "                                            window='hann', \n",
    "                                            center=True, \n",
    "                                            pad_mode='reflect', \n",
    "                                            power=2.0,\n",
    "                                     n_mels=128)\n",
    "log_spec = librosa.power_to_db(spec)\n",
    "reversed_log=librosa.db_to_power(log_spec)\n",
    "# step3 converting mel-spectrogrma back to wav file\n",
    "res = librosa.feature.inverse.mel_to_audio(reversed_log, \n",
    "                                           sr=my_sample_rate, \n",
    "                                           n_fft=2048, \n",
    "                                           hop_length=512, \n",
    "                                           win_length=None, \n",
    "                                           window='hann', \n",
    "                                           center=True, \n",
    "                                           pad_mode='reflect', \n",
    "                                           power=2.0, \n",
    "                                           n_iter=32)\n",
    "\n",
    "# step4 - save it as a wav file\n",
    "import soundfile as sf\n",
    "sf.write(\"test1.wav\", res, my_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sf.write('scale.wav', scale, sr, format='ogg', subtype='vorbis')\n",
    "sf.write('audio.wav', audio, sr, format='ogg', subtype='vorbis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
